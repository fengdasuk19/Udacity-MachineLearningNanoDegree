Simulator.__init__(): Unable to import pygame; display disabled.
ImportError: No module named pygame

/-------------------------
| Training trial 1
\-------------------------

Environment.reset(): Trial set up with start = (7, 2), destination = (5, 4), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -10.0630318224
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', 'left'), 'deadline': 20, 't': 0, 'action': 'left', 'reward': -10.063031822397193, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', 'left')
Agent attempted driving left through a red light. (rewarded -10.06)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -10.4878276073
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': 'left', 'reward': -10.487827607301323, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.49)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: right, reward: 1.08995086911
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 1.0899508691100335, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.09)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: right, reward: 2.16929374236
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 2.1692937423551744, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'right')
Agent followed the waypoint right. (rewarded 2.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: 2.27792981965
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 2.2779298196521363, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent followed the waypoint forward. (rewarded 2.28)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: forward, reward: 0.566012922737
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', 'left'), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': 0.5660129227368096, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'forward', 'left')
Agent drove forward instead of left. (rewarded 0.57)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: left, reward: 2.23877356073
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 2.238773560730884, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.24)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: left, reward: -10.9802112581
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -10.980211258087172, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.98)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: forward, reward: -9.26397180694
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -9.263971806940559, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.26)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: None, reward: 1.74283011524
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.7428301152393921, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 1.74)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: forward, reward: -9.84980470439
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -9.849804704385157, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.85)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: None, reward: -4.0590360767
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'forward'), 'deadline': 9, 't': 11, 'action': None, 'reward': -4.059036076703954, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.06)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: right, reward: 1.45513513365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 1.455135133645266, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', 'left')
Agent drove right instead of left. (rewarded 1.46)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: -19.2962253798
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': -19.29622537981904, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.30)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (0, 1), action: left, reward: 0.407611452106
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': 0.40761145210592054, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.41)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: left, reward: 1.722196224
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': 1.7221962240016067, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 1.72)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: 0.77018837099
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 4, 't': 16, 'action': 'left', 'reward': 0.7701883709898725, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove left instead of forward. (rewarded 0.77)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': -5.243913803650662, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: right, reward: 0.951766036548
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.951766036548022, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 0.95)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 2
\-------------------------

Environment.reset(): Trial set up with start = (8, 5), destination = (6, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: None, reward: 2.19979509582
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'forward'), 'deadline': 20, 't': 0, 'action': None, 'reward': 2.1997950958211927, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'forward')
Agent properly idled at a red light. (rewarded 2.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: right, reward: -19.1561084855
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': -19.15610848552791, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.16)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5449754345550167, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: -9.14928771615
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': 'left', 'reward': -9.149287716153246, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': -4.574643858076623, 'right': 0.5449754345550167, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: forward, reward: -10.8388892623
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -10.838889262343368, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -5.419444631171684, 'left': -4.574643858076623, 'right': 0.5449754345550167, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: None, reward: 2.50204909319
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 2.5020490931892443, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: left, reward: 0.560473176038
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 0.5604731760379766, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'right', None)
Agent drove left instead of right. (rewarded 0.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: None, reward: -4.10741269731
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', 'left'), 'deadline': 14, 't': 6, 'action': None, 'reward': -4.107412697308366, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: right, reward: -20.6289149202
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 13, 't': 7, 'action': 'right', 'reward': -20.628914920241012, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.63)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': -10.314457460120506, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: forward, reward: -40.3190738129
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -40.31907381288574, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.32)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: right, reward: 1.00987210857
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.0098721085716158, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 1.01)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: 2.26043434389
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 10, 't': 10, 'action': None, 'reward': 2.260434343886333, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: left, reward: -9.58583118069
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -9.585831180694221, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent attempted driving left through a red light. (rewarded -9.59)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.631985903470279, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: right, reward: 1.15656376732
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 1.1565637673180529, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded 1.16)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.20380572605296027, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: None, reward: -4.81449061239
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': -4.814490612389047, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.81)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: forward, reward: 0.713074509158
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 0.7130745091577707, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 0.71)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: forward, reward: -10.6635536813
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -10.66355368128335, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', 'left')
Agent attempted driving forward through a red light. (rewarded -10.66)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: -19.6618273995
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'forward'), 'deadline': 4, 't': 16, 'action': 'left', 'reward': -19.661827399543924, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.66)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: -19.9847627762
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', None, 'forward'), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -19.98476277615719, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.98)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: 1.13863324885
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 1.1386332488547364, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent drove left instead of right. (rewarded 1.14)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: 0.00254631691791
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 1, 't': 19, 'action': 'left', 'reward': 0.0025463169179126277, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove left instead of right. (rewarded 0.00)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 3
\-------------------------

Environment.reset(): Trial set up with start = (7, 3), destination = (5, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: left, reward: -20.3113991916
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'right', None, 'right'), 'deadline': 20, 't': 0, 'action': 'left', 'reward': -20.311399191600167, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: left, reward: -19.7794774426
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 19, 't': 1, 'action': 'left', 'reward': -19.779477442617882, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.78)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: left, reward: 1.90867282157
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 18, 't': 2, 'action': 'left', 'reward': 1.908672821568285, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: left, reward: -9.76797472565
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -9.767974725646472, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.20380572605296027, 'right': 0.0, None: -2.4072453061945236}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: right, reward: 2.64886901182
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 2.648869011821545, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.65)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': -9.889738721308941, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: None, reward: -5.70939462509
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': -5.709394625092102, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.71)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.9543364107841426, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: right, reward: 1.62286148891
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.6228614889082584, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.62)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: left, reward: 1.30067819013
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': 1.3006781901265043, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent followed the waypoint left. (rewarded 1.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 2.55539641417
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', 'right'), 'deadline': 12, 't': 8, 'action': None, 'reward': 2.5553964141747096, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'forward', 'right')
Agent properly idled at a red light. (rewarded 2.56)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: left, reward: -9.07513261875
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -9.075132618749784, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.08)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': -4.537566309374892, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: -10.376813925
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -10.37681392499398, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.38)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: left, reward: -20.7445333052
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', None, 'right'), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -20.744533305237272, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.74)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: -4.28218279464
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 8, 't': 12, 'action': None, 'reward': -4.282182794644002, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: forward, reward: 0.725672128681
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 0.7256721286813131, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove forward instead of right. (rewarded 0.73)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.141091397322001}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: forward, reward: 0.00258158161678
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 0.002581581616783235, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded 0.00)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -5.243913803650662, 'right': 0.475883018274011, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 1.51826811932
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': 1.5182681193166223, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.52)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.20380572605296027, 'right': 1.3244345059107725, None: -2.4072453061945236}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: forward, reward: 0.104103175779
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': 0.10410317577889439, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.10)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': -9.64811268990952, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: -5.06466467896
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 3, 't': 17, 'action': None, 'reward': -5.064664678955946, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.06)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.05205158788944719, 'left': 0.20380572605296027, 'right': 1.3244345059107725, None: -2.4072453061945236}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: -4.13302364756
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': -4.133023647564288, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.13)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: left, reward: -20.5676385221
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', None, 'left'), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -20.567638522096576, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'right', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.57)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 4
\-------------------------

Environment.reset(): Trial set up with start = (5, 4), destination = (1, 6), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: left, reward: -39.7043475571
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'left'), 'deadline': 30, 't': 0, 'action': 'left', 'reward': -39.7043475570615, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.70)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: None, reward: 2.69156616907
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 29, 't': 1, 'action': None, 'reward': 2.6915661690709767, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.69)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: right, reward: 1.10029708238
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'right', 'left'), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 1.1002970823816347, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'right', 'left')
Agent drove right instead of forward. (rewarded 1.10)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: right, reward: 0.148705602375
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', 'forward'), 'deadline': 27, 't': 3, 'action': 'right', 'reward': 0.14870560237504737, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'left', 'forward')
Agent drove right instead of left. (rewarded 0.15)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.3457830845354883}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: -9.56118248984
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': -9.561182489839773, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.56)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: None, reward: 0.969106261395
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 25, 't': 5, 'action': None, 'reward': 0.9691062613953944, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.97)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: None, reward: -4.49480469232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'forward'), 'deadline': 24, 't': 6, 'action': None, 'reward': -4.494804692320116, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.49)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.38509418549493624, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: None, reward: -4.22228138896
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 23, 't': 7, 'action': None, 'reward': -4.222281388960087, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.22)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: left, reward: -20.146546739
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'right', None, 'forward'), 'deadline': 22, 't': 8, 'action': 'left', 'reward': -20.146546739027045, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.15)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: -39.9846001792
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'forward'), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': -39.98460017915087, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.98)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: 1.7210615034
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 1.7210615034047776, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent drove right instead of forward. (rewarded 1.72)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: -39.6851197082
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'forward'), 'deadline': 19, 't': 11, 'action': 'forward', 'reward': -39.685119708235355, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.69)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: right, reward: 0.999248671964
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 18, 't': 12, 'action': 'right', 'reward': 0.9992486719637087, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 1.00)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: left, reward: -9.18191378249
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 17, 't': 13, 'action': 'left', 'reward': -9.181913782492323, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.18)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: -9.79145394362
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 16, 't': 14, 'action': 'forward', 'reward': -9.791453943620954, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.79)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: -10.8742110154
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': -10.874211015361903, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.87)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': -9.889738721308941, 'right': 0.0, None: -2.854697312546051}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: right, reward: 0.915242971547
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 14, 't': 16, 'action': 'right', 'reward': 0.9152429715465855, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.92)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: None, reward: 2.22422269619
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'right', 'right'), 'deadline': 13, 't': 17, 'action': None, 'reward': 2.224222696192913, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'right', 'right')
Agent properly idled at a red light. (rewarded 2.22)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: forward, reward: -9.3187992632
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', 'right', None), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -9.318799263204884, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.32)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: forward, reward: 1.18496789621
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'left', None), 'deadline': 11, 't': 19, 'action': 'forward', 'reward': 1.1849678962136312, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'left', None)
Agent drove forward instead of left. (rewarded 1.18)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: left, reward: -9.60742830194
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 10, 't': 20, 'action': 'left', 'reward': -9.607428301939157, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.61)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.8610981120008033, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: forward, reward: 0.755115052839
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 9, 't': 21, 'action': 'forward', 'reward': 0.755115052838656, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 0.76)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 1.119386780365442, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: forward, reward: -0.462518839679
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': -0.4625188396789808, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove forward instead of left. (rewarded -0.46)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: right, reward: 0.815839046682
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 7, 't': 23, 'action': 'right', 'reward': 0.8158390466823103, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.82)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: 0.651832925401
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 6, 't': 24, 'action': 'left', 'reward': 0.651832925401346, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent drove left instead of forward. (rewarded 0.65)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: -40.8251571194
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -40.82515711944538, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.83)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 1.0846468711775872, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: forward, reward: -9.10141364852
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': -9.101413648523724, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.10)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -5.18840696249699, 'left': -4.537566309374892, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: None, reward: 2.1356276756
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 3, 't': 27, 'action': None, 'reward': 2.135627675598855, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.14)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.35653725457888535, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: None, reward: 0.259891445302
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 2, 't': 28, 'action': None, 'reward': 0.2598914453018212, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.26)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.35653725457888535, 'left': 0.0, 'right': 0.0, None: 0.1299457226509106}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: left, reward: -0.412854484415
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 1, 't': 29, 'action': 'left', 'reward': -0.41285448441481565, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded -0.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 5
\-------------------------

Environment.reset(): Trial set up with start = (1, 2), destination = (4, 5), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -5.18840696249699, 'left': -4.537566309374892, 'right': 0.0, None: 1.0678138377994275}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: right, reward: 2.19322987383
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 30, 't': 0, 'action': 'right', 'reward': 2.1932298738298686, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.19)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: right, reward: 0.19728226826
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'right', 'left'), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 0.19728226826049766, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'right', 'left')
Agent drove right instead of forward. (rewarded 0.20)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.40791952334115517, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: 1.05314233444
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 28, 't': 2, 'action': 'forward', 'reward': 1.0531423344383057, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.377557526419328, 'left': 0.8610981120008033, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: 0.978743339121
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 27, 't': 3, 'action': None, 'reward': 0.9787433391208552, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.98)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.5265711672191529, 'left': 0.0, 'right': 0.40791952334115517, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: -5.55627059688
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 26, 't': 4, 'action': None, 'reward': -5.556270596879514, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.56)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.1302171719431664}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: right, reward: 1.35153834043
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 25, 't': 5, 'action': 'right', 'reward': 1.3515383404324246, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.6757691702162123, None: 1.1302171719431664}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: -9.90605710145
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 24, 't': 6, 'action': 'left', 'reward': -9.90605710145221, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.91)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5049360542858079, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: None, reward: 2.10254813621
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 23, 't': 7, 'action': None, 'reward': 2.102548136205648, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.10)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: -10.0829355487
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 22, 't': 8, 'action': 'left', 'reward': -10.082935548710413, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -10.08)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.5265711672191529, 'left': 0.0, 'right': 0.40791952334115517, None: -2.778135298439757}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: left, reward: 1.53139781879
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 21, 't': 9, 'action': 'left', 'reward': 1.5313978187863528, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.53)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.2830064613684048, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (-1, 0), action: right, reward: 0.314453408821
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', 'left'), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 0.31445340882107664, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'forward', 'left')
Agent drove right instead of left. (rewarded 0.31)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0012731584589563139, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: right, reward: 2.00869012972
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 2.008690129723994, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.01)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -5.243913803650662, 'right': 0.475883018274011, None: 0.7591340596583112}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: forward, reward: -9.35492806038
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': -9.354928060383754, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.35)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.35653725457888535, 'left': -0.20642724220740782, 'right': 0.0, None: 0.1299457226509106}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: right, reward: 2.34426764257
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 17, 't': 13, 'action': 'right', 'reward': 2.344267642570623, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent followed the waypoint right. (rewarded 2.34)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: right, reward: 0.914489260747
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 16, 't': 14, 'action': 'right', 'reward': 0.9144892607467257, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent drove right instead of forward. (rewarded 0.91)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -4.953028550726105, 'right': 0.6757691702162123, None: 1.1302171719431664}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: -9.21444305187
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': -9.214443051873205, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.21)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: forward, reward: 0.42101505896
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 14, 't': 16, 'action': 'forward', 'reward': 0.42101505895975944, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 0.42)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: -40.5313851675
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', None, 'forward'), 'deadline': 13, 't': 17, 'action': 'left', 'reward': -40.53138516750094, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.53)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': -20.26569258375047, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: right, reward: -0.000912930079244
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'forward'), 'deadline': 12, 't': 18, 'action': 'right', 'reward': -0.0009129300792438677, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'forward')
Agent drove right instead of left. (rewarded -0.00)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: -10.906069474
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 11, 't': 19, 'action': 'forward', 'reward': -10.906069474025168, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.91)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -5.18840696249699, 'left': -4.537566309374892, 'right': 1.0966149369149343, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: -9.36826065757
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 10, 't': 20, 'action': 'forward', 'reward': -9.36826065757218, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.37)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': -9.64811268990952, 'right': 0.0, None: -2.532332339477973}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 1.14418535549
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 1.144185355485183, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 1.14)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0012907908083916175, 'left': 0.0, 'right': 0.0, None: -2.141091397322001}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: right, reward: 2.32490321457
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 8, 't': 22, 'action': 'right', 'reward': 2.3249032145685877, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent followed the waypoint right. (rewarded 2.32)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: None, reward: -4.58721630173
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 7, 't': 23, 'action': None, 'reward': -4.587216301726983, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.59)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.2936081508634913}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: 0.546742037329
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 6, 't': 24, 'action': 'right', 'reward': 0.5467420373288916, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.55)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.5265711672191529, 'left': 0.7656989093931764, 'right': 0.40791952334115517, None: -2.778135298439757}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: -5.99639298808
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 5, 't': 25, 'action': None, 'reward': -5.996392988077352, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -6.00)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.6503390950632522, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: -0.464053918509
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': -0.46405391850919586, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent drove forward instead of left. (rewarded -0.46)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 0.0, 'left': -4.803714150969578, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: right, reward: 1.17322700854
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 3, 't': 27, 'action': 'right', 'reward': 1.173227008543815, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent drove right instead of left. (rewarded 1.17)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.36283606434065657, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: 1.10096586344
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 2, 't': 28, 'action': None, 'reward': 1.1009658634413775, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent idled at a green light with oncoming traffic. (rewarded 1.10)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.0, 'left': -20.41257855972269, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: forward, reward: -39.2470102465
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 1, 't': 29, 'action': 'forward', 'reward': -39.247010246473764, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 6
\-------------------------

Environment.reset(): Trial set up with start = (7, 7), destination = (1, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: None, reward: -4.77739344773
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 20, 't': 0, 'action': None, 'reward': -4.777393447726581, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0012731584589563139, 'right': 1.004345064861997, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: forward, reward: 1.7079890106
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 1.7079890106008486, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove forward instead of right. (rewarded 1.71)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: right, reward: 2.61975927972
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', 'forward'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 2.619759279722496, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', 'forward')
Agent followed the waypoint right. (rewarded 2.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: 2.30392849811
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'left'), 'deadline': 17, 't': 3, 'action': None, 'reward': 2.303928498111782, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'left')
Agent properly idled at a red light. (rewarded 2.30)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': -4.590956891246162, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: right, reward: 1.92027378669
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.920273786687826, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.92)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: 2.82780224723
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 2.8278022472303843, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent followed the waypoint left. (rewarded 2.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: right, reward: 1.49888799885
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'left', None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.4988879988464678, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'left', None)
Agent followed the waypoint right. (rewarded 1.50)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -4.780591244919886, 'left': 0.0, 'right': 0.0, None: 1.3457830845354883}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: left, reward: -10.5973095241
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -10.597309524082428, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.60)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: forward, reward: -39.0659329048
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -39.06593290482842, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.07)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -4.780591244919886, 'left': -5.298654762041214, 'right': 0.0, None: 1.3457830845354883}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 1.71633281788
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.7163328178805275, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 1.72)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.21050752947987972, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: None, reward: -5.90529156689
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 10, 't': 10, 'action': None, 'reward': -5.905291566894472, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.91)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: left, reward: -19.8173212138
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -19.817321213823067, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'left', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.82)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (0, 1), action: left, reward: 1.16165004294
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': 1.161650042939475, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.16)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: left, reward: 1.69708601128
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 7, 't': 13, 'action': 'left', 'reward': 1.6970860112780937, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent followed the waypoint left. (rewarded 1.70)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 7
\-------------------------

Environment.reset(): Trial set up with start = (1, 7), destination = (8, 4), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 0.475883018274011, None: 0.7591340596583112}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 2.49974405634
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 20, 't': 0, 'action': None, 'reward': 2.499744056342582, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.50)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 0.475883018274011, None: 1.6294390580004465}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: right, reward: 2.33686516914
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 2.336865169136291, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.34)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.8539945053004243, 'left': 0.0012731584589563139, 'right': 1.004345064861997, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: None, reward: -5.65137382317
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 18, 't': 2, 'action': None, 'reward': -5.651373823173368, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': -10.283819261048288, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 2.81217141606
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'left'), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 2.812171416064991, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.49962433598185435, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: None, reward: 1.41882260746
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.4188226074609704, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.42)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.377557526419328, 'left': 0.8610981120008033, 'right': 0.0, None: 0.4893716695604276}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: -0.0094301672387
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 15, 't': 5, 'action': 'right', 'reward': -0.009430167238699605, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded -0.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.7494439994232339, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: 1.45419555856
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'left', None), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': 1.454195558564998, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'left', None)
Agent drove forward instead of right. (rewarded 1.45)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.9543364107841426, 'right': 0.8114307444541292, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: None, reward: -4.66884659805
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': -4.668846598046802, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.38509418549493624, 'right': 0.0, None: -2.1111406944800435}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: forward, reward: 2.17195620413
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 2.171956204127858, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 2.17)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: forward, reward: -10.2895375403
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 11, 't': 9, 'action': 'forward', 'reward': -10.289537540282199, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.29)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -39.6756953897
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'left'), 'deadline': 10, 't': 10, 'action': 'left', 'reward': -39.67569538966539, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.68)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -10.3468529689
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -10.346852968949243, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.35)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.780591244919886, 'left': -5.298654762041214, 'right': 0.8581664089402637, None: 1.3457830845354883}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -10.2236460417
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -10.223646041679489, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.22)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: right, reward: 0.380702449792
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.3807024497916477, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.38)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -20.15953690644287, 'left': 0.0, 'right': -10.314457460120506, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: left, reward: -39.227544345
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -39.227544344975, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.23)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -4.607221525936603, 'left': -4.953028550726105, 'right': 0.6757691702162123, None: 1.1302171719431664}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: -0.193271423462
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': -0.19327142346248904, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded -0.19)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': -9.64811268990952, 'right': 0.5720926777425915, None: -2.532332339477973}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: 0.455835127972
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': 0.45583512797241466, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent drove forward instead of right. (rewarded 0.46)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: None, reward: 2.09589559339
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'left', 'left'), 'deadline': 3, 't': 17, 'action': None, 'reward': 2.0958955933880183, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', 'left')
Agent properly idled at a red light. (rewarded 2.10)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.0998975479105964}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: -40.4409589025
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'forward'), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': -40.4409589024741, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.44)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.35653725457888535, 'left': -0.20642724220740782, 'right': 1.1721338212853114, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: right, reward: 0.341593229773
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 0.34159322977293227, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'left', None, None)
Agent followed the waypoint right. (rewarded 0.34)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 8
\-------------------------

Environment.reset(): Trial set up with start = (5, 6), destination = (1, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.8539945053004243, 'left': 0.0012731584589563139, 'right': 1.004345064861997, None: -2.825686911586684}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: None, reward: -4.26591794596
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 25, 't': 0, 'action': None, 'reward': -4.265917945955487, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.27)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.8539945053004243, 'left': 0.0012731584589563139, 'right': 1.004345064861997, None: -3.5458024287710854}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: left, reward: 0.153917311605
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 24, 't': 1, 'action': 'left', 'reward': 0.1539173116051722, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove left instead of right. (rewarded 0.15)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.8485430056390468, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: forward, reward: 1.67821249162
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 1.678212491619811, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove forward instead of left. (rewarded 1.68)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -4.607221525936603, 'left': -4.953028550726105, 'right': 0.24124887337686163, None: 1.1302171719431664}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: -10.3520256602
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 22, 't': 3, 'action': 'left', 'reward': -10.352025660193496, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.35)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.5265711672191529, 'left': 0.7656989093931764, 'right': 0.40791952334115517, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 2.36253352333
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': 2.3625335233263502, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.36)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: forward, reward: 2.2474748052
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': 2.247474805203571, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent followed the waypoint forward. (rewarded 2.25)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -40.6692431744
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'forward'), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -40.66924317443909, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -4.780591244919886, 'left': -7.761150401860352, 'right': 0.8581664089402637, None: 1.3457830845354883}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -10.6956023457
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 18, 't': 7, 'action': 'left', 'reward': -10.69560234573719, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.70)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -4.780591244919886, 'left': -9.22837637379877, 'right': 0.8581664089402637, None: 1.3457830845354883}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: None, reward: 1.8111250387
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': 1.8111250387025855, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.81)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.19035122489582385, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: 1.17630701257
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 1.176307012572827, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded 1.18)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: forward, reward: -40.6648678449
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', 'left'), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': -40.66486784485839, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.66)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.05205158788944719, 'left': 0.20380572605296027, 'right': 1.3244345059107725, None: -3.270134476879406}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: None, reward: -4.12031510376
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 14, 't': 11, 'action': None, 'reward': -4.120315103761534, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.05205158788944719, 'left': 0.20380572605296027, 'right': 1.3244345059107725, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: left, reward: 0.79990779413
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': 0.7999077941304129, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.36283606434065657, 'left': 0.0, 'right': 0.0, None: 0.5504829317206887}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: left, reward: 1.21026886851
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 1.21026886851363, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove left instead of right. (rewarded 1.21)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.377557526419328, 'left': 0.8610981120008033, 'right': -0.0047150836193498025, None: 0.4893716695604276}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: None, reward: 0.996393220685
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 0.9963932206850591, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.00)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: None, reward: -4.11374493543
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': -4.113744935430271, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.11)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5049360542858079, None: 1.051274068102824}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: forward, reward: -9.8311449243
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -9.831144924302205, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.83)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -4.607221525936603, 'left': -7.6525271054598, 'right': 0.24124887337686163, None: 1.1302171719431664}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: left, reward: -9.70513088195
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -9.70513088195357, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.71)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: left, reward: -9.64239691821
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', 'right'), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -9.642396918209224, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'right')
Agent attempted driving left through a red light. (rewarded -9.64)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: forward, reward: -0.293150445026
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'right'), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -0.2931504450257111, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'right')
Agent drove forward instead of left. (rewarded -0.29)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.5265711672191529, 'left': 1.5641162163597633, 'right': 0.40791952334115517, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: left, reward: 0.425477524271
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': 0.42547752427066166, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 0.43)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.1389649098260681, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: None, reward: -4.96520413496
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 4, 't': 21, 'action': None, 'reward': -4.96520413495701, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.97)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: left, reward: -0.498284063354
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -0.49828406335411346, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded -0.50)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: left, reward: 0.822526479046
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 2, 't': 23, 'action': 'left', 'reward': 0.8225264790456432, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove left instead of right. (rewarded 0.82)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: None, reward: -5.00459910628
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 1, 't': 24, 'action': None, 'reward': -5.0045991062809945, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.00)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 9
\-------------------------

Environment.reset(): Trial set up with start = (5, 6), destination = (2, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': -9.889738721308941, 'right': 0.45762148577329276, None: -2.854697312546051}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: 1.21935728439
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 1.2193572843942337, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent followed the waypoint forward. (rewarded 1.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.4572446303733628, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: None, reward: 1.35298409142
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 19, 't': 1, 'action': None, 'reward': 1.3529840914171563, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.35)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: None, reward: 2.38675595718
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'right'), 'deadline': 18, 't': 2, 'action': None, 'reward': 2.3867559571830075, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'right')
Agent properly idled at a red light. (rewarded 2.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': -4.590956891246162, 'right': 0.960136893343913, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: None, reward: 2.50754558174
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 17, 't': 3, 'action': None, 'reward': 2.5075455817351244, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.51)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.9543364107841426, 'right': 0.8114307444541292, None: -2.334423299023401}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: 1.36404863957
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 1.3640486395737952, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: -10.6212140331
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -10.621214033063277, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.62)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.4572446303733628, None: 0.6764920457085781}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: -9.825777218
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': -9.825777217998128, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.83)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: -40.7445842431
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'right'), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -40.74458424309932, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.74)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': -4.590956891246162, 'right': 0.960136893343913, None: 1.2537727908675622}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: -9.90497508449
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -9.904975084486532, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.90)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 1.64142203163
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.6414220316310304, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent drove right instead of forward. (rewarded 1.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 1.4139011236151922, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 1.68495545619
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 1.6849554561883475, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove right instead of left. (rewarded 1.68)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.4112632395228216, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 0.699868763148
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 9, 't': 11, 'action': None, 'reward': 0.6998687631475946, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.70)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.5022995531404972}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: right, reward: 2.14688567809
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 2.1468856780913645, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent followed the waypoint right. (rewarded 2.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.22791756398620733, 'left': -9.64811268990952, 'right': 0.5720926777425915, None: -2.532332339477973}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: -4.53936152207
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': -4.539361522066772, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.54)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.05205158788944719, 'left': 0.5018567600916866, 'right': 1.3244345059107725, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 2.41939089859
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 2.419390898590395, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.42)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -5.437105507680951, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: -9.80618913678
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -9.806189136782455, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.81)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 1.65973832369
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.6597383236905183, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.66)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.6096786421971169, 'left': -9.889738721308941, 'right': 0.45762148577329276, None: -2.854697312546051}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: 1.96898006661
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 1.9689800666116322, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent followed the waypoint forward. (rewarded 1.97)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -4.607221525936603, 'left': -8.678828993706684, 'right': 0.24124887337686163, None: 1.1302171719431664}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: right, reward: -0.705898593794
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 2, 't': 18, 'action': 'right', 'reward': -0.7058985937938234, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded -0.71)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: right, reward: 1.00588177806
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 1.0058817780607083, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'right', None)
Agent followed the waypoint right. (rewarded 1.01)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 10
\-------------------------

Environment.reset(): Trial set up with start = (4, 7), destination = (1, 4), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: None, reward: 1.964297817
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 30, 't': 0, 'action': None, 'reward': 1.964297816999149, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.96)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: None, reward: 0.863857616458
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', 'left'), 'deadline': 29, 't': 1, 'action': None, 'reward': 0.8638576164577644, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.86)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.4319288082288822}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, -1), action: right, reward: 1.18258587025
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', 'left'), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 1.1825858702532552, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', 'left')
Agent drove right instead of forward. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: right, reward: 1.8372211102
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 27, 't': 3, 'action': 'right', 'reward': 1.8372211101992346, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 1.84)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.6820243197868976, 'left': 0.9543364107841426, 'right': 0.8114307444541292, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: left, reward: 1.19264889711
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 26, 't': 4, 'action': 'left', 'reward': 1.1926488971122824, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.19)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: 2.21789744555
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', 'forward'), 'deadline': 25, 't': 5, 'action': None, 'reward': 2.2178974455497236, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', 'forward')
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: 2.45321419356
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 24, 't': 6, 'action': None, 'reward': 2.453214193558476, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 2.45)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.22791756398620733, 'left': -9.64811268990952, 'right': 0.5720926777425915, None: -3.5358469307723723}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: -4.8084911458
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 23, 't': 7, 'action': None, 'reward': -4.808491145798619, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.81)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.3886967238632906}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: forward, reward: 1.48650342787
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': 1.4865034278703124, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent drove forward instead of right. (rewarded 1.49)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 1.4063740937051512, None: 1.6294390580004465}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: right, reward: 1.59670502182
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 21, 't': 9, 'action': 'right', 'reward': 1.5967050218188512, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.60)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: right, reward: 0.350462133682
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 0.3504621336820619, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent drove right instead of forward. (rewarded 0.35)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -39.8270210703
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', 'forward'), 'deadline': 19, 't': 11, 'action': 'left', 'reward': -39.82702107029783, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.83)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.631985903470279, 'left': 0.0, 'right': 0.5782818836590264, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -9.29253631328
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': -9.29253631327784, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.29)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: -4.06396412251
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 17, 't': 13, 'action': None, 'reward': -4.0639641225103755, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.06)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.0319820612551878}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: -0.153261638802
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 16, 't': 14, 'action': 'right', 'reward': -0.153261638801929, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded -0.15)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 1.5015395577620012, None: 1.6294390580004465}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: 1.41643635225
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 15, 't': 15, 'action': None, 'reward': 1.4164363522507923, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.05205158788944719, 'left': 0.5018567600916866, 'right': 1.8719127022505837, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 0.435655889268
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 14, 't': 16, 'action': 'forward', 'reward': 0.43565588926753707, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.44)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.1237374026017855, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: 0.645399649094
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 13, 't': 17, 'action': 'right', 'reward': 0.6453996490935205, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent drove right instead of forward. (rewarded 0.65)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.5265711672191529, 'left': 0.9947968703152125, 'right': 0.40791952334115517, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: 1.846898149
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 12, 't': 18, 'action': 'left', 'reward': 1.8468981489979255, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.85)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.8207110158155152, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: 2.30632795923
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 11, 't': 19, 'action': 'forward', 'reward': 2.3063279592343764, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent followed the waypoint forward. (rewarded 2.31)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': -5.310607016531638, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: None, reward: 1.99307786656
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 10, 't': 20, 'action': None, 'reward': 1.9930778665591733, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.99)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.5881535062864135, 'right': 0.19035122489582385, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: forward, reward: 1.21575779036
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 9, 't': 21, 'action': 'forward', 'reward': 1.215757790363126, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 1.22)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 11
\-------------------------

Environment.reset(): Trial set up with start = (4, 6), destination = (8, 2), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: 1.5414423085
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': 1.5414423084960438, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent followed the waypoint forward. (rewarded 1.54)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -4.952487542243266, 'left': -4.590956891246162, 'right': 0.960136893343913, None: 1.2537727908675622}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: -10.3229783874
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 29, 't': 1, 'action': 'forward', 'reward': -10.322978387427279, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.32)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.637732964835273, 'left': -4.590956891246162, 'right': 0.960136893343913, None: 1.2537727908675622}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 2.86279474234
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 28, 't': 2, 'action': None, 'reward': 2.862794742335752, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.86)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -7.637732964835273, 'left': -4.590956891246162, 'right': 0.960136893343913, None: 2.058283766601657}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 0.14319457576
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 27, 't': 3, 'action': 'right', 'reward': 0.14319457576004846, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.14)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': -9.908660606911534, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: 1.15949430048
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', None), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': 1.1594943004838418, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'left', None)
Agent drove forward instead of left. (rewarded 1.16)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -4.607221525936603, 'left': -8.678828993706684, 'right': -0.23232486020848087, None: 1.1302171719431664}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 2.42582853472
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 25, 't': 5, 'action': None, 'reward': 2.425828534715362, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.43)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: -10.2760475344
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 24, 't': 6, 'action': 'forward', 'reward': -10.276047534446116, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.28)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 2.61755244939
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'right'), 'deadline': 23, 't': 7, 'action': None, 'reward': 2.617552449387153, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'right')
Agent properly idled at a red light. (rewarded 2.62)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: -9.67433467429
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': -9.674334674293997, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.67)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.377557526419328, 'left': 0.8610981120008033, 'right': -0.0047150836193498025, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: 1.38074929961
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': 1.3807492996078763, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 1.38)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.5265711672191529, 'left': 1.420847509656569, 'right': 0.40791952334115517, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: right, reward: 0.956521623803
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 0.9565216238034937, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.96)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: right, reward: 1.58627441019
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 1.5862744101921584, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.59)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: forward, reward: -40.10193504
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'left'), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': -40.10193503995498, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.10)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: left, reward: -9.05157957007
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', 'left'), 'deadline': 17, 't': 13, 'action': 'left', 'reward': -9.05157957006812, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', 'left')
Agent attempted driving left through a red light. (rewarded -9.05)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: right, reward: 2.64234196639
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 16, 't': 14, 'action': 'right', 'reward': 2.6423419663932703, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent followed the waypoint right. (rewarded 2.64)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -0.24914203167705673, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: 1.20375609163
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': 1.2037560916314527, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 1.20)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': -4.912888608999064, 'right': 0.4572446303733628, None: 0.6764920457085781}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: -9.70596269748
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 14, 't': 16, 'action': 'forward', 'reward': -9.705962697483105, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.71)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.6820243197868976, 'left': 1.0734926539482124, 'right': 0.8114307444541292, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: right, reward: 0.945485962909
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 13, 't': 17, 'action': 'right', 'reward': 0.9454859629087593, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.95)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -4.607221525936603, 'left': -8.678828993706684, 'right': -0.23232486020848087, None: 1.7780228533292641}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: right, reward: 0.51006673602
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 12, 't': 18, 'action': 'right', 'reward': 0.5100667360198077, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.51)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: None, reward: -4.90801472987
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', 'left'), 'deadline': 11, 't': 19, 'action': None, 'reward': -4.9080147298728, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.91)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.9186105550996173, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: forward, reward: 0.509356018373
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 10, 't': 20, 'action': 'forward', 'reward': 0.5093560183730019, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove forward instead of left. (rewarded 0.51)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -4.607221525936603, 'left': -8.678828993706684, 'right': 0.1388709379056634, None: 1.7780228533292641}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 1.48508814524
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 21, 'action': None, 'reward': 1.485088145236145, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.49)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: right, reward: -0.0321006692689
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 8, 't': 22, 'action': 'right', 'reward': -0.03210066926886901, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent drove right instead of left. (rewarded -0.03)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: None, reward: 1.9969950312
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', None), 'deadline': 7, 't': 23, 'action': None, 'reward': 1.996995031202108, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'left', None)
Agent properly idled at a red light. (rewarded 2.00)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -7.278333810034585, 'left': -4.537566309374892, 'right': 1.0966149369149343, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: right, reward: 0.688153941379
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 6, 't': 24, 'action': 'right', 'reward': 0.68815394137855, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 0.69)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: -40.831134201
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -40.831134200967945, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.83)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: right, reward: 0.298655765977
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 4, 't': 26, 'action': 'right', 'reward': 0.29865576597663757, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent drove right instead of forward. (rewarded 0.30)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -4.607221525936603, 'left': -8.678828993706684, 'right': 0.1388709379056634, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: -10.4168700719
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -10.416870071920975, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.42)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.5265711672191529, 'left': 1.420847509656569, 'right': 0.6822205735723245, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: forward, reward: 0.699757993203
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 2, 't': 28, 'action': 'forward', 'reward': 0.6997579932033201, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.70)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.25467800918650096, 'left': 0.0, 'right': 0.9186105550996173, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: -5.02258325757
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 1, 't': 29, 'action': None, 'reward': -5.022583257568929, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.02)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 12
\-------------------------

Environment.reset(): Trial set up with start = (4, 6), destination = (1, 3), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 2.50486854549
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', None), 'deadline': 30, 't': 0, 'action': 'right', 'reward': 2.5048685454879176, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'forward', None)
Agent followed the waypoint right. (rewarded 2.50)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: 2.26561917199
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', 'right', 'forward'), 'deadline': 29, 't': 1, 'action': 'forward', 'reward': 2.265619171993402, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'right', 'forward')
Agent followed the waypoint forward. (rewarded 2.27)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: left, reward: 1.75438172774
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', 'forward'), 'deadline': 28, 't': 2, 'action': 'left', 'reward': 1.754381727743124, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', 'forward')
Agent drove left instead of forward. (rewarded 1.75)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: right, reward: 1.79680212549
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', 'left'), 'deadline': 27, 't': 3, 'action': 'right', 'reward': 1.7968021254886255, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'left', 'left')
Agent followed the waypoint right. (rewarded 1.80)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: left, reward: 0.98256999496
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 26, 't': 4, 'action': 'left', 'reward': 0.9825699949602047, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent followed the waypoint left. (rewarded 0.98)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -4.780591244919886, 'left': -9.22837637379877, 'right': 0.8581664089402637, None: 1.5784540616190368}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: forward, reward: -10.3778337025
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 25, 't': 5, 'action': 'forward', 'reward': -10.377833702458299, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.38)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 1.6624430365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', 'right'), 'deadline': 24, 't': 6, 'action': 'right', 'reward': 1.6624430364951444, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', 'right')
Agent drove right instead of forward. (rewarded 1.66)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -0.14657522251285554, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: None, reward: -5.24817238854
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'right'), 'deadline': 23, 't': 7, 'action': None, 'reward': -5.248172388543026, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.25)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: None, reward: -4.22430864437
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', 'forward'), 'deadline': 22, 't': 8, 'action': None, 'reward': -4.224308644367861, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.22)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: None, reward: 1.23708392659
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 21, 't': 9, 'action': None, 'reward': 1.237083926585755, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent properly idled at a red light. (rewarded 1.24)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: 0.7643227854
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, 'right'), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 0.7643227854002803, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'right')
Agent drove right instead of left. (rewarded 0.76)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -19.623505123236882, 'left': -20.41257855972269, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: -20.3698457085
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 19, 't': 11, 'action': 'right', 'reward': -20.369845708540318, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: left, reward: -10.77585611
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': -10.77585611000675, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -10.78)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.24385373857849213, 'left': 0.5018567600916866, 'right': 1.8719127022505837, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: left, reward: 0.769138425069
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 17, 't': 13, 'action': 'left', 'reward': 0.7691384250689655, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.77)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: right, reward: 1.7300704809
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 16, 't': 14, 'action': 'right', 'reward': 1.7300704809047072, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.73)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -5.387928055003375, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: forward, reward: -10.5257531985
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': -10.525753198463306, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.53)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.24385373857849213, 'left': 0.635497592580326, 'right': 1.8719127022505837, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: right, reward: 1.18539852178
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 14, 't': 16, 'action': 'right', 'reward': 1.1853985217849066, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.19)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.6820243197868976, 'left': 1.0734926539482124, 'right': 0.8784583536814443, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: right, reward: 1.50593612998
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 13, 't': 17, 'action': 'right', 'reward': 1.5059361299758034, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.51)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.21050752947987972, 'left': 0.0, 'right': 0.0, None: -2.952645783447236}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: 2.39824596847
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 12, 't': 18, 'action': 'left', 'reward': 2.398245968472149, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent followed the waypoint left. (rewarded 2.40)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -5.453034737012584, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: -10.1370282241
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': -10.137028224056058, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -10.14)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -5.453034737012584, 'left': -5.068514112028029, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 1.70217425766
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 10, 't': 20, 'action': None, 'reward': 1.702174257662963, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.70)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 1.5015395577620012, None: 1.5229377051256194}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: right, reward: 0.575352944044
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 0.5753529440437986, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 0.58)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: forward, reward: -40.3846021676
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'forward', 'left'), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': -40.384602167567095, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.38)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -4.895726971810477, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 0.920777246199
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 7, 't': 23, 'action': 'right', 'reward': 0.9207772461992659, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent drove right instead of forward. (rewarded 0.92)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: None, reward: -5.57204022244
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', None), 'deadline': 6, 't': 24, 'action': None, 'reward': -5.572040222435401, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.57)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: 0.793325729694
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 5, 't': 25, 'action': 'right', 'reward': 0.793325729694258, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent drove right instead of left. (rewarded 0.79)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: 0.389081550354
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'left'), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': 0.3890815503542093, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, 'left')
Agent drove forward instead of right. (rewarded 0.39)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 0.36283606434065657, 'left': 0.605134434256815, 'right': 0.0, None: 0.5504829317206887}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: right, reward: 1.95200875274
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 3, 't': 27, 'action': 'right', 'reward': 1.9520087527364642, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent followed the waypoint right. (rewarded 1.95)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: right, reward: 0.882087836666
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 2, 't': 28, 'action': 'right', 'reward': 0.8820878366662754, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 0.88)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': -7.5792124736890925, 'left': -9.22837637379877, 'right': 0.8581664089402637, None: 1.5784540616190368}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: None, reward: 1.64274822261
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 1, 't': 29, 'action': None, 'reward': 1.6427482226067225, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.64)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 13
\-------------------------

Environment.reset(): Trial set up with start = (5, 5), destination = (3, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: left, reward: -39.2048092071
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'left', 'forward'), 'deadline': 25, 't': 0, 'action': 'left', 'reward': -39.204809207111786, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.20)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': -19.602404603555893, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: forward, reward: -39.8623911284
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'left', 'forward'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -39.86239112837675, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.86)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.0479477966940092}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: right, reward: 1.7244114948
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'left', 'left'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.7244114947969558, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', 'left')
Agent followed the waypoint right. (rewarded 1.72)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -19.53296645241421, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: 1.07166287783
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 1.0716628778348858, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent drove right instead of forward. (rewarded 1.07)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -4.631985903470279, 'left': -4.64626815663892, 'right': 0.5782818836590264, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: 2.08921985488
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 2.0892198548763954, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.09)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -4.631985903470279, 'left': -4.64626815663892, 'right': 0.5782818836590264, None: 1.0446099274381977}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: 1.1561236067
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 1.1561236066957696, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.16)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.8791534130136022, 'left': 0.8610981120008033, 'right': -0.0047150836193498025, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: 1.10889502018
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 1.1088950201842578, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 1.11)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: right, reward: 0.372570881972
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'forward'), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 0.37257088197189847, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'forward')
Agent drove right instead of left. (rewarded 0.37)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.727567566822633, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: None, reward: -4.66121383036
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'right', 'left'), 'deadline': 17, 't': 8, 'action': None, 'reward': -4.661213830360406, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.66)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.7931372050960792, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: forward, reward: -0.106475018392
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -0.10647501839195539, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove forward instead of left. (rewarded -0.11)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': -4.792915590347111, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: 1.78935551477
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.7893555147710303, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent drove right instead of left. (rewarded 1.79)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0012907908083916175, 'left': 0.0, 'right': 1.1624516072842939, None: -2.141091397322001}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: left, reward: 0.812746379946
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 0.8127463799463782, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 0.81)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.2733710186644458, None: -2.2936081508634913}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: forward, reward: 2.30623537864
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': 2.3062353786417757, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent followed the waypoint forward. (rewarded 2.31)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.151964249055891}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: right, reward: 0.357155045396
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'left'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 0.35715504539562826, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'left')
Agent drove right instead of forward. (rewarded 0.36)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -7.5120457989287885, 'left': -8.678828993706684, 'right': 0.1388709379056634, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: 1.12475180042
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.124751800419316, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.12)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.8391062458099054, 'left': 0.8485430056390468, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: None, reward: -5.80630424448
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 10, 't': 15, 'action': None, 'reward': -5.8063042444820425, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.81)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: forward, reward: -10.9411156252
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'left'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -10.941115625203558, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -10.94)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -5.138023767223058, 'left': 0.0, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: -10.2168402087
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -10.21684020869851, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.22)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -5.138023767223058, 'left': -5.108420104349255, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: -9.66222345926
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -9.662223459263407, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.66)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: -10.0743058223
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -10.074305822274843, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.07)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -5.138023767223058, 'left': -7.385321781806331, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: -10.4262283188
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -10.426228318837035, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.43)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.9940242165989299, 'left': 0.8610981120008033, 'right': -0.0047150836193498025, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: forward, reward: -0.0264027361379
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -0.02640273613793087, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded -0.03)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: left, reward: -39.2931927628
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -39.29319276283917, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.29)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.14932788298831878, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: None, reward: 0.288994286318
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 2, 't': 23, 'action': None, 'reward': 0.2889942863183639, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 0.29)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: None, reward: 1.3264919946
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 1, 't': 24, 'action': None, 'reward': 1.326491994596449, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 1.33)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 14
\-------------------------

Environment.reset(): Trial set up with start = (3, 3), destination = (7, 5), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.1237374026017855, 'left': 0.0, 'right': 0.32269982454676027, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: -5.87775472643
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 30, 't': 0, 'action': None, 'reward': -5.8777547264250245, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.88)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.1531176893208879, 'left': 0.0, 'right': 0.2733710186644458, None: -2.2936081508634913}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: left, reward: 1.30511667052
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 29, 't': 1, 'action': 'left', 'reward': 1.3051166705156536, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 1.31)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0012907908083916175, 'left': 0.4063731899731891, 'right': 1.1624516072842939, None: -2.141091397322001}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: left, reward: 0.348443556456
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 28, 't': 2, 'action': 'left', 'reward': 0.3484435564559142, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 0.35)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: 2.82393890786
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 27, 't': 3, 'action': 'forward', 'reward': 2.823938907855709, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 2.82)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': -5.310607016531638, 'right': 0.0, None: 0.9965389332795866}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: -10.3948262732
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': -10.394826273249658, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.39)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.6632459972982245}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: right, reward: 0.516372775456
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 25, 't': 5, 'action': 'right', 'reward': 0.5163727754557229, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent drove right instead of forward. (rewarded 0.52)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.6131645802112364, 'left': 1.420847509656569, 'right': 0.6822205735723245, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: left, reward: 2.90572634318
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 24, 't': 6, 'action': 'left', 'reward': 2.9057263431811524, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.91)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.6820243197868976, 'left': 1.0734926539482124, 'right': 1.192197241828624, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: 1.27123917559
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 1.2712391755883554, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.27)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: 1.37169767073
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'right'), 'deadline': 22, 't': 8, 'action': None, 'reward': 1.37169767072589, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'right')
Agent idled at a green light with oncoming traffic. (rewarded 1.37)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': -4.792915590347111, 'right': 0.8946777573855151, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: 1.90493096466
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 21, 't': 9, 'action': None, 'reward': 1.904930964658224, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 1.90)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': -4.792915590347111, 'right': 0.8946777573855151, None: 0.952465482329112}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: -10.0073612751
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': -10.007361275135807, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.01)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: left, reward: -20.5955701865
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', None), 'deadline': 19, 't': 11, 'action': 'left', 'reward': -20.595570186518433, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.60)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.5808250214697375, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: -5.32837490605
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 18, 't': 12, 'action': None, 'reward': -5.328374906047836, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: 1.94539600895
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 17, 't': 13, 'action': None, 'reward': 1.945396008945398, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.95)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -7.5120457989287885, 'left': -8.678828993706684, 'right': 0.6318113691624897, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: left, reward: -10.5669200003
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -10.566920000347343, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.57)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 1.4139011236151922, 'right': 0.8424777280941738, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (0, 1), action: forward, reward: 1.32016220667
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': 1.3201622066715117, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove forward instead of left. (rewarded 1.32)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 7), heading: (0, 1), action: None, reward: 1.89586986475
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, 'forward'), 'deadline': 14, 't': 16, 'action': None, 'reward': 1.895869864746834, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'forward')
Agent properly idled at a red light. (rewarded 1.90)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.21050752947987972, 'left': 1.1991229842360744, 'right': 0.0, None: -2.952645783447236}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: right, reward: -0.040805275982
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 13, 't': 17, 'action': 'right', 'reward': -0.040805275982043865, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove right instead of left. (rewarded -0.04)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.24385373857849213, 'left': 0.635497592580326, 'right': 1.528655612017745, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: 1.61414754954
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 12, 't': 18, 'action': 'left', 'reward': 1.6141475495412292, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.61)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.4838107402304995, 'left': 0.8610981120008033, 'right': -0.0047150836193498025, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 0.783795839031
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': 0.7837958390313258, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 0.78)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.247402346160058}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 1.30416805561
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'forward'), 'deadline': 10, 't': 20, 'action': 'right', 'reward': 1.304168055609587, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'forward')
Agent drove right instead of forward. (rewarded 1.30)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -0.2312594198394904, 'left': 1.119386780365442, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: right, reward: 0.749637972946
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 0.7496379729457275, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded 0.75)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -5.138023767223058, 'left': -8.905775050321683, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: -9.17185366088
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': -9.171853660878586, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.17)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 0.594269040399
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 7, 't': 23, 'action': None, 'reward': 0.5942690403992812, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 0.59)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: -40.5079421992
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', 'left'), 'deadline': 6, 't': 24, 'action': 'forward', 'reward': -40.507942199218895, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.51)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 0.2971345201996406}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: -10.7348320442
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 5, 't': 25, 'action': 'forward', 'reward': -10.734832044208552, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.73)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.18628544098594924, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 0.325892708749
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'forward'), 'deadline': 4, 't': 26, 'action': None, 'reward': 0.3258927087485759, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'forward')
Agent idled at a green light with oncoming traffic. (rewarded 0.33)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -0.053237509195977695, 'left': 0.0, 'right': 0.7931372050960792, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: left, reward: 0.383407621852
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 3, 't': 27, 'action': 'left', 'reward': 0.38340762185228194, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent followed the waypoint left. (rewarded 0.38)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.0, 'left': -5.0414677743552065, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: None, reward: 2.12019768993
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 2, 't': 28, 'action': None, 'reward': 2.120197689934157, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 2.12)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: forward, reward: -40.3437151994
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', 'right'), 'deadline': 1, 't': 29, 'action': 'forward', 'reward': -40.34371519940716, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'forward', 'forward', 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.34)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 15
\-------------------------

Environment.reset(): Trial set up with start = (8, 3), destination = (5, 6), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: forward, reward: -40.5944987984
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', 'right'), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -40.59449879844952, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.59)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -7.154938714050822, 'left': -8.905775050321683, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: left, reward: -10.9109594018
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 29, 't': 1, 'action': 'left', 'reward': -10.910959401767823, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.91)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.49962433598185435, None: 0.7094113037304852}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: None, reward: 2.8275208105
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 28, 't': 2, 'action': None, 'reward': 2.827520810495119, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.83)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.49962433598185435, None: 1.7684660571128021}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: right, reward: -0.0167533693913
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 27, 't': 3, 'action': 'right', 'reward': -0.016753369391342532, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded -0.02)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.8391062458099054, 'left': 0.8485430056390468, 'right': 0.0, None: -2.9031521222410213}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: right, reward: 1.54505505055
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 26, 't': 4, 'action': 'right', 'reward': 1.545055050547635, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove right instead of left. (rewarded 1.55)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.35653725457888535, 'left': -0.20642724220740782, 'right': 0.7568635255291218, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: right, reward: 1.59495371837
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 25, 't': 5, 'action': 'right', 'reward': 1.5949537183657923, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -7.637732964835273, 'left': -4.590956891246162, 'right': 0.5516657345519806, None: 2.058283766601657}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: left, reward: -9.57278194891
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 24, 't': 6, 'action': 'left', 'reward': -9.572781948908148, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.57)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.085978102063929, 'left': 0.38509418549493624, 'right': 0.0, None: -2.1111406944800435}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: right, reward: 0.267895343544
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 0.26789534354390476, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove right instead of forward. (rewarded 0.27)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -0.2312594198394904, 'left': 1.119386780365442, 'right': 0.37481898647286377, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: None, reward: -4.748953346
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 22, 't': 8, 'action': None, 'reward': -4.748953346000811, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.75)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: left, reward: -19.6268919464
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 21, 't': 9, 'action': 'left', 'reward': -19.626891946367433, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.63)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -0.2312594198394904, 'left': 1.119386780365442, 'right': 0.37481898647286377, None: -2.3744766730004057}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: left, reward: 1.02982352022
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 20, 't': 10, 'action': 'left', 'reward': 1.0298235202150778, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.03)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: left, reward: -39.4622969798
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', None), 'deadline': 19, 't': 11, 'action': 'left', 'reward': -39.46229697975038, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.46)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: forward, reward: -40.5741047708
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'right'), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': -40.57410477075781, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.57)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.607878895181563, 'left': 0.5881535062864135, 'right': 0.19035122489582385, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: None, reward: 1.59465886108
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 17, 't': 13, 'action': None, 'reward': 1.594658861084941, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.59)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: None, reward: -5.46350905652
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'left'), 'deadline': 16, 't': 14, 'action': None, 'reward': -5.463509056521685, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.46)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.9821489084995745}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: forward, reward: 2.29994606362
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': 2.299946063616301, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent followed the waypoint forward. (rewarded 2.30)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.4845531306976972}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: 2.62420696621
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 14, 't': 16, 'action': 'forward', 'reward': 2.624206966206512, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent followed the waypoint forward. (rewarded 2.62)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 1.0384462509028998, None: 1.5229377051256194}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 0.905828626185
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 13, 't': 17, 'action': None, 'reward': 0.905828626184983, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.91)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -4.677464030191877, 'left': -5.243913803650662, 'right': 1.0384462509028998, None: 1.2143831656553012}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: -10.5023351247
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -10.502335124714422, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.50)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -4.550706824261862, 'left': 0.0, 'right': 1.0846468711775872, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 0.7933513913
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 11, 't': 19, 'action': None, 'reward': 0.7933513912996335, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 0.79)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -7.589899577453149, 'left': -5.243913803650662, 'right': 1.0384462509028998, None: 1.2143831656553012}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: right, reward: 1.49909776816
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 10, 't': 20, 'action': 'right', 'reward': 1.4990977681567499, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.50)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: -39.1795939405
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 9, 't': 21, 'action': 'forward', 'reward': -39.179593940455604, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.18)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.6820243197868976, 'left': 1.0734926539482124, 'right': 1.2317182087084897, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: 2.4292581967
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': 2.4292581967001907, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 2.43)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -7.637732964835273, 'left': -7.081869420077155, 'right': 0.5516657345519806, None: 2.058283766601657}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: -9.29641035381
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 7, 't': 23, 'action': 'forward', 'reward': -9.296410353808335, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.30)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -8.467071659321803, 'left': -7.081869420077155, 'right': 0.5516657345519806, None: 2.058283766601657}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: None, reward: 1.76054888803
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 24, 'action': None, 'reward': 1.760548888028145, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 1.5556412582435442, 'left': 1.0734926539482124, 'right': 1.2317182087084897, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: left, reward: 0.106415393225
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 25, 'action': 'left', 'reward': 0.10641539322478333, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.11)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5029408890303542, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: forward, reward: -0.302861479538
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': -0.3028614795377995, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove forward instead of right. (rewarded -0.30)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 0.35653725457888535, 'left': -0.20642724220740782, 'right': 1.175908621947457, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: forward, reward: -0.281415553964
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -0.2814155539641292, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded -0.28)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -5.419444631171684, 'left': -4.574643858076623, 'right': 0.5449754345550167, None: 1.2510245465946221}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: left, reward: -9.62807865307
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 2, 't': 28, 'action': 'left', 'reward': -9.628078653074768, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.63)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: None, reward: 1.90987662572
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 1, 't': 29, 'action': None, 'reward': 1.9098766257237394, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 1.91)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 16
\-------------------------

Environment.reset(): Trial set up with start = (6, 5), destination = (4, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.8539945053004243, 'left': 0.07759523503206425, 'right': 1.004345064861997, None: -3.5458024287710854}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: 2.64233273462
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 2.6423327346197896, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.64)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -5.419444631171684, 'left': -7.1013612555756955, 'right': 0.5449754345550167, None: 1.2510245465946221}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: -9.1635742451
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -9.163574245101433, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.16)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.291509438136559, 'left': -7.1013612555756955, 'right': 0.5449754345550167, None: 1.2510245465946221}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: right, reward: 1.00943328372
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.0094332837246947, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.01)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.607878895181563, 'left': 0.5881535062864135, 'right': 0.19035122489582385, None: 0.7973294305424705}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: None, reward: 1.28226489173
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.2822648917316497, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.28)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.312103483103256, 'left': 0.0, 'right': 0.0, None: 0.4845531306976972}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: left, reward: 0.184390363514
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': 0.1843903635140841, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent drove left instead of forward. (rewarded 0.18)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: forward, reward: 1.61952311562
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'forward'), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': 1.6195231156155687, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', 'forward')
Agent drove forward instead of right. (rewarded 1.62)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -7.589899577453149, 'left': -5.243913803650662, 'right': 1.2687720095298247, None: 1.2143831656553012}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: forward, reward: -10.164470522
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.164470521989706, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.16)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.24385373857849213, 'left': 1.1248225710607775, 'right': 1.528655612017745, None: -3.69522479032047}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: None, reward: -4.87539489629
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': -4.875394896293521, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.24385373857849213, 'left': 1.1248225710607775, 'right': 1.528655612017745, None: -4.285309843306996}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: None, reward: -5.76039066975
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': -5.760390669753699, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 1.0734428390456823, None: -2.5022995531404972}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 0.289892533682
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 0.28989253368202617, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove left instead of right. (rewarded 0.29)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: right, reward: -19.0587860822
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'right'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': -19.05878608221009, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'right')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.06)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 1.3211709831966352, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: -9.12797537151
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -9.127975371507743, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.13)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -7.291509438136559, 'left': -7.1013612555756955, 'right': 0.7772043591398556, None: 1.2510245465946221}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: None, reward: 2.19768634929
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 13, 't': 12, 'action': None, 'reward': 2.1976863492941376, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.20)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.4112632395228216, 'right': 0.0, None: 0.3499343815737973}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 2.06545793612
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 2.0654579361210876, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent followed the waypoint right. (rewarded 2.07)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: None, reward: -5.39512735182
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 11, 't': 14, 'action': None, 'reward': -5.395127351824248, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.40)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.4410439183331377, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: forward, reward: -40.7941374951
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -40.79413749507105, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.79)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -20.397068747535524, 'left': 0.0, 'right': 0.4410439183331377, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: left, reward: -39.4966242628
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -39.496624262786476, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.50)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.697563675912124}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: right, reward: 2.35833921731
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 2.3583392173084965, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'right')
Agent followed the waypoint right. (rewarded 2.36)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.7707211542480219, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 0.748621615488
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 7, 't': 18, 'action': None, 'reward': 0.748621615488205, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.75)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.6018780458157263, 'left': -0.24914203167705673, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: -5.7364163933
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 6, 't': 19, 'action': None, 'reward': -5.73641639329799, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.74)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.6018780458157263, 'left': -0.24914203167705673, 'right': 0.0, None: -2.868208196648995}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: right, reward: 0.57949335769
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 0.5794933576900603, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove right instead of forward. (rewarded 0.58)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -0.2312594198394904, 'left': 1.0746051502902598, 'right': 0.37481898647286377, None: -2.3744766730004057}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: left, reward: 0.372509523445
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 4, 't': 21, 'action': 'left', 'reward': 0.37250952344512145, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 0.37)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 17
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (1, 7), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: None, reward: 1.77085915296
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 25, 't': 0, 'action': None, 'reward': 1.7708591529579987, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.77)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.972698004472699}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: right, reward: -20.4075355926
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 24, 't': 1, 'action': 'right', 'reward': -20.40753559255788, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.41)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.5120457989287885, 'left': -9.622874497027013, 'right': 0.6318113691624897, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: right, reward: 1.86000253684
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.8600025368416675, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.86)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.24385373857849213, 'left': 1.1248225710607775, 'right': 1.528655612017745, None: -5.022850256530347}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: 0.49684793394
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 0.4968479339403067, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.50)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -20.15953690644287, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: 0.962413149068
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 21, 't': 4, 'action': None, 'reward': 0.9624131490678824, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -4.915572462151102, 'left': 0.0, 'right': 0.5049360542858079, None: 1.051274068102824}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -10.7810251492
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 20, 't': 5, 'action': 'left', 'reward': -10.78102514917559, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.78)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -4.631985903470279, 'left': -4.64626815663892, 'right': 0.5782818836590264, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -10.5414056078
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.541405607809047, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.54)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': -0.0766308194009645, None: -2.0319820612551878}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: 1.71955571674
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': 1.719555716738461, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove forward instead of left. (rewarded 1.72)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.5808250214697375, 'right': 0.0, None: -2.664187453023918}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: right, reward: 1.68846640042
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.6884664004232643, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove right instead of left. (rewarded 1.69)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -7.5120457989287885, 'left': -9.622874497027013, 'right': 1.2459069530020785, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: right, reward: 0.774301693341
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 0.7743016933413756, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.77)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -19.623505123236882, 'left': -20.41257855972269, 'right': -10.184922854270159, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: 0.877441063824
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 15, 't': 10, 'action': None, 'reward': 0.8774410638238475, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 0.88)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -19.623505123236882, 'left': -20.41257855972269, 'right': -10.184922854270159, None: 0.43872053191192373}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: left, reward: -39.4770462031
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -39.47704620314441, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.48)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.453034737012584, 'left': -5.068514112028029, 'right': 0.0, None: 0.8510871288314815}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: right, reward: 1.25780198332
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 1.2578019833242107, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent followed the waypoint right. (rewarded 1.26)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -19.589796970227802, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 0.779658316986
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 12, 't': 13, 'action': None, 'reward': 0.7796583169860971, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 0.78)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -8.467071659321803, 'left': -7.081869420077155, 'right': 0.5516657345519806, None: 1.9094163273149012}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 2.53948000096
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 2.539480000962682, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -19.53296645241421, 'left': 0.0, 'right': 0.5358314389174429, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 1.40708406606
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.4070840660612591, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 1.41)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.607878895181563, 'left': 0.5881535062864135, 'right': 0.19035122489582385, None: 1.0397971611370602}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 1.5849676663
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': 1.5849676663021084, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.58)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.1499730318081505, 'left': 0.0, 'right': 0.0, None: 0.9821489084995745}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: 0.558266542651
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 0.5582665426506641, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent drove right instead of forward. (rewarded 0.56)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -7.5120457989287885, 'left': -9.622874497027013, 'right': 1.010104323171727, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: -9.63079622322
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -9.630796223221301, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.63)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.6131645802112364, 'left': 2.1632869264188606, 'right': 0.6822205735723245, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: right, reward: 0.645566368414
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 0.6455663684142764, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.65)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: -0.529221191373
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -0.5292211913727485, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', None)
Agent drove forward instead of left. (rewarded -0.53)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -8.467071659321803, 'left': -7.081869420077155, 'right': 0.5516657345519806, None: 2.2244481641387917}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: None, reward: 2.29145513933
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 2.2914551393250306, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.5556412582435442, 'left': 0.5899540235864978, 'right': 1.2317182087084897, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: -0.00520007623354
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 3, 't': 22, 'action': 'right', 'reward': -0.0052000762335395745, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded -0.01)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -0.053237509195977695, 'left': 0.19170381092614097, 'right': 0.7931372050960792, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: -5.40443378063
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 2, 't': 23, 'action': None, 'reward': -5.404433780634153, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.40)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -0.053237509195977695, 'left': 0.19170381092614097, 'right': 0.7931372050960792, None: -2.7022168903170765}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: left, reward: 1.54331633009
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 1, 't': 24, 'action': 'left', 'reward': 1.543316330090091, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, 'forward')
Agent followed the waypoint left. (rewarded 1.54)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 18
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (3, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: right, reward: 1.32234680543
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'left', None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 1.32234680543022, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'left', None)
Agent drove right instead of left. (rewarded 1.32)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.14494626684101308, 'right': 1.0734428390456823, None: -2.5022995531404972}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: forward, reward: 1.81025592494
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 1.810255924942551, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove forward instead of right. (rewarded 1.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.291509438136559, 'left': -7.1013612555756955, 'right': 0.7772043591398556, None: 1.72435544794438}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: left, reward: -10.4106505235
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': -10.410650523458198, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.41)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: None, reward: 1.74065881705
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.7406588170546808, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 1.74)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -7.291509438136559, 'left': -8.756005889516947, 'right': 0.7772043591398556, None: 1.72435544794438}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: left, reward: -9.67482753588
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -9.674827535881706, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.36283606434065657, 'left': 0.605134434256815, 'right': 0.9760043763682321, None: 0.5504829317206887}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: None, reward: -0.0332431125594
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 20, 't': 5, 'action': None, 'reward': -0.03324311255937806, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent idled at a green light with oncoming traffic. (rewarded -0.03)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.36283606434065657, 'left': 0.605134434256815, 'right': 0.9760043763682321, None: 0.25861990958065534}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: forward, reward: 0.794222431792
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 0.7942224317922163, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove forward instead of right. (rewarded 0.79)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -5.453034737012584, 'left': -5.068514112028029, 'right': 0.6289009916621053, None: 0.8510871288314815}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: -9.99879400935
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 18, 't': 7, 'action': 'left', 'reward': -9.998794009351297, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -10.00)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': -4.883987362823236, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: None, reward: 2.37578022438
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.3757802243843757, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.38)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.24385373857849213, 'left': 0.8108352525005421, 'right': 1.528655612017745, None: -5.022850256530347}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: forward, reward: 1.34565485306
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': 1.34565485305754, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 1.35)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: right, reward: 1.78501584777
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'right'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.785015847770491, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'right')
Agent followed the waypoint right. (rewarded 1.79)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: left, reward: -9.11743225434
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', 'left'), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -9.117432254336059, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', 'left')
Agent attempted driving left through a red light. (rewarded -9.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -19.53296645241421, 'left': 0.0, 'right': 0.5358314389174429, None: 0.7035420330306296}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: left, reward: -39.360179594
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 13, 't': 12, 'action': 'left', 'reward': -39.36017959399519, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.36)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.607878895181563, 'left': 0.5881535062864135, 'right': 0.19035122489582385, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: forward, reward: 1.78652234345
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 1.786522343450228, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 1.79)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: 1.51814380282
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': 1.518143802818733, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent followed the waypoint forward. (rewarded 1.52)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.5556412582435442, 'left': 0.5899540235864978, 'right': 0.6132590662374751, None: -2.334423299023401}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: None, reward: -5.36380116394
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': -5.3638011639359, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.36)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.8605307517023888, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: -9.86083595534
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -9.860835955341697, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -9.86)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: None, reward: 1.57059141944
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'right'), 'deadline': 8, 't': 17, 'action': None, 'reward': 1.5705914194415067, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', 'right')
Agent properly idled at a red light. (rewarded 1.57)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -19.589796970227802, 'left': 0.0, 'right': 0.0, None: 0.38982915849304856}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: -39.5181101818
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -39.51811018175675, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.52)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.5556412582435442, 'left': 0.5899540235864978, 'right': 0.6132590662374751, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: forward, reward: 1.38979982302
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': 1.3897998230208142, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.39)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.2830064613684048, 'left': 0.0, 'right': 0.15722670441053832, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: left, reward: -20.491800166
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', 'left'), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -20.491800166041475, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'forward', 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.49)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.6185419632928775}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: left, reward: -40.330368682
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -40.33036868195436, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.33)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -4.915572462151102, 'left': -5.390512574587795, 'right': 0.5049360542858079, None: 1.051274068102824}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: forward, reward: -10.0457978676
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': -10.045797867613972, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.05)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -8.571421011075046, 'left': -9.622874497027013, 'right': 1.010104323171727, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: forward, reward: -9.88007298843
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': -9.880072988428132, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.88)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.225746999751589, 'left': -9.622874497027013, 'right': 1.010104323171727, None: 1.6315554992827046}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: None, reward: 1.47268482062
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': None, 'reward': 1.4726848206168475, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 19
\-------------------------

Environment.reset(): Trial set up with start = (7, 7), destination = (2, 2), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -0.2312594198394904, 'left': 0.7235573368676906, 'right': 0.37481898647286377, None: -2.3744766730004057}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: None, reward: -5.66425924286
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 20, 't': 0, 'action': None, 'reward': -5.6642592428595915, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.66)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.8391062458099054, 'left': 0.8485430056390468, 'right': 0.7725275252738175, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: 0.40114434624
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 0.40114434624014017, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove forward instead of left. (rewarded 0.40)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.3087762246935766}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: left, reward: -9.48124855616
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'right'), 'deadline': 18, 't': 2, 'action': 'left', 'reward': -9.481248556157253, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'right')
Agent attempted driving left through a red light. (rewarded -9.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': -4.740624278078626, 'right': 0.0, None: 1.3087762246935766}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -10.2171312551
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'right'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -10.217131255146827, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.22)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 1.7684660571128021}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: None, reward: 1.40886123835
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.40886123835308, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.41)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -0.23202695925459793, 'left': 0.6503390950632522, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: None, reward: 0.839707589299
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 15, 't': 5, 'action': None, 'reward': 0.8397075892989583, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.5808250214697375, 'right': 0.8442332002116322, None: -2.664187453023918}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: forward, reward: 0.515110955579
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': 0.5151109555792626, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 0.52)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': -5.037152911137421, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: forward, reward: -10.3302368254
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -10.33023682537363, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.33)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 1.588663647732941}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: None, reward: 1.03250458129
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 12, 't': 8, 'action': None, 'reward': 1.0325045812854237, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.03)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4838107402304995, 'left': 0.8224469755160646, 'right': -0.0047150836193498025, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: left, reward: 1.63525694029
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': 1.6352569402900117, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 1.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.6018780458157263, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -2.868208196648995}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: forward, reward: 2.41746413357
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': 2.417464133568073, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 2.42)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.17857752269781413, None: 1.151964249055891}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: left, reward: -9.20084728237
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'left'), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -9.20084728236926, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'left')
Agent attempted driving left through a red light. (rewarded -9.20)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -4.9304179776708486, 'right': 0.8605307517023888, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: forward, reward: -10.682571227
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -10.682571227003447, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.68)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.25818638772786145, None: 0.6632459972982245}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: forward, reward: -9.21117937837
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': -9.211179378373124, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.21)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -5.197413136624829, 'left': -5.310607016531638, 'right': 0.0, None: 0.9965389332795866}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: right, reward: 0.359528694427
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 0.359528694427138, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.36)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -0.2312594198394904, 'left': 0.7235573368676906, 'right': 0.37481898647286377, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: left, reward: 0.947991161919
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 5, 't': 15, 'action': 'left', 'reward': 0.9479911619187389, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 0.95)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.225746999751589, 'left': -9.622874497027013, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: left, reward: -9.73502160756
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 16, 'action': 'left', 'reward': -9.735021607555105, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.74)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.6600811033357559, 'left': 1.4139011236151922, 'right': 0.8424777280941738, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: None, reward: -4.40421121923
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 3, 't': 17, 'action': None, 'reward': -4.404211219233974, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.40)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.0568724677151353}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: right, reward: 0.539631778584
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 0.5396317785838447, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove right instead of left. (rewarded 0.54)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -8.467071659321803, 'left': -7.081869420077155, 'right': 0.5516657345519806, None: 2.2579516517319114}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: right, reward: -0.0177552192108
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': -0.01775521921077361, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded -0.02)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 20
\-------------------------

Environment.reset(): Trial set up with start = (3, 2), destination = (8, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -20.15953690644287, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 0.4812065745339412}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: None, reward: 1.62941950763
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 20, 't': 0, 'action': None, 'reward': 1.6294195076266336, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 1.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 1.3105841145091823}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: None, reward: 2.53497084422
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': 2.534970844224032, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.53)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 1.9227774793666073}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: None, reward: 1.06501992034
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': None, 'reward': 1.0650199203387942, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -7.154938714050822, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 1.4938986998527009}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: forward, reward: -9.18840231523
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -9.188402315234239, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.19)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.685848835362945}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: forward, reward: 0.901320125867
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'right'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 0.9013201258671762, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'right')
Agent drove forward instead of left. (rewarded 0.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: forward, reward: 1.40991814532
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'right', None), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': 1.4099181453206728, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'right', None)
Agent drove forward instead of left. (rewarded 1.41)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -7.5792124736890925, 'left': -9.22837637379877, 'right': 0.8581664089402637, None: 1.6106011421128796}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: right, reward: 0.731036995414
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 0.7310369954138158, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.73)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.8597778583692305, 'left': 0.0, 'right': -0.0766308194009645, None: -2.0319820612551878}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: left, reward: -19.9014289601
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -19.901428960114863, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.90)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -0.2312594198394904, 'left': 0.8357742493932148, 'right': 0.37481898647286377, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: forward, reward: 0.814706004345
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 0.8147060043452646, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove forward instead of left. (rewarded 0.81)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.6131645802112364, 'left': 2.1632869264188606, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: left, reward: 1.50696548058
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': 1.50696548058351, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.51)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.1531176893208879, 'left': 0.6525583352578268, 'right': 0.2733710186644458, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: right, reward: 0.112078965717
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 0.112078965716638, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -40.5300366507
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'right', 'forward'), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -40.53003665068133, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.53)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.367416022104276, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 0.2971345201996406}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -9.66759078928
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -9.667590789275302, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.67)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.49128499748010235, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: 0.9892124385
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 0.9892124385002682, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent drove forward instead of left. (rewarded 0.99)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.25467800918650096, 'left': 0.0, 'right': 0.9186105550996173, None: -2.5112916287844644}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: 1.20314832243
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': 1.2031483224296664, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent followed the waypoint left. (rewarded 1.20)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.1531176893208879, 'left': 0.6525583352578268, 'right': 0.1927249921905419, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: right, reward: -0.505840353673
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': -0.5058403536731135, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.6131645802112364, 'left': 1.8351262035011853, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: 0.464258602018
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': 0.46425860201804814, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.46)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -20.253971099609448, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: right, reward: -20.1827307804
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', 'left'), 'deadline': 3, 't': 17, 'action': 'right', 'reward': -20.182730780444953, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.18)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -8.17167051464253, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 1.4938986998527009}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: None, reward: 0.488475432474
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': 0.4884754324740803, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 0.49)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -8.17167051464253, 'left': -9.908367226044753, 'right': 0.2414354832952559, None: 0.9911870661633906}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: left, reward: -9.74836811413
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -9.748368114134648, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.75)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 21
\-------------------------

Environment.reset(): Trial set up with start = (8, 4), destination = (3, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: forward, reward: 2.53551798031
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'forward'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 2.535517980306243, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'forward')
Agent followed the waypoint forward. (rewarded 2.54)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.1972006193158955, 'left': 0.5881535062864135, 'right': 0.19035122489582385, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: 1.56349338927
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 19, 't': 1, 'action': 'left', 'reward': 1.563493389273999, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: None, reward: -4.72203324037
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 18, 't': 2, 'action': None, 'reward': -4.722033240370277, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.72)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: forward, reward: 1.49176791391
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', 'forward'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 1.49176791391363, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'left', 'forward')
Agent drove forward instead of right. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.8539945053004243, 'left': 0.07759523503206425, 'right': 1.8233388997408932, None: -3.5458024287710854}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: -5.11246138503
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 16, 't': 4, 'action': None, 'reward': -5.112461385027825, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.11)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.8539945053004243, 'left': 0.07759523503206425, 'right': 1.8233388997408932, None: -4.329131906899455}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: -5.43940350059
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 15, 't': 5, 'action': None, 'reward': -5.439403500590897, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.44)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: 2.00692350884
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'left'), 'deadline': 14, 't': 6, 'action': None, 'reward': 2.006923508838852, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, 'left')
Agent properly idled at a red light. (rewarded 2.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -5.331776840641675, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: 2.66600974815
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', 'left'), 'deadline': 13, 't': 7, 'action': None, 'reward': 2.6660097481541944, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', 'left')
Agent properly idled at a red light. (rewarded 2.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: left, reward: -40.271108881
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -40.2711088810157, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.27)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: 1.60014080668
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.6001408066807343, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent properly idled at a red light. (rewarded 1.60)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: 2.10953151432
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 10, 't': 10, 'action': None, 'reward': 2.109531514317773, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent properly idled at a red light. (rewarded 2.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.22791756398620733, 'left': -9.64811268990952, 'right': 0.5720926777425915, None: -4.172169038285496}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: right, reward: 2.59244424516
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 2.5924442451601015, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.59)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -5.173426484474621, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: forward, reward: -10.3354776761
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -10.335477676053285, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.34)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.1972006193158955, 'left': 1.0758234477802062, 'right': 0.19035122489582385, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: right, reward: 0.965658987177
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.9656589871773423, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.97)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -9.91125438145
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'right'), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': -9.91125438145176, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -9.91)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -5.165118412686815, 'left': -5.037152911137421, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: 2.19098226077
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 5, 't': 15, 'action': None, 'reward': 2.190982260766103, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 2.19)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: 1.64791190688
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'right'), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.6479119068801489, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'right')
Agent properly idled at a red light. (rewarded 1.65)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.4838107402304995, 'left': 1.2288519579030381, 'right': -0.0047150836193498025, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 0.276729007205
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.27672900720547633, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 0.28)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: 0.502897682962
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', 'forward'), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': 0.502897682962256, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'forward')
Agent drove forward instead of right. (rewarded 0.50)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.8539945053004243, 'left': 0.07759523503206425, 'right': 1.8233388997408932, None: -4.884267703745176}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: -0.349826051089
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -0.3498260510888421, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove left instead of right. (rewarded -0.35)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 22
\-------------------------

Environment.reset(): Trial set up with start = (8, 5), destination = (4, 3), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.1972006193158955, 'left': 1.0758234477802062, 'right': 0.5780051060365831, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: forward, reward: 2.11316693818
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': 2.1131669381845617, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 2.11)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: None, reward: 1.82380708055
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', 'forward'), 'deadline': 29, 't': 1, 'action': None, 'reward': 1.8238070805463495, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', 'forward')
Agent idled at a green light with oncoming traffic. (rewarded 1.82)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.085978102063929, 'left': 0.38509418549493624, 'right': 0.13394767177195238, None: -2.1111406944800435}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: left, reward: 1.73031926105
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 28, 't': 2, 'action': 'left', 'reward': 1.7303192610498304, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove left instead of forward. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -7.278333810034585, 'left': -4.537566309374892, 'right': 0.8923844391467421, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: left, reward: -10.7861979729
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 27, 't': 3, 'action': 'left', 'reward': -10.786197972928383, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -10.79)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.0547657571588864}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: left, reward: -10.4933915701
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 26, 't': 4, 'action': 'left', 'reward': -10.493391570100092, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.49)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.22791756398620733, 'left': -9.64811268990952, 'right': 1.5822684614513465, None: -4.172169038285496}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: 0.0624679153176
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 25, 't': 5, 'action': 'forward', 'reward': 0.06246791531757456, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent drove forward instead of right. (rewarded 0.06)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.8539945053004243, 'left': -0.13611540802838895, 'right': 1.8233388997408932, None: -4.884267703745176}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: right, reward: 2.31600112594
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 24, 't': 6, 'action': 'right', 'reward': 2.3160011259356645, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.32)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.1531176893208879, 'left': 0.6525583352578268, 'right': -0.15655768074128582, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: forward, reward: 1.74414498541
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 23, 't': 7, 'action': 'forward', 'reward': 1.7441449854114117, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent followed the waypoint forward. (rewarded 1.74)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -19.589796970227802, 'left': -19.759055090878373, 'right': 0.0, None: 0.38982915849304856}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: right, reward: 0.319062671004
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 22, 't': 8, 'action': 'right', 'reward': 0.3190626710043686, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded 0.32)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -20.15953690644287, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 1.0553130410802873}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: None, reward: 1.81679678543
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 21, 't': 9, 'action': None, 'reward': 1.8167967854332545, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 1.82)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 0.2971345201996406}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: None, reward: 1.61579625309
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 20, 't': 10, 'action': None, 'reward': 1.6157962530857926, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 1.62)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -8.17167051464253, 'left': -9.828367670089701, 'right': 0.2414354832952559, None: 0.9911870661633906}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: right, reward: 1.53872947643
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 1.5387294764317545, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 1.54)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.794754295818016, 'left': 0.8108352525005421, 'right': 1.528655612017745, None: -5.022850256530347}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: 1.19487339428
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': 1.19487339427562, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.19)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -7.5866957556396635, 'left': -4.64626815663892, 'right': 0.5782818836590264, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: -9.6967229903
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 17, 't': 13, 'action': 'left', 'reward': -9.69672299029871, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.70)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -7.5866957556396635, 'left': -7.171495573468815, 'right': 0.5782818836590264, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: -9.38068535077
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -9.380685350773957, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.38)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.1121113480964564}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: right, reward: 0.204861784463
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'right', 'right'), 'deadline': 15, 't': 15, 'action': 'right', 'reward': 0.2048617844626316, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'right', 'right')
Agent drove right instead of left. (rewarded 0.20)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 2.68859701874
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'left'), 'deadline': 14, 't': 16, 'action': None, 'reward': 2.6885970187393613, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'left')
Agent properly idled at a red light. (rewarded 2.69)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -5.453034737012584, 'left': -7.533654060689663, 'right': 0.6289009916621053, None: 0.8510871288314815}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 0.96160584556
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 13, 't': 17, 'action': None, 'reward': 0.9616058455599479, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 0.96)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': -9.529393041105045, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: -39.0154230647
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'right'), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -39.015423064745285, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.02)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -8.877185049721428, 'left': -5.243913803650662, 'right': 1.2687720095298247, None: 1.2143831656553012}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: -10.6324875594
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': -10.632487559416148, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.63)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -5.262876599231653, 'left': -5.387928055003375, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: right, reward: 0.910019488919
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 10, 't': 20, 'action': 'right', 'reward': 0.9100194889188078, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent followed the waypoint right. (rewarded 0.91)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: right, reward: 1.42703778412
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 1.4270377841232136, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'forward', None)
Agent followed the waypoint right. (rewarded 1.43)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: forward, reward: -10.4243245335
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'right'), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': -10.42432453348418, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.42)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.6551837787502286, 'left': 1.0758234477802062, 'right': 0.5780051060365831, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: forward, reward: 1.49794364452
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 7, 't': 23, 'action': 'forward', 'reward': 1.4979436445164576, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 1.50)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -5.167738838026643, 'left': -5.173426484474621, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: None, reward: 1.38543373513
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 6, 't': 24, 'action': None, 'reward': 1.3854337351281498, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 1.39)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 1.312103483103256, 'left': 0.09219518175704206, 'right': 0.0, None: 0.4845531306976972}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, 1), action: right, reward: 0.950309400424
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 5, 't': 25, 'action': 'right', 'reward': 0.9503094004240711, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent drove right instead of forward. (rewarded 0.95)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.053706348654183}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: right, reward: 0.239140697591
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', 'left'), 'deadline': 4, 't': 26, 'action': 'right', 'reward': 0.23914069759100687, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', 'left')
Agent drove right instead of left. (rewarded 0.24)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -8.877185049721428, 'left': -7.938200681533405, 'right': 1.2687720095298247, None: 1.2143831656553012}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: forward, reward: -10.2502452965
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -10.250245296537434, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.25)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: left, reward: -9.7269890805
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', 'right'), 'deadline': 2, 't': 28, 'action': 'left', 'reward': -9.726989080501097, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', 'right')
Agent attempted driving left through a red light. (rewarded -9.73)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': -9.563715173129431, 'left': -7.938200681533405, 'right': 1.2687720095298247, None: 1.2143831656553012}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: None, reward: 1.46416096009
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 29, 'action': None, 'reward': 1.4641609600905858, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 23
\-------------------------

Environment.reset(): Trial set up with start = (6, 2), destination = (1, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -5.0036806375679035, 'left': -4.792915590347111, 'right': 0.8946777573855151, None: 0.952465482329112}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: right, reward: 0.759900687597
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 0.7599006875965637, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent drove right instead of left. (rewarded 0.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.225746999751589, 'left': -9.678948052291059, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: forward, reward: -10.9929682
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -10.992968199961147, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.8650352404523536, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: left, reward: -40.2675879066
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 18, 't': 2, 'action': 'left', 'reward': -40.267587906642405, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.27)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.396662864847129, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: left, reward: -40.8419992988
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -40.841999298803934, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.5387115911146423, 'left': 1.8351262035011853, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: left, reward: 1.52451625948
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 16, 't': 4, 'action': 'left', 'reward': 1.524516259478924, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.52)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: right, reward: 1.05684652769
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 1.0568465276887988, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent drove right instead of left. (rewarded 1.06)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.1531639796171882, 'left': 0.0, 'right': 0.8207110158155152, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: None, reward: -5.58869457362
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 14, 't': 6, 'action': None, 'reward': -5.588694573623947, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.59)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': -19.85217377853075, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: None, reward: 1.76162299924
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'left'), 'deadline': 13, 't': 7, 'action': None, 'reward': 1.7616229992352659, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 1.76)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -5.197413136624829, 'left': -5.310607016531638, 'right': 0.179764347213569, None: 0.9965389332795866}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (0, -1), action: right, reward: 1.74223777711
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 1.7422377771060833, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.74)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -8.17167051464253, 'left': -9.828367670089701, 'right': 0.8900824798635052, None: 0.9911870661633906}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: right, reward: -0.0296319116857
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': -0.029631911685685797, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded -0.03)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: left, reward: -20.5779134913
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', 'right'), 'deadline': 10, 't': 10, 'action': 'left', 'reward': -20.577913491278508, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.58)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: None, reward: -4.47999086525
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'right'), 'deadline': 9, 't': 11, 'action': None, 'reward': -4.479990865253441, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -4.48)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.44863133736615, 'left': 0.6525583352578268, 'right': -0.15655768074128582, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: left, reward: 0.125478526979
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': 0.1254785269791121, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.13)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.528655612017745, None: -5.022850256530347}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: right, reward: 2.45747622034
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 2.4574762203426093, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.46)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: forward, reward: 1.77947891724
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', 'left', None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 1.779478917241997, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'left', None)
Agent followed the waypoint forward. (rewarded 1.78)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -19.646596381419585, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: right, reward: -19.2967497745
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 5, 't': 15, 'action': 'right', 'reward': -19.29674977454831, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.30)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -7.5792124736890925, 'left': -9.22837637379877, 'right': 0.7946017021770397, None: 1.6106011421128796}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: None, reward: 1.85064836495
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.8506483649483667, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.85)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.576563711633343, 'left': 1.0758234477802062, 'right': 0.5780051060365831, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: right, reward: 0.800493897705
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.8004938977054287, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.80)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: forward, reward: 0.0722143148555
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': 0.07221431485547014, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded 0.07)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -10.109357599856368, 'left': -9.678948052291059, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: forward, reward: -9.09547224451
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': -9.09547224450944, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.10)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 24
\-------------------------

Environment.reset(): Trial set up with start = (2, 7), destination = (6, 3), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.5096710896918997, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -2.868208196648995}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: None, reward: -5.46217717833
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 30, 't': 0, 'action': None, 'reward': -5.46217717832668, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.46)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.5096710896918997, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -4.1651926874878376}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: forward, reward: 2.2449606334
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 29, 't': 1, 'action': 'forward', 'reward': 2.244960633397765, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 2.24)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.44863133736615, 'left': 0.38901843111846945, 'right': -0.15655768074128582, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: right, reward: 1.79532456178
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 1.795324561782556, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 1.80)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.21050752947987972, 'left': 1.1991229842360744, 'right': -0.020402637991021932, None: -2.952645783447236}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: left, reward: 2.73904233454
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 27, 't': 3, 'action': 'left', 'reward': 2.7390423345371113, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent followed the waypoint left. (rewarded 2.74)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -7.5792124736890925, 'left': -9.22837637379877, 'right': 0.7946017021770397, None: 1.7306247535306232}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: -9.60091600149
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': -9.600916001489017, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.60)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.576563711633343, 'left': 1.0758234477802062, 'right': 0.6892495018710059, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 0.431682186567
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 25, 't': 5, 'action': 'right', 'reward': 0.43168218656717616, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.43)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.602414922182904, 'left': -9.678948052291059, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -10.8377112706
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 24, 't': 6, 'action': 'left', 'reward': -10.837711270578453, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.84)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.602414922182904, 'left': -10.258329661434756, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: forward, reward: -10.1336214457
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 23, 't': 7, 'action': 'forward', 'reward': -10.133621445688808, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.13)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.8854295764789993}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -10.1221734371
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 22, 't': 8, 'action': 'left', 'reward': -10.122173437053807, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -10.12)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.25467800918650096, 'left': 0.6015741612148332, 'right': 0.9186105550996173, None: -2.5112916287844644}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: 2.00159247093
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 21, 't': 9, 'action': 'left', 'reward': 2.00159247092606, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent followed the waypoint left. (rewarded 2.00)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: None, reward: -5.11100527546
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', 'forward', 'left'), 'deadline': 20, 't': 10, 'action': None, 'reward': -5.111005275460612, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.11)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': -19.837847694832696, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: forward, reward: -39.4208063415
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'left'), 'deadline': 19, 't': 11, 'action': 'forward', 'reward': -39.42080634150385, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.42)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.144768770141099, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: right, reward: 0.987881375315
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 18, 't': 12, 'action': 'right', 'reward': 0.987881375314673, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent drove right instead of forward. (rewarded 0.99)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.5387115911146423, 'left': 1.6798212314900547, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: left, reward: 1.8524017423
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 17, 't': 13, 'action': 'left', 'reward': 1.8524017423013235, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.85)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 1.1791696086542482, None: -2.697563675912124}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: 0.1249426003
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 16, 't': 14, 'action': 'left', 'reward': 0.12494260029983595, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'right')
Agent drove left instead of right. (rewarded 0.12)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -7.291509438136559, 'left': -9.215416712699326, 'right': 0.7772043591398556, None: 1.72435544794438}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: 2.34536427298
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 15, 't': 15, 'action': None, 'reward': 2.345364272984554, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.35)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: 0.224996324054
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 14, 't': 16, 'action': 'forward', 'reward': 0.22499632405379644, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent drove forward instead of right. (rewarded 0.22)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -5.197413136624829, 'left': -5.310607016531638, 'right': 0.9610010621598262, None: 0.9965389332795866}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: left, reward: -10.7152201432
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 13, 't': 17, 'action': 'left', 'reward': -10.715220143241183, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.72)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -5.341285613501723, 'left': -4.9304179776708486, 'right': 0.8605307517023888, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: 2.09001624801
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 12, 't': 18, 'action': None, 'reward': 2.0900162480110556, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 2.09)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.44863133736615, 'left': 0.38901843111846945, 'right': 0.819383440520635, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: 0.937283834365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': 0.9372838343651072, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.94)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.563715173129431, 'left': -7.938200681533405, 'right': 1.2687720095298247, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: -9.7330470071
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 10, 't': 20, 'action': 'left', 'reward': -9.73304700709635, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.73)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.563715173129431, 'left': -8.835623844314878, 'right': 1.2687720095298247, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: right, reward: 1.45602309398
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 1.456023093976029, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.46)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.5822684614513465, None: -4.172169038285496}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: None, reward: -4.28633310562
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 8, 't': 22, 'action': None, 'reward': -4.286333105618203, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.29)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -7.291509438136559, 'left': -9.215416712699326, 'right': 0.7772043591398556, None: 2.0348598604644668}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: forward, reward: -10.539707268
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 7, 't': 23, 'action': 'forward', 'reward': -10.53970726803347, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.54)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.9549383128618697}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: left, reward: -39.3190716224
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 6, 't': 24, 'action': 'left', 'reward': -39.31907162239186, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.32)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.037560850307378074, 'left': -0.20642724220740782, 'right': 1.175908621947457, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -0.0924751034892
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 5, 't': 25, 'action': 'forward', 'reward': -0.09247510348921195, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded -0.09)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: left, reward: -0.396206681293
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 4, 't': 26, 'action': 'left', 'reward': -0.39620668129322967, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent drove left instead of right. (rewarded -0.40)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 0.9051279624712755, 'left': 0.14494626684101308, 'right': 1.0734428390456823, None: -2.5022995531404972}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: None, reward: -5.14592122137
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 3, 't': 27, 'action': None, 'reward': -5.145921221372634, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.15)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.9051279624712755, 'left': 0.14494626684101308, 'right': 1.0734428390456823, None: -3.8241103872565656}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (0, 1), action: right, reward: 0.446550808937
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 2, 't': 28, 'action': 'right', 'reward': 0.4465508089374626, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent followed the waypoint right. (rewarded 0.45)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.9930659161801771, None: -5.022850256530347}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (0, 1), action: None, reward: -5.90426331784
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 1, 't': 29, 'action': None, 'reward': -5.904263317841136, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.90)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 25
\-------------------------

Environment.reset(): Trial set up with start = (3, 6), destination = (6, 4), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: None, reward: 2.39708012373
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 25, 't': 0, 'action': None, 'reward': 2.397080123728118, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 2.40)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': -5.061086718526903, 'right': 0.0, None: 0.8854295764789993}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: right, reward: 1.45315810445
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 1.4531581044541997, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent drove right instead of left. (rewarded 1.45)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.17523106684103096, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: left, reward: 1.6603442599
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': 1.6603442599032763, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent drove left instead of forward. (rewarded 1.66)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.9051279624712755, 'left': 0.14494626684101308, 'right': 0.7599968239915724, None: -3.8241103872565656}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: None, reward: -5.81052780663
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 22, 't': 3, 'action': None, 'reward': -5.810527806625386, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.4112632395228216, 'right': 1.0327289680605438, None: 0.3499343815737973}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: 1.62353288126
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 1.6235328812584058, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove forward instead of right. (rewarded 1.62)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -8.915608353085014, 'left': -9.215416712699326, 'right': 0.7772043591398556, None: 2.0348598604644668}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: None, reward: 2.72023405947
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 2.720234059469357, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.72)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -8.915608353085014, 'left': -9.215416712699326, 'right': 0.7772043591398556, None: 2.377546959966912}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: -9.0461670401
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -9.046167040100908, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.05)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -0.02745712659091694, 'left': -0.20642724220740782, 'right': 1.175908621947457, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: right, reward: 1.69444201554
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.694442015543407, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.69)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.8773158615448324, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -4.1651926874878376}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: None, reward: -4.74663696171
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 17, 't': 8, 'action': None, 'reward': -4.746636961714929, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.75)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.8773158615448324, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -4.455914824601383}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: forward, reward: 2.78859171297
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': 2.788591712973721, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 2.79)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': -4.558716127168029, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: 1.25672353216
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', 'left'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.256723532155143, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', 'left')
Agent drove right instead of forward. (rewarded 1.26)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': -5.0414677743552065, 'right': 0.0, None: 1.0600988449670785}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: right, reward: 0.289164127637
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 0.28916412763660027, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent drove right instead of left. (rewarded 0.29)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.5693166244273682, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: -4.56252396112
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 13, 't': 12, 'action': None, 'reward': -4.562523961115797, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.56)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 1.309879639861248, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: -40.9575430859
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'right', 'forward'), 'deadline': 12, 't': 13, 'action': 'left', 'reward': -40.957543085882584, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.96)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.563715173129431, 'left': -8.835623844314878, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: -10.3902023993
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'left', 'reward': -10.390202399256415, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.39)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.563715173129431, 'left': -9.612913121785645, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: forward, reward: -9.28070671618
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -9.280706716183287, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.28)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -0.02745712659091694, 'left': -0.20642724220740782, 'right': 1.435175318745432, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: -0.215638098429
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -0.21563809842913273, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded -0.22)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0012907908083916175, 'left': 0.3774083732145517, 'right': 1.1624516072842939, None: -2.141091397322001}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: None, reward: -5.99008297116
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 8, 't': 17, 'action': None, 'reward': -5.990082971157685, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.99)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.8925079238852455, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: 0.256711305964
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'right'), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': 0.25671130596433, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'right')
Agent drove forward instead of right. (rewarded 0.26)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -8.467071659321803, 'left': -7.081869420077155, 'right': 0.2669552576706035, None: 2.2579516517319114}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: -9.26854377478
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -9.26854377477835, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.27)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -8.590064237589054, 'left': -9.22837637379877, 'right': 0.7946017021770397, None: 1.7306247535306232}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: None, reward: 1.65715673125
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': None, 'reward': 1.6571567312472866, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.66)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.576563711633343, 'left': 1.0758234477802062, 'right': 0.5604658442190911, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: left, reward: -0.165059154928
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -0.16505915492779244, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded -0.17)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.5822684614513465, None: -4.2292510719518495}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: right, reward: 0.428484032558
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 3, 't': 22, 'action': 'right', 'reward': 0.4284840325578321, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 0.43)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.1531639796171882, 'left': 0.0, 'right': 0.8207110158155152, None: -2.7943472868119734}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: left, reward: 1.09943897599
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 2, 't': 23, 'action': 'left', 'reward': 1.0994389759878627, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent drove left instead of forward. (rewarded 1.10)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.062471300149917974, 'right': 1.1791696086542482, None: -2.697563675912124}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: 0.231320571281
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': 0.23132057128136063, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, 'right')
Agent drove forward instead of right. (rewarded 0.23)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 26
\-------------------------

Environment.reset(): Trial set up with start = (1, 5), destination = (5, 4), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -5.144768770141099, 'left': 0.0, 'right': 0.4939406876573365, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 2.18915638295
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 25, 't': 0, 'action': None, 'reward': 2.189156382954463, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.19)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -8.590064237589054, 'left': -9.22837637379877, 'right': 0.7946017021770397, None: 1.6938907423889549}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: -10.5210478442
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -10.52104784416327, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.52)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -19.53296645241421, 'left': -19.680089796997596, 'right': 0.5358314389174429, None: 0.7035420330306296}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 2.02512696643
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.0251269664328424, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 2.03)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.167738838026643, 'left': -5.173426484474621, 'right': 0.0, None: 0.6927168675640749}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: right, reward: 0.248107391607
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 0.24810739160717832, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent drove right instead of forward. (rewarded 0.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.8714150576196961}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: right, reward: 0.215374623553
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 0.2153746235526407, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent drove right instead of left. (rewarded 0.22)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.4727205406321793, 'left': 0.5899540235864978, 'right': 0.6132590662374751, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: forward, reward: 1.06313984774
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': 1.0631398477376068, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.06)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -5.197413136624829, 'left': -8.01291357988641, 'right': 0.9610010621598262, None: 0.9965389332795866}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: left, reward: -9.29622347434
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -9.296223474336255, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.30)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -8.867807717050077, 'left': -7.081869420077155, 'right': 0.2669552576706035, None: 2.2579516517319114}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, 1), action: right, reward: 0.859940208296
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 0.859940208296452, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.86)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.2917232922528871, 'left': 0.8357742493932148, 'right': 0.37481898647286377, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: left, reward: 2.15458625081
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 2.1545862508093343, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.15)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.267930194184893, 'left': 0.5899540235864978, 'right': 0.6132590662374751, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: forward, reward: 2.67575715622
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': 2.6757571562169997, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 2.68)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': -0.016050334634434504, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: forward, reward: 1.45123425351
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 1.4512342535117941, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent drove forward instead of left. (rewarded 1.45)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: -5.88926926693
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 14, 't': 11, 'action': None, 'reward': -5.889269266929158, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.89)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.944634633464579}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: 0.515461369344
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 0.5154613693436032, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 0.52)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': -4.5639876857538715, 'right': 1.3211709831966352, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: -9.95044118843
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': -9.950441188425755, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.95)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -8.980887696592962, 'left': -9.215416712699326, 'right': 0.7772043591398556, None: 2.377546959966912}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: left, reward: -10.0356203498
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 11, 't': 14, 'action': 'left', 'reward': -10.035620349783201, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.04)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -0.12154761251002483, 'left': -0.20642724220740782, 'right': 1.435175318745432, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: 1.55860458473
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 10, 't': 15, 'action': 'left', 'reward': 1.5586045847347136, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.56)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': -20.133793953321202, 'right': 0.8650352404523536, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: forward, reward: -39.8850625383
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -39.88506253831355, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.89)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -7.480685164882537, 'left': -5.390512574587795, 'right': 0.5049360542858079, None: 1.051274068102824}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: -10.1908238276
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -10.190823827594668, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.19)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.868018183935856, 'left': -10.258329661434756, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: -9.04794018212
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -9.047940182116516, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.05)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.5387115911146423, 'left': 1.766111486895689, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (0, -1), action: left, reward: 1.43389355068
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 1.4338935506760004, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.43)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5284232638443994, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: left, reward: 0.717516344488
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 5, 't': 20, 'action': 'left', 'reward': 0.7175163444878012, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent followed the waypoint left. (rewarded 0.72)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -8.867807717050077, 'left': -7.081869420077155, 'right': 0.5634477329835278, None: 2.2579516517319114}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: None, reward: 2.18573135809
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 2.185731358085177, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.9718436752009465, 'left': 0.5899540235864978, 'right': 0.6132590662374751, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: 0.512762493604
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 0.5127624936037227, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 0.51)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0012907908083916175, 'left': 0.3774083732145517, 'right': 1.1624516072842939, None: -4.065587184239843}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 0.657071910686
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': 0.6570719106862071, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded 0.66)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': -4.883987362823236, 'right': 0.0, None: 1.1878901121921879}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: -10.4588678632
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -10.458867863196726, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.46)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 27
\-------------------------

Environment.reset(): Trial set up with start = (3, 7), destination = (8, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.1389649098260681, 'left': 0.0, 'right': 0.0, None: -2.482602067478505}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: right, reward: 1.38183174517
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 1.3818317451706637, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent drove right instead of forward. (rewarded 1.38)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -8.17167051464253, 'left': -9.828367670089701, 'right': 0.4302252840889097, None: 0.9911870661633906}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: forward, reward: -9.1325788423
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -9.132578842297116, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.13)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 0.9564653866427166}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: None, reward: 2.20477317518
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.2047731751775554, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 1.580619280910136}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: None, reward: 2.10384873263
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.10384873262853, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.10)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -8.652124678469823, 'left': -9.828367670089701, 'right': 0.4302252840889097, None: 0.9911870661633906}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: None, reward: 1.82846464674
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 1.8284646467421377, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -0.23202695925459793, 'left': 0.6503390950632522, 'right': 0.0, None: 0.41985379464947914}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: right, reward: 0.746682517944
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 0.746682517944048, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent drove right instead of left. (rewarded 0.75)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.2575554777896313, 'left': 0.5808250214697375, 'right': 0.8442332002116322, None: -2.664187453023918}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: left, reward: 1.66130232746
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 1.6613023274638086, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.66)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': -9.813445973183716, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: -4.13839819966
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 18, 't': 7, 'action': None, 'reward': -4.138398199660531, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.2917232922528871, 'left': 1.4951802501012745, 'right': 0.37481898647286377, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: right, reward: 1.65577826564
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.6557782656389297, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded 1.66)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 0.0, None: 0.8298691618452592}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: right, reward: 1.7915821007
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 1.791582100702145, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 1.79)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.6600811033357559, 'left': 1.4139011236151922, 'right': 0.8424777280941738, None: -2.202105609616987}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: forward, reward: 0.791923294034
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 0.791923294034432, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove forward instead of left. (rewarded 0.79)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.2577306846718016, None: -2.944634633464579}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: forward, reward: 1.36296615558
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.3629661555847394, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove forward instead of left. (rewarded 1.36)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: forward, reward: 0.135346136921
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', 'forward'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': 0.13534613692072095, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', 'forward')
Agent drove forward instead of left. (rewarded 0.14)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.6814830777923697, 'left': 0.0, 'right': 0.2577306846718016, None: -2.944634633464579}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: left, reward: 2.69810323561
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 2.6981032356105077, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent followed the waypoint left. (rewarded 2.70)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -8.867807717050077, 'left': -7.081869420077155, 'right': 0.5634477329835278, None: 2.2218415049085443}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: forward, reward: -10.8340495595
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -10.834049559540587, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.83)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.7590719014093665, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: None, reward: -5.51641256032
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 10, 't': 15, 'action': None, 'reward': -5.516412560319456, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.2423030844023346, 'left': 0.5899540235864978, 'right': 0.6132590662374751, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: 0.924418710476
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 0.9244187104759868, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.92)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: -39.1052554121
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -39.10525541206739, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.11)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': -19.552627706033697, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: None, reward: 1.87390165489
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 7, 't': 18, 'action': None, 'reward': 1.873901654894353, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 1.87)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': -19.552627706033697, 'right': 0.0, None: 0.9369508274471765}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: forward, reward: -39.6895782711
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -39.68957827112225, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.69)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -7.5866957556396635, 'left': -8.276090462121386, 'right': 0.5782818836590264, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: forward, reward: -9.88267112221
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -9.882671122209164, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.88)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.8597778583692305, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -2.0319820612551878}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: None, reward: -5.99448533428
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': -5.9944853342815545, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.99)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.5387115911146423, 'left': 1.6000025187858449, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: 0.798754315622
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 0.7987543156220738, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.80)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -8.734683438924414, 'left': -8.276090462121386, 'right': 0.5782818836590264, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 0.330136473795
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.3301364737952226, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.33)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.8539945053004243, 'left': -0.13611540802838895, 'right': 2.069670012838279, None: -4.884267703745176}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: right, reward: 1.54965457898
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 1.5496545789755611, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.55)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 28
\-------------------------

Environment.reset(): Trial set up with start = (8, 4), destination = (5, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -4.837167337146998, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: right, reward: 1.3407470273
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.3407470273019078, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent drove right instead of left. (rewarded 1.34)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.2917232922528871, 'left': 1.4951802501012745, 'right': 1.0152986260558967, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: forward, reward: 0.664956159457
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 0.6649561594567267, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove forward instead of left. (rewarded 0.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.1531639796171882, 'left': 0.5497194879939313, 'right': 0.8207110158155152, None: -2.7943472868119734}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: left, reward: 1.79400635915
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 18, 't': 2, 'action': 'left', 'reward': 1.7940063591538906, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent drove left instead of forward. (rewarded 1.79)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.8539945053004243, 'left': -0.13611540802838895, 'right': 1.80966229590692, None: -4.884267703745176}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: None, reward: -5.28385603609
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 17, 't': 3, 'action': None, 'reward': -5.283856036087354, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.19454077517710466, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: None, reward: -4.4901106443
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'left'), 'deadline': 16, 't': 4, 'action': None, 'reward': -4.490110644295237, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -5.229433931598363, 'left': -4.883987362823236, 'right': 0.0, None: 1.1878901121921879}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: right, reward: 2.364431368
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 2.364431367995728, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.36)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.7590719014093665, 'left': 0.0, 'right': 0.0, None: -2.758206280159728}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: left, reward: -0.0790742557088
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 14, 't': 6, 'action': 'left', 'reward': -0.07907425570880489, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent drove left instead of forward. (rewarded -0.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.0053762470045893, None: -4.2292510719518495}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: -4.34259992232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': -4.342599922318771, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.34)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.7432517139351562, 'left': 0.0, 'right': 0.0, None: -2.3886967238632906}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: left, reward: -19.3148212113
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -19.314821211262302, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.31)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.9930659161801771, None: -5.463556787185741}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: -5.85792654463
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 11, 't': 9, 'action': None, 'reward': -5.8579265446306925, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.86)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -4.550706824261862, 'left': 0.0, 'right': 1.0846468711775872, None: 0.39667569564981675}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: left, reward: -10.1981606558
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 10, 't': 10, 'action': 'left', 'reward': -10.19816065584943, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'right')
Agent attempted driving left through a red light. (rewarded -10.20)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -7.278333810034585, 'left': -7.661882141151637, 'right': 0.8923844391467421, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: left, reward: -9.67222712489
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -9.672227124894514, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.67)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -7.278333810034585, 'left': -8.667054633023076, 'right': 0.8923844391467421, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: forward, reward: -10.7973118864
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -10.79731188639855, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.80)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.0053762470045893, None: -4.28592549713531}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: right, reward: 2.17855958215
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 2.1785595821454766, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.18)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.555556040876162, 'left': -9.22837637379877, 'right': 0.7946017021770397, None: 1.6938907423889549}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: left, reward: -10.0442286928
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -10.044228692756622, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.04)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.576563711633343, 'left': 0.4553821464262069, 'right': 0.5604658442190911, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: left, reward: 0.85817897755
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': 0.8581789775495012, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded 0.86)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.5785292480664365, 'left': 0.605134434256815, 'right': 0.9760043763682321, None: 0.25861990958065534}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: None, reward: 0.16721231801
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 4, 't': 16, 'action': None, 'reward': 0.16721231800973113, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.17)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: right, reward: 0.95906397785
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.9590639778495813, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent followed the waypoint right. (rewarded 0.96)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.8000704033403672}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: left, reward: -40.5579982349
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': -40.557998234892054, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.56)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': -20.13555444050785, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: right, reward: 1.04709282965
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 1.0470928296455317, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'right', None, None)
Agent followed the waypoint right. (rewarded 1.05)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 29
\-------------------------

Environment.reset(): Trial set up with start = (8, 3), destination = (3, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.18628544098594924, None: 0.16294635437428795}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: 2.65726797713
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'forward'), 'deadline': 25, 't': 0, 'action': 'left', 'reward': 2.657267977132866, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'forward')
Agent followed the waypoint left. (rewarded 2.66)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -8.734683438924414, 'left': -8.276090462121386, 'right': 0.4542091787271245, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: -10.9938552449
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 24, 't': 1, 'action': 'left', 'reward': -10.993855244936075, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -10.99)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -5.165118412686815, 'left': -5.037152911137421, 'right': 0.0, None: 1.0954911303830515}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: right, reward: 0.785803534693
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 0.7858035346926535, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent drove right instead of left. (rewarded 0.79)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.25467800918650096, 'left': 1.3015833160704466, 'right': 0.9186105550996173, None: -2.5112916287844644}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: -5.04995771422
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 22, 't': 3, 'action': None, 'reward': -5.049957714216319, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.05)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': -20.16518434097718, 'right': 0.0, None: 0.6185419632928775}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: forward, reward: -40.3218961065
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': -40.32189610645593, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.32)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.868018183935856, 'left': -9.653134921775635, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: -10.7713143334
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': -10.771314333442971, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.77)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.868018183935856, 'left': -10.212224627609302, 'right': 1.010104323171727, None: 1.552120159949776}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: 2.34239540265
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': None, 'reward': 2.342395402647247, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.668732953368358, 'left': 1.6000025187858449, 'right': 0.6638934709933004, None: -4.387264143258554}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: -5.40838112448
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': -5.408381124476151, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.8597778583692305, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -4.013233697768371}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: forward, reward: 0.563538703526
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': 0.5635387035260547, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove forward instead of left. (rewarded 0.56)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.2423030844023346, 'left': 0.5899540235864978, 'right': 0.768838888356731, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: 0.256016750662
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 0.25601675066200835, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.26)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -20.05096751997749, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: right, reward: -19.0170750752
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'left'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': -19.0170750752067, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.02)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -8.980887696592962, 'left': -9.625518531241264, 'right': 0.7772043591398556, None: 2.377546959966912}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: 2.14114038123
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 14, 't': 11, 'action': None, 'reward': 2.141140381230067, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.14)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -8.980887696592962, 'left': -9.625518531241264, 'right': 0.7772043591398556, None: 2.2593436705984895}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -10.4218517264
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': -10.42185172642532, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.42)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -0.12154761251002483, 'left': 0.6760886712636529, 'right': 1.435175318745432, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 2.4372672575
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 2.43726725750156, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent followed the waypoint right. (rewarded 2.44)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.555556040876162, 'left': -9.636302533277696, 'right': 0.7946017021770397, None: 1.6938907423889549}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: left, reward: -9.03842028719
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 11, 't': 14, 'action': 'left', 'reward': -9.038420287188984, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.04)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -20.287052385378907, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: -20.3708414432
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'right'), 'deadline': 10, 't': 15, 'action': 'right', 'reward': -20.370841443211958, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', 'right')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.37)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.877190863871562, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 1.81037636574
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', 'forward'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': 1.8103763657383274, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', 'forward')
Agent followed the waypoint forward. (rewarded 1.81)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.085978102063929, 'left': 1.0577067232723834, 'right': 0.13394767177195238, None: -2.1111406944800435}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: -0.375310108754
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 8, 't': 17, 'action': 'right', 'reward': -0.3753101087536681, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove right instead of forward. (rewarded -0.38)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -7.480685164882537, 'left': -7.790668201091232, 'right': 0.5049360542858079, None: 1.051274068102824}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: -9.09951743409
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -9.099517434088495, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.10)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.868018183935856, 'left': -10.212224627609302, 'right': 1.010104323171727, None: 1.9472577812985115}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: 0.814701722379
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 6, 't': 19, 'action': None, 'reward': 0.8147017223794633, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.81)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.868018183935856, 'left': -10.212224627609302, 'right': 1.010104323171727, None: 1.3809797518389875}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: -9.16899639909
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -9.168996399091647, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.17)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.03610715742773507, 'left': 0.0, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: -20.8782087902
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -20.878208790224377, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.88)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.7256171267558971, 'left': 0.0, 'right': -0.016050334634434504, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: 0.930319877306
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': 0.9303198773064825, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent followed the waypoint left. (rewarded 0.93)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: right, reward: -0.48609440917
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, 'left'), 'deadline': 2, 't': 23, 'action': 'right', 'reward': -0.4860944091697226, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'left')
Agent drove right instead of left. (rewarded -0.49)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.42221094465636, 'left': -9.612913121785645, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: -9.59015900589
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -9.590159005893499, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.59)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 30
\-------------------------

Environment.reset(): Trial set up with start = (4, 3), destination = (7, 6), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: forward, reward: 1.27903598273
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': 1.279035982732565, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent followed the waypoint forward. (rewarded 1.28)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.4119694539278544, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: right, reward: 1.83887678563
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 1.8388767856293282, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.84)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.518507291513751, 'left': -10.212224627609302, 'right': 1.010104323171727, None: 1.3809797518389875}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: None, reward: 2.46796444017
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 28, 't': 2, 'action': None, 'reward': 2.4679644401661243, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.518507291513751, 'left': -10.212224627609302, 'right': 1.010104323171727, None: 1.9244720960025559}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: None, reward: 1.16683835428
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 27, 't': 3, 'action': None, 'reward': 1.1668383542756042, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.518507291513751, 'left': -10.212224627609302, 'right': 1.010104323171727, None: 1.54565522513908}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: 0.297688911115
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 26, 't': 4, 'action': 'right', 'reward': 0.2976889111153017, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.30)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -8.652124678469823, 'left': -9.828367670089701, 'right': 0.4302252840889097, None: 1.4098258564527641}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: right, reward: 0.126685384252
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 25, 't': 5, 'action': 'right', 'reward': 0.1266853842521246, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.13)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.9930659161801771, None: -5.660741665908217}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: -5.15250434641
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 24, 't': 6, 'action': None, 'reward': -5.152504346405343, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.15)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.50618497527493, 'left': -9.612913121785645, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: -9.35611070386
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 23, 't': 7, 'action': 'forward', 'reward': -9.356110703855947, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.36)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: 2.30767431145
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'right'), 'deadline': 22, 't': 8, 'action': None, 'reward': 2.307674311452038, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'right')
Agent properly idled at a red light. (rewarded 2.31)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.153837155726019}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: right, reward: 1.39811016717
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'right'), 'deadline': 21, 't': 9, 'action': 'right', 'reward': 1.3981101671673875, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'right')
Agent followed the waypoint right. (rewarded 1.40)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.555556040876162, 'left': -9.33736141023334, 'right': 0.7946017021770397, None: 1.6938907423889549}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: forward, reward: -10.0142257205
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': -10.014225720499349, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.01)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.576563711633343, 'left': 0.6567805619878541, 'right': 0.5604658442190911, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: right, reward: 0.414535931179
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 0.4145359311786906, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.41)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.518507291513751, 'left': -10.212224627609302, 'right': 0.6538966171435143, None: 1.54565522513908}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: forward, reward: -9.87479745056
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': -9.874797450558706, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.87)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.69665237103623, 'left': -10.212224627609302, 'right': 0.6538966171435143, None: 1.54565522513908}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: left, reward: -10.8458530098
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 17, 't': 13, 'action': 'left', 'reward': -10.845853009845852, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.85)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.69665237103623, 'left': -10.529038818727578, 'right': 0.6538966171435143, None: 1.54565522513908}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: 1.15567148196
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 14, 'action': 'right', 'reward': 1.1556714819559568, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.16)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -19.844789135561125, 'left': -19.552627706033697, 'right': 0.0, None: 0.9369508274471765}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: -20.049022717
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 15, 't': 15, 'action': 'right', 'reward': -20.04902271702737, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.05)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.7116582809476426, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -4.013233697768371}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: None, reward: -4.4217041979
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 14, 't': 16, 'action': None, 'reward': -4.421704197904098, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.42)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.2575554777896313, 'left': 1.121063674466773, 'right': 0.8442332002116322, None: -2.664187453023918}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: 0.750931907803
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 13, 't': 17, 'action': 'left', 'reward': 0.7509319078025036, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 0.75)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': -5.061086718526903, 'right': 0.7265790522270998, None: 0.8854295764789993}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: forward, reward: -10.8750278755
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -10.875027875542555, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.88)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -4.9249023521925785, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: right, reward: 0.209963011181
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 11, 't': 19, 'action': 'right', 'reward': 0.2099630111809493, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent drove right instead of left. (rewarded 0.21)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': -10.155699595800083, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: forward, reward: 1.83182918488
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, 'right'), 'deadline': 10, 't': 20, 'action': 'forward', 'reward': 1.831829184876475, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, 'right')
Agent followed the waypoint forward. (rewarded 1.83)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.6395179913662825, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: left, reward: -20.450359449
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 9, 't': 21, 'action': 'left', 'reward': -20.450359448973078, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.45)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.2423030844023346, 'left': 0.4229853871242531, 'right': 0.768838888356731, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: 1.49188859799
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': 1.491888597988334, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.49)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.1237374026017855, 'left': 0.0, 'right': 0.32269982454676027, None: -2.9388773632125123}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: -19.3175817426
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 7, 't': 23, 'action': 'left', 'reward': -19.317581742617975, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.32)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.44863133736615, 'left': 0.6631511327417883, 'right': 0.819383440520635, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: 0.0584688366312
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 6, 't': 24, 'action': 'left', 'reward': 0.05846883663115188, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.06)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: 1.00907136092
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 5, 't': 25, 'action': None, 'reward': 1.0090713609195894, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent properly idled at a red light. (rewarded 1.01)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: -39.3677187797
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'left', None), 'deadline': 4, 't': 26, 'action': 'left', 'reward': -39.36771877973979, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'left', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.37)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -5.262876599231653, 'left': -5.387928055003375, 'right': 0.4550097444594039, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: 2.0433350434
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 3, 't': 27, 'action': None, 'reward': 2.0433350433972377, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 2.04)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.9930659161801771, None: -5.40662300615678}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: -5.25678062119
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 2, 't': 28, 'action': None, 'reward': -5.256780621192183, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.26)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.128355652982165, 'left': 0.0, 'right': 0.8925079238852455, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: -5.90965520398
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'right'), 'deadline': 1, 't': 29, 'action': None, 'reward': -5.909655203976117, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'forward', 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.91)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 31
\-------------------------

Environment.reset(): Trial set up with start = (5, 7), destination = (1, 3), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.1237374026017855, 'left': -9.658790871308987, 'right': 0.32269982454676027, None: -2.9388773632125123}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: None, reward: -4.93492142832
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 30, 't': 0, 'action': None, 'reward': -4.934921428321091, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.2399954326267206}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: right, reward: 1.95173149963
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'right'), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 1.9517314996334143, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'right')
Agent drove right instead of forward. (rewarded 1.95)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.6201252960250228, 'left': 0.8485430056390468, 'right': 0.7725275252738175, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: right, reward: 0.762446517575
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 0.7624465175750758, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove right instead of left. (rewarded 0.76)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.850928638295333, 'left': -7.081869420077155, 'right': 0.5634477329835278, None: 2.2218415049085443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: left, reward: -10.0563223738
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 27, 't': 3, 'action': 'left', 'reward': -10.056322373813444, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.06)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.850928638295333, 'left': -8.5690958969453, 'right': 0.5634477329835278, None: 2.2218415049085443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: left, reward: -9.15653695005
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 26, 't': 4, 'action': 'left', 'reward': -9.156536950051494, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.16)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: right, reward: 0.0792504570365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', 'left'), 'deadline': 25, 't': 5, 'action': 'right', 'reward': 0.07925045703646505, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', 'left')
Agent drove right instead of forward. (rewarded 0.08)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.69665237103623, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.54565522513908}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: None, reward: 2.32642167356
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 24, 't': 6, 'action': None, 'reward': 2.326421673556176, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.726002198685094, 'left': 1.4139011236151922, 'right': 0.8424777280941738, None: -2.202105609616987}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: None, reward: -5.89079465915
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 23, 't': 7, 'action': None, 'reward': -5.89079465914928, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.89)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.4783397258548069, 'left': 1.4951802501012745, 'right': 1.0152986260558967, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: right, reward: 1.61567734762
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 22, 't': 8, 'action': 'right', 'reward': 1.6156773476179476, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded 1.62)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: left, reward: -20.3434320659
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 21, 't': 9, 'action': 'left', 'reward': -20.34343206594815, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.34)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.4119694539278544, 'left': 0.0, 'right': 0.9194383928146641, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: left, reward: 0.507135645822
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 20, 't': 10, 'action': 'left', 'reward': 0.5071356458218814, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded 0.51)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -0.12154761251002483, 'left': 0.6760886712636529, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: forward, reward: 1.58483537598
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 19, 't': 11, 'action': 'forward', 'reward': 1.5848353759767848, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.58)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: right, reward: 1.06085163944
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 18, 't': 12, 'action': 'right', 'reward': 1.0608516394355632, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent followed the waypoint right. (rewarded 1.06)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.2893293544043745, 'left': -9.889738721308941, 'right': 0.45762148577329276, None: -2.854697312546051}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: -4.42489253104
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 17, 't': 13, 'action': None, 'reward': -4.424892531037463, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.42)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 2.3329537872592767, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -4.455914824601383}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: -4.95000681272
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 16, 't': 14, 'action': None, 'reward': -4.950006812720783, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.95)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 2.3329537872592767, 'left': -0.24914203167705673, 'right': 0.28974667884503014, None: -4.702960818661083}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: left, reward: 1.05092227506
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 15, 't': 15, 'action': 'left', 'reward': 1.05092227506402, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.05)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -19.623505123236882, 'left': -29.94481238143355, 'right': -10.184922854270159, None: 0.43872053191192373}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: None, reward: 2.62883951689
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 14, 't': 16, 'action': None, 'reward': 2.6288395168939465, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.63)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -0.15143073976889976, 'left': 0.0, 'right': 0.5029408890303542, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: left, reward: 0.75609429106
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 13, 't': 17, 'action': 'left', 'reward': 0.7560942910598372, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove left instead of right. (rewarded 0.76)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.9930659161801771, None: -5.331701813674481}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: None, reward: -4.97822618166
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 12, 't': 18, 'action': None, 'reward': -4.978226181656335, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.98)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.794754295818016, 'left': 1.0028543233880811, 'right': 1.9930659161801771, None: -5.1549639976654085}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: left, reward: 1.27771229799
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': 1.2777122979907514, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.28)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -8.652124678469823, 'left': -9.828367670089701, 'right': 0.27845533417051715, None: 1.4098258564527641}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: left, reward: -10.8338916705
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 10, 't': 20, 'action': 'left', 'reward': -10.833891670514745, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.83)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -8.652124678469823, 'left': -10.331129670302223, 'right': 0.27845533417051715, None: 1.4098258564527641}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: right, reward: 0.294783252782
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 0.2947832527816917, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.29)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.3670958411953342, 'left': 0.4229853871242531, 'right': 0.768838888356731, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: forward, reward: 1.9770629461
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': 1.9770629460965101, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: -0.0843737991295
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'right'), 'deadline': 7, 't': 23, 'action': 'right', 'reward': -0.08437379912953913, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'right')
Agent drove right instead of forward. (rewarded -0.08)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -8.734683438924414, 'left': -9.63497285352873, 'right': 0.4542091787271245, None: 1.1003667670669837}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 0.448046430835
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 6, 't': 24, 'action': None, 'reward': 0.44804643083516327, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 0.45)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.029518038351977}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: left, reward: -20.6263534828
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'forward'), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -20.62635348282727, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.63)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -0.053237509195977695, 'left': 0.867510070508116, 'right': 0.7931372050960792, None: -2.7022168903170765}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: 1.18527321665
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': 1.1852732166468438, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove forward instead of left. (rewarded 1.19)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -9.69665237103623, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.936038449347628}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: -9.52290365619
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -9.522903656194973, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.52)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.668732953368358, 'left': 1.6000025187858449, 'right': 0.6638934709933004, None: -4.897822633867353}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: forward, reward: 0.183696010606
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 2, 't': 28, 'action': 'forward', 'reward': 0.1836960106062412, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.18)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.4262144819872996, 'left': 1.6000025187858449, 'right': 0.6638934709933004, None: -4.897822633867353}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: right, reward: 1.06776714791
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 29, 'action': 'right', 'reward': 1.0677671479132393, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 1.07)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 32
\-------------------------

Environment.reset(): Trial set up with start = (2, 7), destination = (6, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 2.3329537872592767, 'left': 0.4008901216934816, 'right': 0.28974667884503014, None: -4.702960818661083}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: forward, reward: 1.63682310195
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': 1.636823101945831, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 1.64)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: left, reward: -39.8128836387
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'forward'), 'deadline': 24, 't': 1, 'action': 'left', 'reward': -39.812883638739564, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.850928638295333, 'left': -8.862816423498398, 'right': 0.5634477329835278, None: 2.2218415049085443}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: right, reward: 1.89811529191
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.898115291907036, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.90)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.609778013615601, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.936038449347628}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 1.52355451867
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.523554518665139, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.4262144819872996, 'left': 1.6000025187858449, 'right': 0.8658303094532699, None: -4.897822633867353}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: -5.85482596852
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': -5.854825968521834, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.85)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.4783397258548069, 'left': 1.4951802501012745, 'right': 1.315487986836922, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: right, reward: 0.495411427432
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 0.49541142743206434, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded 0.50)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.431147839565439, 'left': -9.612913121785645, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: -9.12934860803
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -9.129348608025719, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.13)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.794754295818016, 'left': 1.1402833106894161, 'right': 1.9930659161801771, None: -5.1549639976654085}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: 1.53451035748
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 1.5345103574782868, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.53)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.25467800918650096, 'left': 1.3015833160704466, 'right': 0.9186105550996173, None: -3.7806246715003917}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: 2.59984743574
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 2.5998474357441315, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent followed the waypoint left. (rewarded 2.60)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -19.589796970227802, 'left': -19.759055090878373, 'right': 0.1595313355021843, None: 0.38982915849304856}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: -39.3116808599
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -39.31168085991792, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.31)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 0.8957910503510725, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: right, reward: 1.46136424666
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.4613642466599175, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 1.46)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -19.844789135561125, 'left': -19.552627706033697, 'right': -10.024511358513685, None: 0.9369508274471765}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: None, reward: 1.433968678
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 14, 't': 11, 'action': None, 'reward': 1.4339686779995628, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 1.43)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.9249023521925785, 'left': 0.0, 'right': 0.10498150559047464, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: None, reward: 0.853782375746
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 13, 't': 12, 'action': None, 'reward': 0.853782375746243, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent properly idled at a red light. (rewarded 0.85)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.7116582809476426, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -4.217468947836235}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: 0.241704973219
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 0.24170497321854878, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove forward instead of left. (rewarded 0.24)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.609778013615601, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.7297964840063833}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: -9.79547854115
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -9.79547854115306, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.80)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.702628277384331, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.7297964840063833}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: None, reward: 1.7599844783
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.7599844782990757, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.4262144819872996, 'left': 1.6000025187858449, 'right': 0.8658303094532699, None: -5.376324301194593}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: None, reward: -5.23308099232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -5.233080992322713, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.23)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.4838107402304995, 'left': 1.2288519579030381, 'right': 0.13600696179306326, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: right, reward: -0.2034810735
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 8, 't': 17, 'action': 'right', 'reward': -0.20348107350026234, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded -0.20)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.947934932373417}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: right, reward: 0.986393216623
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, 'forward'), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 0.9863932166229366, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'forward')
Agent drove right instead of left. (rewarded 0.99)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -20.397068747535524, 'left': -19.748312131393238, 'right': 0.4410439183331377, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: 0.750362750659
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 6, 't': 19, 'action': None, 'reward': 0.7503627506593484, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 0.75)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.431147839565439, 'left': -9.371130864905682, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: -10.1035961509
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -10.1035961509116, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.10)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.794754295818016, 'left': 1.3373968340838513, 'right': 1.9930659161801771, None: -5.1549639976654085}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: -5.57526044621
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': -5.575260446212528, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.58)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.794754295818016, 'left': 1.3373968340838513, 'right': 1.9930659161801771, None: -5.365112221938968}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: left, reward: 0.69089863013
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 3, 't': 22, 'action': 'left', 'reward': 0.6908986301301825, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.69)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -5.144768770141099, 'left': 0.0, 'right': 0.4939406876573365, None: 1.0945781914772315}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: left, reward: -10.6332418059
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -10.633241805922252, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -10.63)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.784890880687755, 'left': -9.33736141023334, 'right': 0.7946017021770397, None: 1.6938907423889549}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: None, reward: 0.876987605379
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 1, 't': 24, 'action': None, 'reward': 0.8769876053794847, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 0.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 33
\-------------------------

Environment.reset(): Trial set up with start = (3, 6), destination = (1, 4), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -19.931195564188375, 'left': -19.602404603555893, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: right, reward: 1.04790198863
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'left', 'forward'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.04790198862529, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.05)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.085978102063929, 'left': 1.0577067232723834, 'right': -0.12068121849085786, None: -2.1111406944800435}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: None, reward: -4.42428575041
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 19, 't': 1, 'action': None, 'reward': -4.424285750414859, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.42)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.325916462700673, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: forward, reward: 2.14616399373
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': 2.1461639937272685, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent followed the waypoint forward. (rewarded 2.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.47953198892479065, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: 0.567488631655
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 0.5674886316549869, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent drove forward instead of right. (rewarded 0.57)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.8539945053004243, 'left': -0.13611540802838895, 'right': 1.80966229590692, None: -5.084061869916265}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (-1, 0), action: forward, reward: 0.881197191987
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 0.8811971919866951, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove forward instead of right. (rewarded 0.88)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.037822848216567, 'left': -8.667054633023076, 'right': 0.8923844391467421, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (-1, 0), action: forward, reward: -10.8355048955
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': -10.835504895527958, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.936663871872263, 'left': -8.667054633023076, 'right': 0.8923844391467421, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (0, -1), action: right, reward: 2.85099734605
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 2.8509973460472278, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.85)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.76737199523852, 'left': -9.371130864905682, 'right': 1.362397551752927, None: 1.3392720628729435}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 5), heading: (0, -1), action: None, reward: 2.42142315412
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': 2.4214231541171216, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.42)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -5.262876599231653, 'left': -5.387928055003375, 'right': 0.4550097444594039, None: 1.0216675216986189}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: right, reward: 1.66624186021
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 1.6662418602080538, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent followed the waypoint right. (rewarded 1.67)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.7707211542480219, 'left': 0.0, 'right': 0.0, None: 0.3743108077441025}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: left, reward: 0.4151926632
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': 0.41519266319983705, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent drove left instead of forward. (rewarded 0.42)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.73164388173338, 'left': 0.6760886712636529, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: -0.110251827748
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': -0.11025182774753672, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded -0.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -19.623505123236882, 'left': -29.94481238143355, 'right': -10.184922854270159, None: 1.5337800244029351}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: 2.6542174753
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 9, 't': 11, 'action': None, 'reward': 2.6542174752969854, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.65)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.76737199523852, 'left': -9.371130864905682, 'right': 1.362397551752927, None: 1.8803476084950326}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: -9.5364284466
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -9.536428446596299, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.54)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.591967914575033, None: -4.28592549713531}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: -4.60271054315
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': -4.602710543149559, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.60)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -0.15143073976889976, 'left': 0.3780471455299186, 'right': 0.5029408890303542, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: -4.54469551714
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 6, 't': 14, 'action': None, 'reward': -4.544695517141343, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.54)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.794754295818016, 'left': 1.014147732107017, 'right': 1.9930659161801771, None: -5.365112221938968}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: -4.90195274133
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': -4.90195274133378, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.90)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.76737199523852, 'left': -9.45377965575099, 'right': 1.362397551752927, None: 1.8803476084950326}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: right, reward: 1.40430024648
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 1.40430024647686, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.40)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.73164388173338, 'left': 0.2829184217580581, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: left, reward: 0.441091287329
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': 0.4410912873285636, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.44)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.4838107402304995, 'left': 1.2288519579030381, 'right': -0.03373705585359954, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: 1.37004124641
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 1.3700412464051264, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 1.37)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.702628277384331, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: forward, reward: -9.66259694814
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': -9.662596948137416, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.66)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 34
\-------------------------

Environment.reset(): Trial set up with start = (3, 5), destination = (1, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: forward, reward: -39.255821977
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'right', 'left', 'forward'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -39.25582197697062, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.682612612760874, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: forward, reward: -10.4395471585
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -10.43954715854567, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.44)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.061079885653271, 'left': -10.529038818727578, 'right': 0.9047840495497356, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 1.45498979843
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 1.4549897984265456, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.8675958486435598, 'left': -0.13611540802838895, 'right': 1.80966229590692, None: -5.084061869916265}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: right, reward: 1.19353739651
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 1.1935373965101408, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: left, reward: -40.8001154749
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 16, 't': 4, 'action': 'left', 'reward': -40.80011547489217, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.80)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': -20.400057737446087, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: forward, reward: -39.6713115933
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': -39.671311593302036, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.67)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -5.229433931598363, 'left': -4.883987362823236, 'right': 1.182215683997864, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 1.03308741769
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.0330874176920366, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.03)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -5.341285613501723, 'left': -4.9304179776708486, 'right': 0.8605307517023888, None: 1.0450081240055278}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 0.813361929993
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 0.813361929992557, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent drove right instead of forward. (rewarded 0.81)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.7256171267558971, 'left': 0.46515993865324123, 'right': -0.016050334634434504, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: None, reward: -5.19543788011
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 12, 't': 8, 'action': None, 'reward': -5.195437880113841, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.20)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -10.061079885653271, 'left': -10.529038818727578, 'right': 1.1798869239881407, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: forward, reward: -9.70613354464
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 11, 't': 9, 'action': 'forward', 'reward': -9.706133544641002, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.71)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.883606715147137, 'left': -10.529038818727578, 'right': 1.1798869239881407, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: forward, reward: -9.72210817007
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -9.722108170072948, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.72)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.4766816270830957, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -4.217468947836235}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: None, reward: -5.54847780831
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 9, 't': 11, 'action': None, 'reward': -5.548477808312746, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.4262144819872996, 'left': 1.6000025187858449, 'right': 0.8658303094532699, None: -5.304702646758653}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 0.899454891685
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.8994548916851282, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.90)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.5045356804597947}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: right, reward: 0.765359201994
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.7653592019937312, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 0.77)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 1.97889605977
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', 'left'), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 1.9788960597745588, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'left')
Agent followed the waypoint right. (rewarded 1.98)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.784890880687755, 'left': -9.33736141023334, 'right': 0.7946017021770397, None: 1.2854391738842197}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: -10.7236114844
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -10.72361148436396, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.72)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.784890880687755, 'left': -10.03048644729865, 'right': 0.7946017021770397, None: 1.2854391738842197}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 0.785050111197
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 0.7850501111972619, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 0.79)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.576563711633343, 'left': 0.6567805619878541, 'right': 0.48750088769889083, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: left, reward: -0.485666084672
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -0.48566608467181316, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded -0.49)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -0.15143073976889976, 'left': 0.3780471455299186, 'right': 0.5029408890303542, None: -2.2723477585706715}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: forward, reward: 0.537525341889
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': 0.5375253418890449, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove forward instead of right. (rewarded 0.54)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.5693166244273682, 'right': 0.0, None: -2.2812619805578986}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: forward, reward: 0.151604882616
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': 0.15160488261578087, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, 'forward')
Agent drove forward instead of right. (rewarded 0.15)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 35
\-------------------------

Environment.reset(): Trial set up with start = (1, 2), destination = (6, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.4119694539278544, 'left': 0.2535678229109407, 'right': 0.9194383928146641, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: None, reward: -5.71380019398
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 25, 't': 0, 'action': None, 'reward': -5.713800193977289, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.71)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.1531639796171882, 'left': 1.171862923573911, 'right': 0.8207110158155152, None: -2.7943472868119734}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: right, reward: 0.461422813294
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 0.46142281329397816, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent drove right instead of forward. (rewarded 0.46)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.11957034879550343, None: -2.053706348654183}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: left, reward: -19.1848030611
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', 'left'), 'deadline': 23, 't': 2, 'action': 'left', 'reward': -19.184803061103857, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.18)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.25467800918650096, 'left': 1.950715375907289, 'right': 0.9186105550996173, None: -3.7806246715003917}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: right, reward: 1.59299660957
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 1.5929966095677361, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 1.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.3587581722439006, 'right': 0.5284232638443994, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: 0.794438218715
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 0.7944382187154325, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent drove forward instead of left. (rewarded 0.79)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.784890880687755, 'left': -10.03048644729865, 'right': 0.7946017021770397, None: 1.0352446425407407}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: right, reward: 1.32261411827
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 1.3226141182655193, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 1.32)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.802857442610042, 'left': -10.529038818727578, 'right': 1.1798869239881407, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: right, reward: 1.24209512794
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'right', 'reward': 1.2420951279424108, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.24)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.1499730318081505, 'left': 0.0, 'right': 0.27913327132533206, None: 0.9821489084995745}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: left, reward: 0.543018375989
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 0.5430183759891292, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent drove left instead of forward. (rewarded 0.54)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.76737199523852, 'left': -9.45377965575099, 'right': 1.3833488991148934, None: 1.8803476084950326}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -9.01686153439
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -9.016861534393753, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.02)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.794754295818016, 'left': 1.014147732107017, 'right': 1.9930659161801771, None: -5.133532481636374}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: 1.31288650531
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': 1.3128865053076872, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 1.31)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.3291813507472994, 'left': 0.3774083732145517, 'right': 1.1624516072842939, None: -4.065587184239843}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: left, reward: 1.59970183702
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 15, 't': 10, 'action': 'left', 'reward': 1.5997018370181832, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 1.60)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.44863133736615, 'left': 0.3608099846864701, 'right': 0.819383440520635, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, 1), action: right, reward: 0.49071590387
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 0.4907159038697845, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.49)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.0036806375679035, 'left': -4.792915590347111, 'right': 0.8272892224910394, None: 0.952465482329112}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: right, reward: 0.587611912053
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 0.5876119120532217, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent drove right instead of left. (rewarded 0.59)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.6720793936459222, 'left': 0.4229853871242531, 'right': 0.768838888356731, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: 2.40271004199
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 2.4027100419886116, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 2.40)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.6395179913662825, 'left': -10.225179724486539, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: -5.37525009617
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': -5.375250096174745, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.38)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 2.037394717817267, 'left': 0.4229853871242531, 'right': 0.768838888356731, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: 0.775664615087
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': 'left', 'reward': 0.7756646150865474, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.78)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.73164388173338, 'left': 0.3620048545433109, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: -0.227557729675
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -0.2275577296750929, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded -0.23)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -5.229433931598363, 'left': -4.883987362823236, 'right': 1.1076515508449503, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: right, reward: 1.03833804037
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 1.038338040368066, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.04)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.07580244130789043, 'left': 0.5693166244273682, 'right': 0.0, None: -2.2812619805578986}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: right, reward: 0.831426425464
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 0.831426425464006, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 0.83)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -29.45073891507286, 'left': -19.759055090878373, 'right': 0.1595313355021843, None: 0.38982915849304856}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: right, reward: 0.299545203786
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 0.29954520378575766, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded 0.30)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -8.290101299485517, 'left': -7.790668201091232, 'right': 0.5049360542858079, None: 1.051274068102824}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 1.09918916867
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 5, 't': 20, 'action': None, 'reward': 1.0991891686688184, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.10)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.802857442610042, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 1.7448904811527295}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 2.17862113742
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 2.1786211374220263, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.03610715742773507, 'left': -10.439104395112189, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: forward, reward: 0.14072537332
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 0.14072537332028356, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded 0.14)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.8239559534400744}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: left, reward: -9.10303263675
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'right'), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -9.103032636747729, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'right')
Agent attempted driving left through a red light. (rewarded -9.10)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -19.842559854117678, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: right, reward: 1.02754796406
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'forward'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 1.0275479640602105, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'left', 'left', 'forward')
Agent drove right instead of left. (rewarded 1.03)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 36
\-------------------------

Environment.reset(): Trial set up with start = (1, 6), destination = (7, 3), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 1.842234006769333}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 2.95392005349
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 25, 't': 0, 'action': None, 'reward': 2.953920053493875, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.95)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -8.652124678469823, 'left': -10.331129670302223, 'right': 0.28661929347610443, None: 1.4098258564527641}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: -10.926885219
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 24, 't': 1, 'action': 'left', 'reward': -10.92688521899573, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.93)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -5.165118412686815, 'left': -5.037152911137421, 'right': 0.39290176734632676, None: 1.0954911303830515}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: -10.8228584537
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': -10.822858453650861, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.82)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -8.652124678469823, 'left': -10.629007444648977, 'right': 0.28661929347610443, None: 1.4098258564527641}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 2.11570683184
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.1157068318350616, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.12)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': -0.03373705585359954, None: 0.7428824451227434}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 1.39709128116
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 1.3970912811636165, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': -0.03373705585359954, None: 1.06998686314318}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 0.220419534433
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 0.22041953443342843, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4262144819872996, 'left': 1.6000025187858449, 'right': 0.882642600569199, None: -5.304702646758653}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: left, reward: 2.76389413914
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 2.763894139135024, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 2.037394717817267, 'left': 0.5993250011054002, 'right': 0.768838888356731, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 1.1401721481
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.1401721481006797, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.14)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.25467800918650096, 'left': 1.950715375907289, 'right': 1.2558035823336766, None: -3.7806246715003917}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: 1.92603288494
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 1.9260328849449118, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent followed the waypoint left. (rewarded 1.93)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.3291813507472994, 'left': 0.9885551051163675, 'right': 1.1624516072842939, None: -4.065587184239843}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: right, reward: 1.18304277713
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 1.1830427771303134, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent followed the waypoint right. (rewarded 1.18)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.576563711633343, 'left': 0.08555723865802045, 'right': 0.48750088769889083, None: 1.3123824137195843}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: None, reward: 1.74714228741
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 15, 't': 10, 'action': None, 'reward': 1.7471422874113696, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.75)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: left, reward: 0.420923647447
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'right'), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 0.42092364744732447, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'right')
Agent drove left instead of forward. (rewarded 0.42)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -0.0971468694787
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 13, 't': 12, 'action': 'left', 'reward': -0.0971468694787494, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent drove left instead of right. (rewarded -0.10)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.4262144819872996, 'left': 2.1819483289604342, 'right': 0.882642600569199, None: -5.304702646758653}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: -5.76464701487
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': None, 'reward': -5.7646470148660764, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.76)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.4783397258548069, 'left': 1.4951802501012745, 'right': 0.9054497071344931, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 1.48519717981
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.4851971798087296, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded 1.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: 0.850018183315
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 10, 't': 15, 'action': None, 'reward': 0.8500181833151315, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent properly idled at a red light. (rewarded 0.85)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.73164388173338, 'left': 0.06722356243410899, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 0.960246641849
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': 0.960246641848907, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 0.96)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.0538204005628518, 'left': 1.014147732107017, 'right': 1.9930659161801771, None: -5.133532481636374}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: 1.20752362155
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 1.2075236215462402, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.21)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': -20.13555444050785, 'right': 0.5235464148227659, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: 2.13451136241
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 7, 't': 18, 'action': None, 'reward': 2.134511362407302, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 2.13)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.936663871872263, 'left': -8.667054633023076, 'right': 1.8716908925969848, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: -10.086654721
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -10.086654721047546, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.09)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -8.980887696592962, 'left': -10.023685128833293, 'right': 0.7772043591398556, None: 2.2593436705984895}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: 0.8509108664
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': None, 'reward': 0.8509108663996121, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 0.85)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.8459452617911435, 'left': 0.06722356243410899, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: 1.12528333754
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': 1.1252833375421059, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.13)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.9856142996666246, 'left': 0.06722356243410899, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 2), heading: (0, -1), action: forward, reward: 1.07610693546
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 1.07610693546186, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.08)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.3610166201851386}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: right, reward: 0.876803409101
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.8768034091012311, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 0.88)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.784890880687755, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.0352446425407407}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: None, reward: 1.20938127204
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 1, 't': 24, 'action': None, 'reward': 1.2093812720393786, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.21)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 37
\-------------------------

Environment.reset(): Trial set up with start = (3, 6), destination = (7, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.4262144819872996, 'left': 2.1819483289604342, 'right': 0.882642600569199, None: -5.5346748308123646}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: right, reward: 0.416279699037
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 0.416279699037406, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.42)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: forward, reward: 1.42785525972
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', 'left'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 1.427855259722402, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'left', 'left')
Agent drove forward instead of right. (rewarded 1.43)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.43840170455061556, None: -2.3610166201851386}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: forward, reward: 0.381261948841
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 0.3812619488405262, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove forward instead of right. (rewarded 0.38)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.0308606175642423, 'left': 0.06722356243410899, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: forward, reward: 1.16398244444
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': 1.1639824444382878, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.392116764816137, 'left': -9.45377965575099, 'right': 1.3833488991148934, None: 1.8803476084950326}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: right, reward: 1.36122591691
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 1.3612259169135463, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.36)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.850928638295333, 'left': -8.862816423498398, 'right': 1.2307815124452819, None: 2.2218415049085443}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: None, reward: 2.61130489988
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 2.61130489988068, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.850928638295333, 'left': -8.862816423498398, 'right': 1.2307815124452819, None: 2.416573202394612}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: left, reward: -10.9232462144
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -10.923246214369755, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.44863133736615, 'left': 0.3608099846864701, 'right': 0.6550496721952097, None: -2.2936081508634913}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: None, reward: -5.51273406674
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 18, 't': 7, 'action': None, 'reward': -5.512734066736207, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.51)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.44863133736615, 'left': 0.3608099846864701, 'right': 0.6550496721952097, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: 0.423358337458
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 0.4233583374581483, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.42)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': -0.0485734347393747, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: forward, reward: 1.5997386713
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': 1.5997386712988084, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent drove forward instead of right. (rewarded 1.60)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.097421531001265, 'left': 0.06722356243410899, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: 1.14243724931
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 1.142437249312215, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.14)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.392116764816137, 'left': -9.45377965575099, 'right': 1.3722874080142198, None: 1.8803476084950326}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: None, reward: 2.75364939708
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 14, 't': 11, 'action': None, 'reward': 2.7536493970825555, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -10.011659296459904, 'left': -8.667054633023076, 'right': 1.8716908925969848, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -10.2149498477
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -10.214949847654218, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.21)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': -5.246695785050046, 'right': 0.0, None: 1.0547657571588864}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: right, reward: 1.55326845949
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 1.553268459490703, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent followed the waypoint right. (rewarded 1.55)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -19.53296645241421, 'left': -19.680089796997596, 'right': 0.5358314389174429, None: 1.364334499731736}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: right, reward: 0.465696120436
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 0.4656961204360973, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent drove right instead of forward. (rewarded 0.47)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -8.734683438924414, 'left': -9.63497285352873, 'right': 0.4542091787271245, None: 0.7742065989510735}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: -9.1258076211
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 10, 't': 15, 'action': 'left', 'reward': -9.125807621095852, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.13)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.4766816270830957, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -4.882973378074491}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: -4.13165898201
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -4.131658982012988, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.13)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.4262144819872996, 'left': 2.1819483289604342, 'right': 0.6494611498033025, None: -5.5346748308123646}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: forward, reward: 0.400875044449
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': 0.4008750444491471, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.40)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.4783397258548069, 'left': 1.4951802501012745, 'right': 1.1953234434716113, None: -4.019367957929998}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: None, reward: -4.8860011789
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 7, 't': 18, 'action': None, 'reward': -4.886001178904376, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.89)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.4783397258548069, 'left': 1.4951802501012745, 'right': 1.1953234434716113, None: -4.452684568417187}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (-1, 0), action: left, reward: 1.31542262196
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 1.315422621956113, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.32)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 2.037394717817267, 'left': 0.5993250011054002, 'right': 0.9545055182287053, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: right, reward: -0.301619876852
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': -0.3016198768523941, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded -0.30)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.802857442610042, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 1.961755809287378}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: forward, reward: -9.08062415474
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -9.080624154740729, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.08)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.4783397258548069, 'left': 1.4053014360286937, 'right': 1.1953234434716113, None: -4.452684568417187}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: None, reward: -5.61257363698
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 3, 't': 22, 'action': None, 'reward': -5.612573636980278, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.61)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: None, reward: -0.179102023421
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 2, 't': 23, 'action': None, 'reward': -0.17910202342066617, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded -0.18)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.6201252960250228, 'left': 0.8485430056390468, 'right': 0.7674870214244467, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: forward, reward: -0.636905566346
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -0.6369055663459191, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove forward instead of left. (rewarded -0.64)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 38
\-------------------------

Environment.reset(): Trial set up with start = (4, 4), destination = (7, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: right, reward: 0.33156846663
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', 'left'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 0.3315684666304022, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'left', 'left')
Agent drove right instead of left. (rewarded 0.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.0730819968636343, 'left': 0.325916462700673, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: -4.13771512659
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 24, 't': 1, 'action': None, 'reward': -4.137715126592402, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.0730819968636343, 'left': 0.325916462700673, 'right': 0.0, None: -2.068857563296201}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: right, reward: 0.456055759972
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 0.4560557599721653, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent drove right instead of forward. (rewarded 0.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.41354476321822337, 'left': 2.1819483289604342, 'right': 0.6494611498033025, None: -5.5346748308123646}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: right, reward: 0.957895776381
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 0.9578957763814222, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.96)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -29.45073891507286, 'left': -19.759055090878373, 'right': 0.22953826964397098, None: 0.38982915849304856}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 2.22539222708
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 21, 't': 4, 'action': None, 'reward': 2.2253922270842956, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.23)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.850928638295333, 'left': -9.893031318934076, 'right': 1.2307815124452819, None: 2.416573202394612}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 1.89843715519
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 1.8984371551863801, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.850928638295333, 'left': -9.893031318934076, 'right': 1.2307815124452819, None: 2.1575051787904957}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 2.80512598943
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': None, 'reward': 2.8051259894321063, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.4119694539278544, 'left': 0.2535678229109407, 'right': 0.9194383928146641, None: -2.8569000969886447}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: -4.8250409319
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 18, 't': 7, 'action': None, 'reward': -4.825040931901485, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.83)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.1531639796171882, 'left': 1.171862923573911, 'right': 0.6410669145547467, None: -2.7943472868119734}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (0, -1), action: left, reward: 0.263473531904
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 0.26347353190372547, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent drove left instead of forward. (rewarded 0.26)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.229433931598363, 'left': -4.883987362823236, 'right': 1.0729947956065082, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: right, reward: 1.78465014332
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 1.7846501433199118, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.78)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -5.197413136624829, 'left': -8.654568527111334, 'right': 0.9610010621598262, None: 0.9965389332795866}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: right, reward: -0.0068176718547
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': -0.00681767185469806, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded -0.01)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -20.160948053227965, 'left': -20.16518434097718, 'right': 0.0, None: 0.6185419632928775}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: right, reward: -20.9355818592
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 14, 't': 11, 'action': 'right', 'reward': -20.935581859218342, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.94)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -8.652124678469823, 'left': -10.629007444648977, 'right': 0.28661929347610443, None: 1.762766344143913}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: forward, reward: -9.96123736913
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -9.961237369133762, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.96)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.306681023801794, 'left': -10.629007444648977, 'right': 0.28661929347610443, None: 1.762766344143913}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: right, reward: 0.723429314606
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 0.7234293146055768, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.72)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.11992939015674, 'left': 0.06722356243410899, 'right': 1.936221288123496, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (0, -1), action: right, reward: 1.49153527005
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.4915352700493738, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5304258197177816, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 2), heading: (0, -1), action: None, reward: -0.237727874925
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 10, 't': 15, 'action': None, 'reward': -0.23772787492508396, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent idled at a green light with oncoming traffic. (rewarded -0.24)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: forward, reward: 0.0262852589955
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'forward'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': 0.026285258995496386, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'forward', 'forward')
Agent drove forward instead of right. (rewarded 0.03)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': -9.992381388078595, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (0, -1), action: forward, reward: 1.51802611841
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'forward'), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': 1.5180261184094734, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'forward')
Agent drove forward instead of right. (rewarded 1.52)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.42500909165756573}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: right, reward: 2.11444944832
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 2.114449448315931, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent followed the waypoint right. (rewarded 2.11)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.850928638295333, 'left': -9.893031318934076, 'right': 1.2307815124452819, None: 2.481315584111301}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: left, reward: -10.2606369019
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -10.260636901929022, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.26)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.850928638295333, 'left': -10.076834110431548, 'right': 1.2307815124452819, None: 2.481315584111301}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: forward, reward: -9.75908974064
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -9.75908974064321, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.76)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.2893293544043745, 'left': -9.889738721308941, 'right': 0.45762148577329276, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: forward, reward: 1.62999586259
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': 1.6299958625939424, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent followed the waypoint forward. (rewarded 1.63)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -9.80500918946927, 'left': -10.076834110431548, 'right': 1.2307815124452819, None: 2.481315584111301}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: None, reward: 1.79855753904
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 22, 'action': None, 'reward': 1.798557539043156, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.44863133736615, 'left': 0.3920841610723092, 'right': 0.6550496721952097, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (0, 1), action: right, reward: 0.00156725772738
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.0015672577273779797, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.00)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: right, reward: 0.796334195654
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', 'forward'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.7963341956540462, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, 'right', 'forward')
Agent drove right instead of left. (rewarded 0.80)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 39
\-------------------------

Environment.reset(): Trial set up with start = (7, 2), destination = (6, 5), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -20.22047945123705, 'left': 0.0, 'right': 0.0, None: 1.0998975479105964}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: right, reward: -19.7239491636
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'forward'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': -19.72394916363383, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.72)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -8.980887696592962, 'left': -10.023685128833293, 'right': 0.7772043591398556, None: 1.5551272684990507}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -9.23326819489
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -9.233268194888996, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.23)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.10707794574098, 'left': -10.023685128833293, 'right': 0.7772043591398556, None: 1.5551272684990507}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -10.3030089282
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.303008928192739, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -4.975220594212877, 'left': -4.5639876857538715, 'right': 1.3211709831966352, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: None, reward: 2.78332861748
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 17, 't': 3, 'action': None, 'reward': 2.783328617483127, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.78)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.705043436966859, 'left': -10.023685128833293, 'right': 0.7772043591398556, None: 1.5551272684990507}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: left, reward: -10.4925453363
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 16, 't': 4, 'action': 'left', 'reward': -10.492545336259093, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.8117664406292029, 'left': 0.4112632395228216, 'right': 1.0327289680605438, None: 0.3499343815737973}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: left, reward: 0.137958017405
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 0.13795801740489244, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove left instead of right. (rewarded 0.14)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.21050752947987972, 'left': 1.9690826593865929, 'right': -0.020402637991021932, None: -2.952645783447236}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: left, reward: 2.61119851962
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 2.611198519619526, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent followed the waypoint left. (rewarded 2.61)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.41354476321822337, 'left': 2.1819483289604342, 'right': 0.8036784630923623, None: -5.5346748308123646}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: 1.80243472718
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 1.8024347271801084, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -8.734683438924414, 'left': -9.380390237312291, 'right': 0.4542091787271245, None: 0.7742065989510735}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: None, reward: 1.04582301499
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 12, 't': 8, 'action': None, 'reward': 1.0458230149925425, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.05)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4766816270830957, 'left': -9.950714480057432, 'right': -0.0766308194009645, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: left, reward: -19.2690131298
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -19.269013129793663, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.27)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': -9.813445973183716, 'right': 0.0, None: -2.0691990998302656}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: forward, reward: 0.621240949452
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': 0.6212409494519472, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent drove forward instead of left. (rewarded 0.62)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.441740798675387, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 1.961755809287378}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: None, reward: 2.1033102976
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': None, 'reward': 2.1033102975957947, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.10)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -8.290101299485517, 'left': -7.790668201091232, 'right': 0.5049360542858079, None: 1.0752316183858213}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: right, reward: 0.721881756214
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.7218817562138304, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 0.72)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.07580244130789043, 'left': 0.5693166244273682, 'right': 0.415713212732003, None: -2.2812619805578986}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: None, reward: -5.30799072801
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 7, 't': 13, 'action': None, 'reward': -5.307990728008159, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.31)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': -0.19810334064661483, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: 1.71610684327
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 1.716106843273908, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent followed the waypoint right. (rewarded 1.72)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.392116764816137, 'left': -9.45377965575099, 'right': 1.3722874080142198, None: 2.316998502788794}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: right, reward: 2.11692776575
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': 2.1169277657509293, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.12)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.576563711633343, 'left': 0.08555723865802045, 'right': 0.48750088769889083, None: 1.529762350565477}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: None, reward: 1.18924792672
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.1892479267233644, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.19)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.80500918946927, 'left': -10.076834110431548, 'right': 1.2307815124452819, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 0.247927043678
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.24792704367768514, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.25)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: -19.8995914214
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'right'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': -19.899591421436543, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'right')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.90)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -8.290101299485517, 'left': -7.790668201091232, 'right': 0.6134089052498192, None: 1.0752316183858213}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -10.7990352938
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -10.799035293813471, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.80)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 40
\-------------------------

Environment.reset(): Trial set up with start = (1, 7), destination = (4, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -4.852981348741553, 'left': -4.912888608999064, 'right': 0.4572446303733628, None: 0.6764920457085781}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: -9.74482559448
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 25, 't': 0, 'action': 'left', 'reward': -9.744825594477447, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.80500918946927, 'left': -10.076834110431548, 'right': 0.7393542780614835, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: right, reward: 0.21957610384
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 0.21957610383993043, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.22)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -0.008390135160448176, 'left': 0.8485430056390468, 'right': 0.7674870214244467, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: left, reward: 1.07799641803
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 23, 't': 2, 'action': 'left', 'reward': 1.0779964180348303, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent followed the waypoint left. (rewarded 1.08)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.197413136624829, 'left': -8.654568527111334, 'right': 0.47709169515256405, None: 0.9965389332795866}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: right, reward: 1.57528727944
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 1.5752872794409822, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.58)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.25467800918650096, 'left': 1.9383741304261004, 'right': 1.2558035823336766, None: -3.7806246715003917}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: 0.880264978222
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 0.880264978222384, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove forward instead of left. (rewarded 0.88)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.441740798675387, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: -10.325362062
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -10.3253620619913, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.33)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 1.198540061864059}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: -10.1589518503
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.158951850264362, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.16)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.1079897451991658, 'left': 2.1819483289604342, 'right': 0.8036784630923623, None: -5.5346748308123646}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: right, reward: 1.22365218612
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.223652186124475, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 1.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -20.253971099609448, 'left': 0.0, 'right': -10.091365390222476, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: None, reward: 2.57563796635
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', 'left'), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.5756379663535904, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', 'left')
Agent properly idled at a red light. (rewarded 2.58)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.0, None: -0.08955101171033308}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: right, reward: 0.345929301201
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 0.3459293012007916, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent drove right instead of left. (rewarded 0.35)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.705043436966859, 'left': -10.258115232546192, 'right': 0.7772043591398556, None: 1.5551272684990507}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: None, reward: 1.46296694675
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 15, 't': 10, 'action': None, 'reward': 1.462966946745438, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.46)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.11992939015674, 'left': 0.06722356243410899, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: forward, reward: 1.28204432998
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.2820443299790276, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.28)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.8675958486435598, 'left': -0.13611540802838895, 'right': 1.5015998462085305, None: -5.084061869916265}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: forward, reward: 0.519195968066
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': 0.5191959680659111, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove forward instead of right. (rewarded 0.52)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.200986860067884, 'left': 0.06722356243410899, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: forward, reward: 0.376451574762
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 0.3764515747620176, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 0.38)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.392116764816137, 'left': -9.45377965575099, 'right': 1.7446075868825746, None: 2.316998502788794}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: forward, reward: -10.8583383854
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -10.85833838543889, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.86)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.125227575127514, 'left': -9.45377965575099, 'right': 1.7446075868825746, None: 2.316998502788794}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: right, reward: 1.3586797745
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': 'right', 'reward': 1.3586797744990071, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.36)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -29.45073891507286, 'left': -19.759055090878373, 'right': 0.22953826964397098, None: 1.3076106927886721}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: None, reward: 0.961596370099
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 9, 't': 16, 'action': None, 'reward': 0.9615963700994552, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 0.96)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: -10.7649545563
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'left'), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -10.764954556282587, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', 'left')
Agent attempted driving left through a red light. (rewarded -10.76)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -5.197413136624829, 'left': -8.654568527111334, 'right': 1.0261894872967732, None: 0.9965389332795866}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: None, reward: 2.09063987581
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 7, 't': 18, 'action': None, 'reward': 2.0906398758143494, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.09)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 2.037394717817267, 'left': 0.5993250011054002, 'right': 0.3264428206881556, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: 0.334134834165
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 0.3341348341645821, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.33)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.705043436966859, 'left': -10.258115232546192, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -9.91489268621
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -9.914892686207367, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.91)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5304258197177816, None: -0.11886393746254198}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: -0.473507795845
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -0.4735077958447018, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent drove forward instead of right. (rewarded -0.47)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -19.623505123236882, 'left': -29.94481238143355, 'right': -10.184922854270159, None: 2.0939987498499604}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: 2.2657079615
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 3, 't': 22, 'action': None, 'reward': 2.265707961504603, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.27)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.2802365880189883, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: -0.629502146774
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', None), 'deadline': 2, 't': 23, 'action': None, 'reward': -0.6295021467740303, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded -0.63)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.2802365880189883, 'right': 0.0, None: -0.31475107338701513}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: right, reward: 0.630046045076
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', None), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.6300460450757468, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'left', 'right', None)
Agent followed the waypoint right. (rewarded 0.63)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 41
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (3, 2), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -8.734683438924414, 'left': -9.380390237312291, 'right': 0.4542091787271245, None: 0.9100148069718079}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: right, reward: 1.45300212824
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 30, 't': 0, 'action': 'right', 'reward': 1.4530021282415293, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded 1.45)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 2.037394717817267, 'left': 0.46672991763499116, 'right': 0.3264428206881556, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: right, reward: 1.65849581181
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 1.6584958118134083, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.66)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.39721910935771626, 'left': 0.3587581722439006, 'right': 0.5284232638443994, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: None, reward: -5.15569834375
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 28, 't': 2, 'action': None, 'reward': -5.155698343746144, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.16)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -20.160948053227965, 'left': -20.16518434097718, 'right': -10.467790929609171, None: 0.6185419632928775}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: right, reward: -19.4043154043
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 27, 't': 3, 'action': 'right', 'reward': -19.404315404316282, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.40)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.883551430333343, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: forward, reward: -9.9557346865
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': -9.955734686497122, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.96)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.1079897451991658, 'left': 2.1819483289604342, 'right': 1.0136653246084186, None: -5.5346748308123646}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: None, reward: -4.20175357125
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 25, 't': 5, 'action': None, 'reward': -4.201753571251878, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.20)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.566017853725433, 'left': 0.867510070508116, 'right': 0.7931372050960792, None: -2.7022168903170765}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: right, reward: 1.13071640348
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 24, 't': 6, 'action': 'right', 'reward': 1.1307164034806632, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.13)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': -10.171716032974075, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: None, reward: -4.49439164128
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 23, 't': 7, 'action': None, 'reward': -4.494391641283274, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.49)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.6520840278047935, None: -2.247402346160058}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: 2.25489551417
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'forward'), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': 2.2548955141715803, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'forward')
Agent followed the waypoint forward. (rewarded 2.25)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.7590719014093665, 'left': -0.03953712785440244, 'right': 0.0, None: -2.758206280159728}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: right, reward: 1.49437191072
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 21, 't': 9, 'action': 'right', 'reward': 1.4943719107236235, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent drove right instead of forward. (rewarded 1.49)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.7049590726603364, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: right, reward: 1.18563015315
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'right', None), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 1.1856301531515807, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'right', None)
Agent drove right instead of left. (rewarded 1.19)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: right, reward: 2.52964517949
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 2.529645179486098, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 2.53)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.07580244130789043, 'left': 0.5693166244273682, 'right': 0.415713212732003, None: -3.7946263542830287}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: left, reward: 0.395326947248
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 18, 't': 12, 'action': 'left', 'reward': 0.395326947248375, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent drove left instead of right. (rewarded 0.40)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.80500918946927, 'left': -10.076834110431548, 'right': 0.47946519095070694, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: left, reward: -10.1618766958
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 17, 't': 13, 'action': 'left', 'reward': -10.161876695754623, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.16)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.80500918946927, 'left': -10.119355403093085, 'right': 0.47946519095070694, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: forward, reward: -9.91731856177
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 16, 't': 14, 'action': 'forward', 'reward': -9.91731856176715, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.92)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.4119694539278544, 'left': 0.2535678229109407, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: 1.12193454621
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 15, 't': 15, 'action': 'left', 'reward': 1.1219345462128962, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded 1.12)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.19454077517710466, 'left': 0.0, 'right': 0.0, None: -2.2450553221476186}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: -20.2403669544
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'left'), 'deadline': 14, 't': 16, 'action': 'left', 'reward': -20.24036695442124, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.24)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -5.229433931598363, 'left': -4.883987362823236, 'right': 1.42882246946321, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: -9.49277444142
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 13, 't': 17, 'action': 'left', 'reward': -9.492774441422759, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.49)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -10.125227575127514, 'left': -9.45377965575099, 'right': 1.5516436806907907, None: 2.316998502788794}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: right, reward: 2.01455484264
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 18, 'action': 'right', 'reward': 2.0145548426350643, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.01)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: right, reward: -0.321300984164
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', 'forward'), 'deadline': 11, 't': 19, 'action': 'right', 'reward': -0.32130098416404296, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', 'forward')
Agent drove right instead of forward. (rewarded -0.32)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -5.108565627573413, 'left': -4.740624278078626, 'right': 0.0, None: 1.3087762246935766}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: right, reward: -0.0643719267227
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'right'), 'deadline': 10, 't': 20, 'action': 'right', 'reward': -0.064371926722675, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'right')
Agent drove right instead of left. (rewarded -0.06)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.919643058415232, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: forward, reward: -10.252472203
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 21, 'action': 'forward', 'reward': -10.252472203000433, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.25)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -5.079475925132181, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 1.198540061864059}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: None, reward: 0.818940493781
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 8, 't': 22, 'action': None, 'reward': 0.8189404937812876, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 0.82)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.1079897451991658, 'left': 2.1819483289604342, 'right': 1.0136653246084186, None: -4.868214201032121}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: -0.319746678808
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 7, 't': 23, 'action': 'right', 'reward': -0.31974667880818386, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded -0.32)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.8097615578077844, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: None, reward: 1.39970082678
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'forward'), 'deadline': 6, 't': 24, 'action': None, 'reward': 1.3997008267759707, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', 'forward')
Agent idled at a green light with oncoming traffic. (rewarded 1.40)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.3826796009968656, None: 0.5045356804597947}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: -39.9412402032
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -39.94124020318822, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.94)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -20.397068747535524, 'left': -19.748312131393238, 'right': 0.4410439183331377, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: right, reward: 1.69468649373
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 4, 't': 26, 'action': 'right', 'reward': 1.6946864937293582, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 1.69)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 1.1389649098260681, 'left': 0.0, 'right': 0.6909158725853318, None: -2.482602067478505}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: left, reward: -0.621964714514
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 3, 't': 27, 'action': 'left', 'reward': -0.6219647145136695, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent drove left instead of forward. (rewarded -0.62)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -10.125227575127514, 'left': -9.45377965575099, 'right': 1.7830992616629275, None: 2.316998502788794}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: forward, reward: -10.8208833199
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 2, 't': 28, 'action': 'forward', 'reward': -10.82088331994618, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.82)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: None, reward: 1.36654162999
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', 'right', 'right'), 'deadline': 1, 't': 29, 'action': None, 'reward': 1.3665416299857867, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'right', 'right', 'right')
Agent properly idled at a red light. (rewarded 1.37)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 42
\-------------------------

Environment.reset(): Trial set up with start = (7, 6), destination = (2, 4), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.0538204005628518, 'left': 1.014147732107017, 'right': 1.6002947688632085, None: -5.133532481636374}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: None, reward: -4.82320348611
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 25, 't': 0, 'action': None, 'reward': -4.823203486107335, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.82)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.11249816202689822, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: left, reward: 1.93791612061
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 24, 't': 1, 'action': 'left', 'reward': 1.9379161206136644, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent drove left instead of right. (rewarded 1.94)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.07580244130789043, 'left': 0.4823217858378716, 'right': 0.415713212732003, None: -3.7946263542830287}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: forward, reward: 1.97613462263
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 1.9761346226293688, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent drove forward instead of right. (rewarded 1.98)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 2.037394717817267, 'left': 0.46672991763499116, 'right': 0.9924693162507819, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: 1.09551096748
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': 1.0955109674778098, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.10)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -4.852981348741553, 'left': -7.328857101738256, 'right': 0.4572446303733628, None: 0.6764920457085781}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: -0.00096559986359
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': -0.0009655998635901275, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent drove right instead of forward. (rewarded -0.00)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: 1.00153426187
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', 'forward'), 'deadline': 20, 't': 5, 'action': None, 'reward': 1.0015342618705894, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'forward')
Agent properly idled at a red light. (rewarded 1.00)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': -5.0414677743552065, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: forward, reward: -10.678038332
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.678038331975767, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.68)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: forward, reward: -39.0148290962
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -39.01482909616023, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.01)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -19.507414548080114, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: -19.3821438097
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': -19.382143809655783, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.38)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': -0.03373705585359954, None: 0.6452031987883042}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: 0.162509885715
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 16, 't': 9, 'action': None, 'reward': 0.16250988571490255, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.16)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.1079897451991658, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -4.868214201032121}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: 0.0408902191212
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 0.040890219121231364, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.04)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.4506600629335881, 'left': 0.0, 'right': 0.0, None: 0.685848835362945}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: 0.845831546499
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'right'), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 0.8458315464989439, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'right')
Agent followed the waypoint left. (rewarded 0.85)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: 2.23164452279
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', 'right'), 'deadline': 13, 't': 12, 'action': None, 'reward': 2.231644522787526, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', 'right')
Agent properly idled at a red light. (rewarded 2.23)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.4596626084991584, 'left': -9.889738721308941, 'right': 0.45762148577329276, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: -20.8955318616
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': -20.895531861610657, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.90)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.44863133736615, 'left': 0.3920841610723092, 'right': 0.32830846496129384, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (0, 1), action: left, reward: 1.54275822187
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 11, 't': 14, 'action': 'left', 'reward': 1.5427582218733944, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 1.54)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.7887192174149508, 'left': 0.06722356243410899, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: left, reward: 0.458864244262
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 10, 't': 15, 'action': 'left', 'reward': 0.45886424426155603, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.46)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.4783397258548069, 'left': 1.4053014360286937, 'right': 1.1953234434716113, None: -5.032629102698732}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: -5.4084651108
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 9, 't': 16, 'action': None, 'reward': -5.408465110803586, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -20.15953690644287, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 1.436054913256771}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: forward, reward: -40.0075938774
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': -40.00759387738861, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.01)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.306681023801794, 'left': -10.629007444648977, 'right': 0.5050243040408406, None: 1.762766344143913}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: forward, reward: -10.6612084655
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -10.661208465512992, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.66)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.983944744657393, 'left': -10.629007444648977, 'right': 0.5050243040408406, None: 1.762766344143913}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 1.38963150389
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 6, 't': 19, 'action': None, 'reward': 1.3896315038908984, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.39)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.983944744657393, 'left': -10.629007444648977, 'right': 0.5050243040408406, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: right, reward: 1.30896628875
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 1.3089662887473137, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 1.31)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.705043436966859, 'left': -10.08650395937678, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: forward, reward: -9.83240610264
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -9.832406102639219, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.83)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: -0.135988399204
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 3, 't': 22, 'action': None, 'reward': -0.1359883992037627, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent idled at a green light with oncoming traffic. (rewarded -0.14)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.7432517139351562, 'left': -9.657410605631151, 'right': 0.0, None: -2.3886967238632906}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 1.42001377539
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 1.420013775391219, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent followed the waypoint right. (rewarded 1.42)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.085978102063929, 'left': 1.0577067232723834, 'right': -0.12068121849085786, None: -3.267713222447451}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 1.0385880034
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 1.0385880033990393, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove right instead of forward. (rewarded 1.04)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 43
\-------------------------

Environment.reset(): Trial set up with start = (1, 5), destination = (5, 5), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.7432517139351562, 'left': -9.657410605631151, 'right': 0.7100068876956095, None: -2.3886967238632906}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: None, reward: -5.22525750867
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 20, 't': 0, 'action': None, 'reward': -5.225257508672085, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.23)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.0538204005628518, 'left': 1.014147732107017, 'right': 1.6002947688632085, None: -4.978367983871855}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: forward, reward: 0.28783344085
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 0.2878334408500586, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.29)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.4119694539278544, 'left': 0.6877511845619184, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: forward, reward: 1.76001732092
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': 1.7600173209201069, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 1.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': -19.646596381419585, 'right': -9.648374887274155, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: None, reward: 2.74656410321
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 17, 't': 3, 'action': None, 'reward': 2.7465641032134114, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent properly idled at a red light. (rewarded 2.75)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.86116387561821, 'left': -10.119355403093085, 'right': 0.47946519095070694, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: forward, reward: -9.97578293287
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': -9.975782932869402, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.98)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.5664528426475384, 'left': 0.46672991763499116, 'right': 0.9924693162507819, None: -3.8491122314796504}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: None, reward: -4.96712104316
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': -4.967121043161865, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.97)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.4596626084991584, 'left': -15.392635291459799, 'right': 0.45762148577329276, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: right, reward: 0.233596505576
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 0.23359650557583134, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.23)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.983944744657393, 'left': -10.629007444648977, 'right': 0.9069952963940772, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: left, reward: -10.3722358476
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -10.372235847592378, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: right, reward: -19.1284864372
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', 'forward'), 'deadline': 12, 't': 8, 'action': 'right', 'reward': -19.128486437199243, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.13)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: right, reward: 1.26513264494
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', 'forward'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.2651326449357603, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', 'forward')
Agent drove right instead of left. (rewarded 1.27)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.6708269207064552, 'left': 1.014147732107017, 'right': 1.6002947688632085, None: -4.978367983871855}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: right, reward: 0.805403944176
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 0.8054039441757446, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 0.81)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -10.473055447536847, 'left': -9.45377965575099, 'right': 1.7830992616629275, None: 2.316998502788794}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: None, reward: 0.769984267853
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': None, 'reward': 0.7699842678526094, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.77)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 1.0572247241579655, None: 0.42500909165756573}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -10.9738990815
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -10.973899081452753, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.97)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.76872476980304, 'left': -10.08650395937678, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -10.6910727881
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': -10.691072788107642, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.69)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.76872476980304, 'left': -10.38878837374221, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: -9.54901215963
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': -9.549012159625724, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.55)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.65886846471438, 'left': -10.38878837374221, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -10.15100816
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -10.151008159988624, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.15)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.7887192174149508, 'left': 0.2630439033478325, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: 0.69756338667
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 4, 't': 16, 'action': 'left', 'reward': 0.6975633866698956, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.70)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.918473404243805, 'left': -10.119355403093085, 'right': 0.47946519095070694, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: -9.32496866261
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -9.324968662609303, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.32)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -4.605589689186562, 'left': 0.0, 'right': 0.25818638772786145, None: 0.6632459972982245}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: -9.56910081249
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': -9.56910081249178, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent attempted driving left through a red light. (rewarded -9.57)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.918473404243805, 'left': -9.722162032851195, 'right': 0.47946519095070694, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: -9.38716986891
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': -9.3871698689095, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 44
\-------------------------

Environment.reset(): Trial set up with start = (4, 4), destination = (8, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: right, reward: 2.75342171621
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'right'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 2.753421716205545, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', 'right')
Agent followed the waypoint right. (rewarded 2.75)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.19304730106007256, 'left': 0.3780471455299186, 'right': 0.5029408890303542, None: -2.2723477585706715}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: forward, reward: 0.983046589753
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 0.9830465897531062, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove forward instead of right. (rewarded 0.98)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.473055447536847, 'left': -9.45377965575099, 'right': 1.7830992616629275, None: 1.5434913853207017}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: 2.52555315156
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.525553151562617, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -10.473055447536847, 'left': -9.45377965575099, 'right': 1.7830992616629275, None: 2.0345222684416595}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: 2.18274167631
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.182741676311422, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.6708269207064552, 'left': 1.014147732107017, 'right': 1.2028493565194767, None: -4.978367983871855}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: forward, reward: 0.731923241272
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 0.7319232412722928, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.73)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -10.473055447536847, 'left': -9.45377965575099, 'right': 1.7830992616629275, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: forward, reward: -9.59541266165
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -9.595412661647543, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.60)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -20.397068747535524, 'left': -19.748312131393238, 'right': 1.067865206031248, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: left, reward: -40.8080740412
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -40.80807404115774, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.251448841481128, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: left, reward: 0.901291648474
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', 'forward'), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 0.9012916484744868, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'forward')
Agent drove left instead of right. (rewarded 0.90)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 0.47946519095070694, None: 2.139936561577229}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: None, reward: 1.24089622337
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': 1.2408962233689036, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.24)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.576563711633343, 'left': 0.08555723865802045, 'right': 0.48750088769889083, None: 1.3595051386444208}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: left, reward: 0.0812027016157
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 0.08120270161570131, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded 0.08)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -10.034234054592195, 'left': -9.45377965575099, 'right': 1.7830992616629275, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: right, reward: 1.31272054948
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.3127205494849765, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.31)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.7317545282608426}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: forward, reward: 2.5993425357
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'left'), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 2.5993425356959525, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'left')
Agent followed the waypoint forward. (rewarded 2.60)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.1237374026017855, 'left': -9.658790871308987, 'right': 0.32269982454676027, None: -3.9368993957668015}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: 1.87730535545
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': 1.8773053554474315, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent followed the waypoint forward. (rewarded 1.88)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -5.437513937771278, 'left': -5.061086718526903, 'right': 0.7265790522270998, None: 0.8854295764789993}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: left, reward: -10.8217049713
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': -10.821704971325282, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -10.82)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -5.339019165987883, 'left': -5.0414677743552065, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: -9.01661645217
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -9.016616452165923, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.02)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.5744399821601986, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -4.868214201032121}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: -4.18455188007
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': -4.184551880073976, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.18)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.5744399821601986, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -4.526383040553048}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: -5.50965947705
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -5.509659477050092, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.51)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -10.086057630707833, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: -10.1505327751
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': -10.150532775057105, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.15)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -10.118295202882468, 'left': -10.529038818727578, 'right': 1.2109910259652756, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: right, reward: 1.46016607009
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 1.4601660700947583, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.46)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.701375080989374, 'left': 1.014147732107017, 'right': 1.2028493565194767, None: -4.978367983871855}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: None, reward: -4.14949372342
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': None, 'reward': -4.149493723424003, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.15)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: left, reward: -19.3083525079
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'left'), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -19.30835250788126, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'forward', 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.31)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -19.835655796651018, 'left': -20.400057737446087, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: right, reward: -19.1022016247
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 4, 't': 21, 'action': 'right', 'reward': -19.102201624668947, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.10)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.003461754419426}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: left, reward: -9.26274763502
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'left'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -9.262747635015955, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.26)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.5785292480664365, 'left': 0.605134434256815, 'right': 0.9760043763682321, None: 0.21291611379519323}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: 2.1833479402
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 2.1833479401980633, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent followed the waypoint right. (rewarded 2.18)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -10.034234054592195, 'left': -9.45377965575099, 'right': 1.547909905573952, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: right, reward: 0.310851469057
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.31085146905652494, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 0.31)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 45
\-------------------------

Environment.reset(): Trial set up with start = (7, 2), destination = (5, 4), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.701375080989374, 'left': 1.014147732107017, 'right': 1.2028493565194767, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: right, reward: 2.71865768256
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 2.7186576825645075, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.72)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.6395179913662825, 'left': -10.225179724486539, 'right': 0.0, None: -2.6876250480873725}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (0, -1), action: right, reward: 0.321319719872
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.32131971987150176, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent drove right instead of forward. (rewarded 0.32)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -5.079475925132181, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 1.0087402778226733}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (0, -1), action: forward, reward: -10.7556307882
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.755630788164813, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.7256171267558971, 'left': 0.46515993865324123, 'right': -0.016050334634434504, None: -2.5977189400569203}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: left, reward: 2.69311044325
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 17, 't': 3, 'action': 'left', 'reward': 2.6931104432533584, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent followed the waypoint left. (rewarded 2.69)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: 1.83493579631
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'left', 'forward'), 'deadline': 16, 't': 4, 'action': 'left', 'reward': 1.8349357963114277, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'left', 'forward')
Agent followed the waypoint left. (rewarded 1.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 0.47946519095070694, None: 1.6904163924730662}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: None, reward: 2.31019337216
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': 2.3101933721649526, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -29.45073891507286, 'left': -19.759055090878373, 'right': 0.22953826964397098, None: 1.1346035314440637}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: -40.2315910447
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 14, 't': 6, 'action': 'left', 'reward': -40.23159104465584, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.23)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.5664528426475384, 'left': 0.46672991763499116, 'right': 0.9924693162507819, None: -4.408116637320758}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 1.24649252249
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': 1.2464925224876335, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.25)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.7887192174149508, 'left': 0.480303645008864, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: forward, reward: 0.0164575823245
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 0.016457582324519993, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 0.02)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 1.264822589743049, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -40.6027818224
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -40.60278182243128, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.60)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -10.034234054592195, 'left': -9.45377965575099, 'right': 0.9293806873152385, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -10.5003511239
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': -10.500351123948228, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.50)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -19.623505123236882, 'left': -29.94481238143355, 'right': -10.184922854270159, None: 2.179853355677282}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: forward, reward: -39.7654596351
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': -39.765459635145675, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.77)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -10.034234054592195, 'left': -9.97706538984961, 'right': 0.9293806873152385, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: forward, reward: -10.6926271109
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -10.692627110903281, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.69)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.701375080989374, 'left': 1.014147732107017, 'right': 1.960753519541992, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: right, reward: 0.749275655444
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.7492756554439917, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 0.75)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.4025883998697354, 'left': 0.480303645008864, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: forward, reward: 0.623989344664
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 0.6239893446644805, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 0.62)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.701375080989374, 'left': 1.014147732107017, 'right': 1.3550145874929918, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: right, reward: 2.36655320339
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': 2.3665532033937025, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.37)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: None, reward: -4.48382989216
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', None, 'left'), 'deadline': 4, 't': 16, 'action': None, 'reward': -4.48382989216095, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.48)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -5.197413136624829, 'left': -8.654568527111334, 'right': 1.0261894872967732, None: 1.543589404546968}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: left, reward: -10.3160216325
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -10.316021632537351, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.32)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -29.45073891507286, 'left': -29.99532306776711, 'right': 0.22953826964397098, None: 1.1346035314440637}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: right, reward: -0.465761017586
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': -0.4657610175863347, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded -0.47)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.5744399821601986, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -5.01802125880157}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: forward, reward: 0.127426022961
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': 0.12742602296082606, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.13)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 46
\-------------------------

Environment.reset(): Trial set up with start = (7, 7), destination = (5, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: left, reward: -20.568746558
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', 'forward'), 'deadline': 20, 't': 0, 'action': 'left', 'reward': -20.568746557976432, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'forward', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.57)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.013142629497748193, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: None, reward: -4.20628708146
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'forward'), 'deadline': 19, 't': 1, 'action': None, 'reward': -4.206287081457255, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.21)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': -0.19810334064661483, 'right': 0.858053421636954, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: 0.771629166902
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': 0.771629166901831, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent drove forward instead of right. (rewarded 0.77)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.3858145834509155, 'left': -0.19810334064661483, 'right': 0.858053421636954, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: None, reward: -5.9088820505
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 17, 't': 3, 'action': None, 'reward': -5.908882050496662, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.3858145834509155, 'left': -0.19810334064661483, 'right': 0.858053421636954, None: -2.954441025248331}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: forward, reward: 0.811769267735
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 0.8117692677352711, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent drove forward instead of right. (rewarded 0.81)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.513288872267108, 'left': 0.480303645008864, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: left, reward: 0.563952557068
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 0.563952557068061, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -19.835655796651018, 'left': -20.400057737446087, 'right': -9.551100812334473, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: None, reward: 1.10226237428
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 14, 't': 6, 'action': None, 'reward': 1.1022623742770528, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 1.10)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.8703294085273404}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -10.026701129
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -10.026701129004497, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.03)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.65886846471438, 'left': -10.269898266865418, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: left, reward: -10.602188856
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -10.602188856049125, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.60)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -9.65886846471438, 'left': -10.436043561457272, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -9.84267646816
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'forward', 'reward': -9.842676468160954, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.84)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.750772466437667, 'left': -10.436043561457272, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -9.52390441577
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -9.523904415773622, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.52)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.8117664406292029, 'left': 0.27461062846385703, 'right': 1.0327289680605438, None: 0.3499343815737973}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: 0.570531115989
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': 0.5705311159888748, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove left instead of right. (rewarded 0.57)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.9249023521925785, 'left': 0.0, 'right': 0.10498150559047464, None: 0.4268911878731215}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: -9.12105835188
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -9.121058351877432, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.12)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.4766816270830957, 'left': -14.609863804925547, 'right': -0.0766308194009645, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: -20.3713699036
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': -20.37136990361709, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.37)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.35093300256051235, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -5.01802125880157}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: 1.35209037723
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 1.3520903772285506, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.35)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.118295202882468, 'left': -10.529038818727578, 'right': 1.3355785480300169, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: -9.72840413619
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -9.728404136186176, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.73)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: None, reward: 1.87071429371
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', 'forward', None), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.8707142937122017, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'forward', None)
Agent properly idled at a red light. (rewarded 1.87)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.8515116898945315, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -5.01802125880157}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: 0.112182244721
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.11218224472108174, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.11)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.10768731177632035, None: 0.8714150576196961}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: left, reward: -40.8141444692
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': -40.814144469238755, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.81)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.4766816270830957, 'left': -17.49061685427132, 'right': -0.0766308194009645, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: right, reward: 1.1032673022
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 1.1032673021981578, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 1.10)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 47
\-------------------------

Environment.reset(): Trial set up with start = (4, 5), destination = (5, 2), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': -19.73114848987519, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 1.51202090394
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', None), 'deadline': 20, 't': 0, 'action': None, 'reward': 1.5120209039428545, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', None)
Agent properly idled at a red light. (rewarded 1.51)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -19.53296645241421, 'left': -19.680089796997596, 'right': 0.5007637796767701, None: 1.364334499731736}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: left, reward: -39.7878672327
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 19, 't': 1, 'action': 'left', 'reward': -39.78786723273461, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.79)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.784890880687755, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.1223129572900596}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: forward, reward: -9.30782134259
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -9.307821342592328, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.31)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.54635611164004, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.1223129572900596}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 1.76801748825
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': None, 'reward': 1.768017488253342, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -5.144768770141099, 'left': -5.316620902961126, 'right': 0.4939406876573365, None: 1.0945781914772315}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 1.59324982421
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.5932498242081674, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 1.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.576563711633343, 'left': 0.08337997013686088, 'right': 0.48750088769889083, None: 1.3595051386444208}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 1.6468419229
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': 1.6468419228997575, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.65)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.576563711633343, 'left': 0.08337997013686088, 'right': 0.48750088769889083, None: 1.5031735307720893}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: 0.987510667152
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 0.9875106671520837, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded 0.99)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.7998693356494042, 'left': -0.0485734347393747, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: right, reward: 1.49454541368
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.4945454136824665, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent followed the waypoint right. (rewarded 1.49)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.2575554777896313, 'left': 0.9359977911346383, 'right': 0.8442332002116322, None: -2.664187453023918}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: forward, reward: 0.0274493177764
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 0.02744931777635673, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 0.03)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -19.844789135561125, 'left': -19.552627706033697, 'right': -10.024511358513685, None: 1.1854597527233697}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: right, reward: -20.7068633713
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': -20.7068633713065, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.71)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -8.734683438924414, 'left': -9.380390237312291, 'right': 0.9536056534843269, None: 0.9100148069718079}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: 1.34578365982
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 10, 't': 10, 'action': None, 'reward': 1.345783659817481, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.35)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -8.734683438924414, 'left': -9.380390237312291, 'right': 0.9536056534843269, None: 1.1278992333946445}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: 0.977674790981
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 9, 't': 11, 'action': None, 'reward': 0.9776747909805616, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 0.98)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.9249023521925785, 'left': -4.560529175938716, 'right': 0.10498150559047464, None: 0.4268911878731215}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: right, reward: 0.72899314793
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.7289931479303886, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent drove right instead of left. (rewarded 0.73)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -5.453034737012584, 'left': -7.533654060689663, 'right': 0.6289009916621053, None: 0.9063464871957148}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -9.99650153315
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': -9.996501533151049, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.00)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.363430582747739, 'left': -9.97706538984961, 'right': 0.9293806873152385, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -9.40602197401
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': -9.406021974013385, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.41)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -9.32051938969
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', 'right'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -9.320519389689153, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -9.32)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.701375080989374, 'left': 1.014147732107017, 'right': 1.860783895443347, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 1.58717601282
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 1.5871760128163586, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.59)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -7.177817809076903, 'left': -5.0414677743552065, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: -10.2661614294
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': -10.26616142938037, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.27)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: 0.603101377323
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', 'left'), 'deadline': 2, 't': 18, 'action': None, 'reward': 0.6031013773230316, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'left')
Agent properly idled at a red light. (rewarded 0.60)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.48184696730780663, 'left': 2.1819483289604342, 'right': 0.3469593229001174, None: -5.01802125880157}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: left, reward: 1.21136476946
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': 1.2113647694611702, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.21)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 48
\-------------------------

Environment.reset(): Trial set up with start = (4, 6), destination = (7, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: 0.757599273492
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'left'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 0.7575992734921408, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'left')
Agent drove right instead of left. (rewarded 0.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.28374431582749343, 'left': 0.0, 'right': 0.47953198892479065, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: -5.86659452502
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 19, 't': 1, 'action': None, 'reward': -5.866594525019263, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.87)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.28374431582749343, 'left': 0.0, 'right': 0.47953198892479065, None: -2.9332972625096314}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: left, reward: 0.645872098913
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 18, 't': 2, 'action': 'left', 'reward': 0.6458720989134533, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent drove left instead of right. (rewarded 0.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.197413136624829, 'left': -9.485295079824343, 'right': 1.0261894872967732, None: 1.543589404546968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: 0.428995720321
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 0.42899572032140176, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.43)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.3981670978270231, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: -5.00429587281
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'right', 'forward'), 'deadline': 16, 't': 4, 'action': None, 'reward': -5.004295872805322, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.00)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -19.942531269156774, 'left': -20.133793953321202, 'right': 0.8650352404523536, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 1.72805627524
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 15, 't': 5, 'action': None, 'reward': 1.7280562752393147, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 1.73)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.923349669534321, 'left': -10.529038818727578, 'right': 1.3355785480300169, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: left, reward: -9.44356476221
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': -9.443564762212905, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.44)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -19.942531269156774, 'left': -20.133793953321202, 'right': 0.8650352404523536, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: left, reward: -39.3448555115
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -39.344855511488475, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.34)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.923349669534321, 'left': -9.986301790470241, 'right': 1.3355785480300169, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: left, reward: -10.6435440394
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -10.643544039421446, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': -0.03373705585359954, None: 0.4038565422516034}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: right, reward: 0.179899000492
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 0.17989900049220475, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 0.18)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.0730819968636343, 'left': 0.325916462700673, 'right': 0.22802787998608265, None: -2.068857563296201}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: left, reward: 1.33057265508
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 1.3305726550793775, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent drove left instead of forward. (rewarded 1.33)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.884726278380562, 'left': -9.97706538984961, 'right': 0.9293806873152385, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: -10.3951756985
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': -10.395175698473487, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.40)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.229433931598363, 'left': -7.188380902122997, 'right': 1.42882246946321, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: -9.74104536626
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -9.741045366258351, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.74)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.415713212732003, None: -3.7946263542830287}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: right, reward: 1.29561106022
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 1.2956110602152138, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 1.30)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.5859933874239807, 'left': 0.6877511845619184, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: forward, reward: 2.09146120723
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 2.0914612072295435, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 2.09)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: forward, reward: 1.94347695517
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', 'forward'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': 1.943476955174136, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', 'forward')
Agent followed the waypoint forward. (rewarded 1.94)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.923349669534321, 'left': -10.314922914945843, 'right': 1.3355785480300169, None: 2.032533053441586}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: None, reward: 0.745721123957
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 0.7457211239568031, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.75)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': 0.0730809723193026, None: 0.4038565422516034}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: right, reward: 0.888032860927
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.88803286092696, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 0.89)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -7.485239648928357, 'left': -7.188380902122997, 'right': 1.42882246946321, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: right, reward: 0.447900910318
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 0.44790091031764456, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 0.45)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -10.139950988427024, 'left': -9.97706538984961, 'right': 0.9293806873152385, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: right, reward: 1.26622851831
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 1.2662285183081285, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.27)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 49
\-------------------------

Environment.reset(): Trial set up with start = (8, 7), destination = (5, 6), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.3442985093696806}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: 2.7618843104
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'left'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 2.7618843104040636, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'left')
Agent followed the waypoint right. (rewarded 2.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.1531639796171882, 'left': 0.7176682277388182, 'right': 0.6410669145547467, None: -2.7943472868119734}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: None, reward: -5.60449717629
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 19, 't': 1, 'action': None, 'reward': -5.60449717629485, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.60)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.1531639796171882, 'left': 0.7176682277388182, 'right': 0.6410669145547467, None: -4.199422231553411}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: right, reward: 0.209860715676
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 0.20986071567632825, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent drove right instead of forward. (rewarded 0.21)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.4783397258548069, 'left': 1.4053014360286937, 'right': 1.1953234434716113, None: -5.220547106751159}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: left, reward: 2.55908985678
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 17, 't': 3, 'action': 'left', 'reward': 2.5590898567803912, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.5664528426475384, 'left': 0.8566112200613123, 'right': 0.9924693162507819, None: -4.408116637320758}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, -1), action: right, reward: 0.787490862335
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 0.7874908623345037, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.79)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.48184696730780663, 'left': 1.6966565492108021, 'right': 0.3469593229001174, None: -5.01802125880157}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, -1), action: None, reward: -4.86704341872
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': -4.86704341872033, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.87)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.566017853725433, 'left': 0.867510070508116, 'right': 0.9619268042883712, None: -2.7022168903170765}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: forward, reward: 0.809441122416
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': 0.8094411224161147, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove forward instead of left. (rewarded 0.81)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -8.290101299485517, 'left': -9.294851747452352, 'right': 0.6134089052498192, None: 1.0752316183858213}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: left, reward: -10.2203749762
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -10.220374976182867, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.22)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -8.734683438924414, 'left': -9.380390237312291, 'right': 0.9536056534843269, None: 1.052787012187603}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: forward, reward: -10.3736412452
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -10.373641245169667, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.37)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': -10.297785093259217, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: None, reward: -4.77561807306
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', None), 'deadline': 11, 't': 9, 'action': None, 'reward': -4.775618073061918, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.78)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.142502397782994, 'left': 0.9359977911346383, 'right': 0.8442332002116322, None: -2.664187453023918}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: None, reward: -5.68405673642
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 10, 't': 10, 'action': None, 'reward': -5.684056736424452, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.68)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -0.14657522251285554, 'left': 0.0, 'right': 0.0, None: -2.624086194271513}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: left, reward: 2.2435808495
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'right'), 'deadline': 9, 't': 11, 'action': 'left', 'reward': 2.243580849498163, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'right')
Agent followed the waypoint left. (rewarded 2.24)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.5866135042719075, None: 2.398077030131604}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: right, reward: 1.08835330915
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 1.0883533091517246, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent drove right instead of left. (rewarded 1.09)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.9848884446025539, 'left': 0.4008901216934816, 'right': 0.28974667884503014, None: -4.702960818661083}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: right, reward: -0.280332482562
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': -0.2803324825618173, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove right instead of forward. (rewarded -0.28)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.4766816270830957, 'left': -17.49061685427132, 'right': 0.5133182413985966, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: 0.906648364378
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 0.906648364377925, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.91)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.11330457205706, 'left': -8.667054633023076, 'right': 1.8716908925969848, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: right, reward: 0.74681634612
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': 0.7468163461203108, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 0.75)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': 0.48055691662313127, None: 0.4038565422516034}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: None, reward: 1.10498691876
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.1049869187603347, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.10)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.1729646506003958, None: -0.08955101171033308}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: None, reward: 0.588828848798
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 3, 't': 17, 'action': None, 'reward': 0.5888288487983658, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.59)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.1729646506003958, None: 0.24963891854401637}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: None, reward: -0.389378541056
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 2, 't': 18, 'action': None, 'reward': -0.3893785410562267, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded -0.39)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.1729646506003958, None: -0.06986981125610517}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: right, reward: 0.259986885066
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 0.25998688506642675, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent drove right instead of left. (rewarded 0.26)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 50
\-------------------------

Environment.reset(): Trial set up with start = (5, 4), destination = (2, 5), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -4.6602596948445765, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: 1.64369079665
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'right'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.6436907966500813, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'right')
Agent followed the waypoint right. (rewarded 1.64)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -5.167738838026643, 'left': -5.173426484474621, 'right': 0.12405369580358916, None: 0.6927168675640749}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: right, reward: 0.856609321924
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.8566093219244898, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent drove right instead of forward. (rewarded 0.86)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.21050752947987972, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -2.952645783447236}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: -4.91305480758
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 18, 't': 2, 'action': None, 'reward': -4.913054807582725, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.21050752947987972, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -3.9328502955149807}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: -4.97200675526
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 17, 't': 3, 'action': None, 'reward': -4.972006755257631, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.97)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -8.721989619228637, 'left': -5.0414677743552065, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: -10.1890455676
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': -10.189045567648327, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.19)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.55416234204704, 'left': -9.380390237312291, 'right': 0.9536056534843269, None: 1.052787012187603}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: 1.08820172898
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': 1.0882017289771486, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -19.844789135561125, 'left': -19.552627706033697, 'right': -15.365687364910093, None: 1.1854597527233697}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: 2.60218937752
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 14, 't': 6, 'action': None, 'reward': 2.602189377519185, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 2.60)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.4766816270830957, 'left': -17.49061685427132, 'right': 0.7099833028882607, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: left, reward: -20.4374283251
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -20.43742832512896, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.4783397258548069, 'left': 1.9821956464045425, 'right': 1.1953234434716113, None: -5.220547106751159}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: -5.99344895491
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 12, 't': 8, 'action': None, 'reward': -5.993448954907512, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.99)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4783397258548069, 'left': 1.9821956464045425, 'right': 1.1953234434716113, None: -5.6069980308293355}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: left, reward: 2.18511771025
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 11, 't': 9, 'action': 'left', 'reward': 2.1851177102487505, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.19)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -5.341285613501723, 'left': -4.9304179776708486, 'right': 0.8369463408474729, None: 1.0450081240055278}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 1.52003388061
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 10, 't': 10, 'action': None, 'reward': 1.5200338806130964, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.52)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -5.197413136624829, 'left': -9.485295079824343, 'right': 0.7275926038090874, None: 1.543589404546968}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: forward, reward: -9.55681708707
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': -9.556817087073094, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.56)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -29.45073891507286, 'left': -29.99532306776711, 'right': -0.11811137397118186, None: 1.1346035314440637}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: forward, reward: -40.2045136802
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -40.204513680175396, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.20)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.44863133736615, 'left': 0.9674211914728518, 'right': 0.32830846496129384, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: right, reward: 0.0460448961261
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.04604489612613183, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.05)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -30.08356539191574, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 1.436054913256771}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: 2.25289988929
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 6, 't': 14, 'action': None, 'reward': 2.2528998892940417, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 2.25)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.923349669534321, 'left': -10.314922914945843, 'right': 1.3355785480300169, None: 1.3891270886991947}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: 0.843271649881
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': 0.8432716498807018, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.84)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.48184696730780663, 'left': 1.6966565492108021, 'right': 0.3469593229001174, None: -4.94253233876095}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: -5.94908969423
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': -5.949089694226336, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.48184696730780663, 'left': 1.6966565492108021, 'right': 0.3469593229001174, None: -5.445811016493643}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (0, -1), action: forward, reward: 0.263686066078
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.26368606607838363, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.26)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.55416234204704, 'left': -9.380390237312291, 'right': 0.9536056534843269, None: 1.070494370582376}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (0, -1), action: None, reward: 0.803539868045
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': 0.8035398680449299, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 0.80)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (0, -1), action: forward, reward: -10.1115205626
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'right'), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': -10.111520562573904, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'forward', None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.11)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 51
\-------------------------

Environment.reset(): Trial set up with start = (8, 3), destination = (5, 2), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -39.0450404413
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'right', 'forward'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -39.04504044133094, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.05)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.983944744657393, 'left': -10.500621646120678, 'right': 0.9069952963940772, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -9.12399994128
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -9.123999941275164, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.12)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': -20.26569258375047, 'right': -0.00045646503962193385, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -39.8863126536
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', None, 'forward'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -39.88631265355214, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.89)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.55397234296628, 'left': -10.500621646120678, 'right': 0.9069952963940772, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: right, reward: 0.283071690378
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 0.28307169037817104, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -7.485239648928357, 'left': -7.188380902122997, 'right': 0.9383616898904272, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: right, reward: 2.47820451675
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 2.4782045167491833, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.1389649098260681, 'left': -0.31098235725683476, 'right': 0.6909158725853318, None: -2.482602067478505}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: left, reward: 1.09118324134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 1.0911832413404094, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent drove left instead of forward. (rewarded 1.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.513288872267108, 'left': 0.5221281010384625, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: 0.594896404034
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 0.5948964040338389, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.59)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.11249816202689822, 'left': 0.9689580603068322, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: right, reward: 1.95346986624
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.9534698662426864, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent followed the waypoint right. (rewarded 1.95)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -10.139950988427024, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: forward, reward: -9.43095052197
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -9.43095052196806, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.513288872267108, 'left': 0.5585122525361508, 'right': 1.7138782790864349, None: 0.1299457226509106}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: None, reward: 1.58549312186
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.585493121858015, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.59)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.701375080989374, 'left': 1.014147732107017, 'right': 1.7239799541298528, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: left, reward: 0.740591813538
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 0.740591813537612, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.701375080989374, 'left': 0.8773697728223144, 'right': 1.7239799541298528, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: 1.32741545169
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': 1.3274154516936998, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 1.33)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.0730819968636343, 'left': 0.8282445588900252, 'right': 0.22802787998608265, None: -2.068857563296201}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: right, reward: 1.56540164687
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 1.5654016468724383, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent drove right instead of forward. (rewarded 1.57)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.37276651669309513, 'left': 1.6966565492108021, 'right': 0.3469593229001174, None: -5.445811016493643}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: 1.59851088706
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 1.5985108870644131, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.60)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.4783397258548069, 'left': 2.0836566783266464, 'right': 1.1953234434716113, None: -5.6069980308293355}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: left, reward: 1.85771930655
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': 1.8577193065504967, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.86)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.5664528426475384, 'left': 0.8566112200613123, 'right': 0.8899800892926428, None: -4.408116637320758}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: None, reward: -4.68289578125
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': -4.682895781254484, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.68)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': -20.415567100483972, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: forward, reward: -40.7717569888
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': -40.771756988846455, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.77)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 0.47946519095070694, None: 2.0003048823190093}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: right, reward: 0.657055183655
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.6570551836545474, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.66)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.6814830777923697, 'left': 1.3490516178052538, 'right': 0.2577306846718016, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: right, reward: 0.640051064588
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 0.6400510645879904, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 0.64)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.5987919255930934, 'left': -0.19810334064661483, 'right': 0.858053421636954, None: -2.954441025248331}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: -4.27290063693
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 1, 't': 19, 'action': None, 'reward': -4.272900636927574, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.27)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 52
\-------------------------

Environment.reset(): Trial set up with start = (3, 6), destination = (7, 6), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -0.06799419960188136}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: left, reward: 0.706163769591
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 20, 't': 0, 'action': 'left', 'reward': 0.7061637695908449, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent drove left instead of right. (rewarded 0.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.085978102063929, 'left': 1.0577067232723834, 'right': 0.4589533924540907, None: -3.267713222447451}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: None, reward: -5.98692410417
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 19, 't': 1, 'action': None, 'reward': -5.986924104170219, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.085978102063929, 'left': 1.0577067232723834, 'right': 0.4589533924540907, None: -4.627318663308835}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: None, reward: -5.56747335778
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 18, 't': 2, 'action': None, 'reward': -5.5674733577758815, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.57)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.085978102063929, 'left': 1.0577067232723834, 'right': 0.4589533924540907, None: -5.097396010542358}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: left, reward: 1.90223802949
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 17, 't': 3, 'action': 'left', 'reward': 1.9022380294909462, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove left instead of forward. (rewarded 1.90)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.3291813507472994, 'left': 0.9885551051163675, 'right': 1.1727471922073036, None: -4.065587184239843}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: -5.80382386149
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 16, 't': 4, 'action': None, 'reward': -5.803823861488549, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.80)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -29.694482379191278, 'left': -29.94481238143355, 'right': -10.184922854270159, None: 2.179853355677282}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: -19.8117226528
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 15, 't': 5, 'action': 'right', 'reward': -19.811722652831932, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.785450755197541, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: forward, reward: -10.0073215053
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': -10.007321505276892, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.896386130237216, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: forward, reward: -10.0428704342
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -10.042870434235208, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': -20.13555444050785, 'right': 0.5235464148227659, None: 1.067255681203651}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: forward, reward: -10.3734798162
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -10.373479816201113, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.37)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.3291813507472994, 'left': 0.9885551051163675, 'right': 1.1727471922073036, None: -4.934705522864196}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: -5.54750597118
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 11, 't': 9, 'action': None, 'reward': -5.547505971175713, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.3291813507472994, 'left': 0.9885551051163675, 'right': 1.1727471922073036, None: -5.2411057470199545}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: left, reward: 0.68893816794
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 0.6889381679403189, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 0.69)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: left, reward: -40.756520748
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', 'forward', 'forward'), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -40.75652074798338, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.76)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.14932788298831878, None: 0.14449714315918194}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: forward, reward: -9.26814897751
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -9.2681489775097, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -9.27)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.5664528426475384, 'left': 0.8566112200613123, 'right': 0.8899800892926428, None: -4.545506209287621}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: 1.33066526726
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 1.330665267263118, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.33)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -8.290101299485517, 'left': -9.75761336181761, 'right': 0.6134089052498192, None: 1.0752316183858213}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: left, reward: -9.44160896297
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -9.44160896296973, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.44)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.923349669534321, 'left': -10.314922914945843, 'right': 1.3355785480300169, None: 1.1161993692899483}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 2.1797887105
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': 2.1797887105042992, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.923349669534321, 'left': -10.314922914945843, 'right': 1.3355785480300169, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: -9.79495766921
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': -9.79495766921188, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.79)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.9856387018787541, 'left': 1.6966565492108021, 'right': 0.3469593229001174, None: -5.445811016493643}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: left, reward: 1.45979409542
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': 1.4597940954196067, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.46)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5912929351266276, None: 0.4319288082288822}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: 0.679203692768
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', 'left'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 0.6792036927679321, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', 'left')
Agent drove left instead of forward. (rewarded 0.68)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.1906309744202631, 'left': 0.0, 'right': 0.43840170455061556, None: -2.3610166201851386}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: left, reward: 0.263863617032
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 1, 't': 19, 'action': 'left', 'reward': 0.2638636170323597, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove left instead of right. (rewarded 0.26)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 53
\-------------------------

Environment.reset(): Trial set up with start = (8, 2), destination = (5, 5), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.1499730318081505, 'left': 0.2715091879945646, 'right': 0.27913327132533206, None: 0.9821489084995745}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: 0.898010096252
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 30, 't': 0, 'action': 'right', 'reward': 0.8980100962522604, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent drove right instead of forward. (rewarded 0.90)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.8591536693731, 'left': -10.314922914945843, 'right': 1.3355785480300169, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: right, reward: 1.49880102198
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 1.4988010219781405, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.50)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.7256171267558971, 'left': 1.5791351909532998, 'right': -0.016050334634434504, None: -2.5977189400569203}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: 2.09800533001
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 28, 't': 2, 'action': 'left', 'reward': 2.0980053300114507, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent followed the waypoint left. (rewarded 2.10)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.6814830777923697, 'left': 1.3490516178052538, 'right': 0.448890874629896, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: left, reward: 1.63845771677
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 27, 't': 3, 'action': 'left', 'reward': 1.6384577167714418, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent followed the waypoint left. (rewarded 1.64)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.9848884446025539, 'left': 0.4008901216934816, 'right': 0.004707098141606414, None: -4.702960818661083}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 0.302371571653
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 26, 't': 4, 'action': 'right', 'reward': 0.3023715716526335, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove right instead of forward. (rewarded 0.30)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -5.165118412686815, 'left': -7.930005682394141, 'right': 0.39290176734632676, None: 1.0954911303830515}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: left, reward: -9.14780483517
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 25, 't': 5, 'action': 'left', 'reward': -9.147804835165264, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -9.15)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -5.165118412686815, 'left': -8.538905258779703, 'right': 0.39290176734632676, None: 1.0954911303830515}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: None, reward: 1.41887751747
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 24, 't': 6, 'action': None, 'reward': 1.418877517466395, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 1.42)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.55397234296628, 'left': -10.500621646120678, 'right': 0.5950334933861241, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: right, reward: 1.73109172014
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 1.7310917201448723, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 1.73)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.8556621364736083, None: -3.7946263542830287}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: 1.07120168265
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 22, 't': 8, 'action': 'right', 'reward': 1.0712016826479551, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 1.07)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.1906309744202631, 'left': 0.13193180851617986, 'right': 0.43840170455061556, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: 0.863182643998
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': 0.8631826439980783, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove forward instead of right. (rewarded 0.86)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.513288872267108, 'left': 0.5585122525361508, 'right': 1.7138782790864349, None: 0.8577194222544628}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: left, reward: 1.81433618669
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 20, 't': 10, 'action': 'left', 'reward': 1.8143361866944896, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.81)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -4.605589689186562, 'left': -4.78455040624589, 'right': 0.25818638772786145, None: 0.6632459972982245}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: right, reward: 0.894732305733
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 0.8947323057334177, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent drove right instead of forward. (rewarded 0.89)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.455517593438483, 'left': -5.0414677743552065, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: left, reward: -9.89787612072
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': -9.89787612071957, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.90)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.6877294880707738, 'left': 0.867510070508116, 'right': 0.9619268042883712, None: -2.7022168903170765}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: left, reward: 1.46281451004
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 17, 't': 13, 'action': 'left', 'reward': 1.4628145100375727, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent followed the waypoint left. (rewarded 1.46)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: left, reward: -19.2804419622
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'right', 'forward', 'forward'), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -19.280441962192754, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'forward', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.28)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.0730819968636343, 'left': 0.8282445588900252, 'right': 0.8967147634292605, None: -2.068857563296201}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: forward, reward: 2.72020071845
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': 2.7202007184464114, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent followed the waypoint forward. (rewarded 2.72)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 0.5682601873026272, None: 2.0003048823190093}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: right, reward: 1.10679198583
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 14, 't': 16, 'action': 'right', 'reward': 1.1067919858332185, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.11)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': -4.821198459104612, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: forward, reward: -10.4444748328
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', 'right'), 'deadline': 13, 't': 17, 'action': 'forward', 'reward': -10.444474832819134, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'right')
Agent attempted driving forward through a red light. (rewarded -10.44)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -4.95562719072588, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: -9.3528534349
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'right'), 'deadline': 12, 't': 18, 'action': 'left', 'reward': -9.35285343489847, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'right')
Agent attempted driving left through a red light. (rewarded -9.35)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.726002198685094, 'left': 1.4139011236151922, 'right': 0.8424777280941738, None: -4.046450134383133}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: right, reward: 0.14088429247
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 11, 't': 19, 'action': 'right', 'reward': 0.14088429247015044, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove right instead of left. (rewarded 0.14)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.6877294880707738, 'left': 1.1651622902728445, 'right': 0.9619268042883712, None: -2.7022168903170765}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: -5.34457059944
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 10, 't': 20, 'action': None, 'reward': -5.344570599443385, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.34)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 1.328633988566433, 'right': 0.18628544098594924, None: 0.16294635437428795}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: forward, reward: -0.218618109653
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'forward'), 'deadline': 9, 't': 21, 'action': 'forward', 'reward': -0.21861810965349526, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'forward')
Agent drove forward instead of left. (rewarded -0.22)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -9.55397234296628, 'left': -10.500621646120678, 'right': 1.1630626067654983, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: right, reward: 0.30424268676
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 8, 't': 22, 'action': 'right', 'reward': 0.3042426867600343, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.30)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': -9.578054242763955, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: forward, reward: -40.4042815159
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', None), 'deadline': 7, 't': 23, 'action': 'forward', 'reward': -40.40428151591309, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.40)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.637338441105644, 'left': -10.436043561457272, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: left, reward: -10.3293072167
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 6, 't': 24, 'action': 'left', 'reward': -10.329307216660265, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.33)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.513288872267108, 'left': 1.1864242196153203, 'right': 1.7138782790864349, None: 0.8577194222544628}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: None, reward: -0.360556375135
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 5, 't': 25, 'action': None, 'reward': -0.3605563751350467, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded -0.36)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.513288872267108, 'left': 1.1864242196153203, 'right': 1.7138782790864349, None: 0.24858152355970808}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: None, reward: 0.70187736179
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 4, 't': 26, 'action': None, 'reward': 0.7018773617902536, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.70)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -9.969628282236211, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: forward, reward: -10.6737991468
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -10.673799146755503, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.67)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -10.321713714495857, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.108631972376541}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: None, reward: 1.93321665554
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 2, 't': 28, 'action': None, 'reward': 1.9332166555351, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.93)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.7998693356494042, 'left': -0.0485734347393747, 'right': 0.7472727068412333, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: None, reward: -4.58834701847
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 1, 't': 29, 'action': None, 'reward': -4.588347018471527, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.59)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 54
\-------------------------

Environment.reset(): Trial set up with start = (2, 7), destination = (7, 3), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.8097615578077844, 'left': 0.0, 'right': 0.0, None: 0.6998504133879854}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: right, reward: 1.82459262377
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'forward'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 1.8245926237725687, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.82)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.0143952663415368, 'left': 0.8773697728223144, 'right': 1.7239799541298528, None: -4.563930853647928}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: None, reward: -4.68227117068
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 24, 't': 1, 'action': None, 'reward': -4.6822711706830695, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.68)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.0143952663415368, 'left': 0.8773697728223144, 'right': 1.7239799541298528, None: -4.623101012165499}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: right, reward: 1.31073400023
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.3107340002288406, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.31)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.241914946080475}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: left, reward: -19.9065896481
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'right', None, 'left'), 'deadline': 22, 't': 3, 'action': 'left', 'reward': -19.906589648141093, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.91)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.1499730318081505, 'left': 0.2715091879945646, 'right': 0.5885716837887962, None: 0.9821489084995745}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: left, reward: 0.0257242469607
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 21, 't': 4, 'action': 'left', 'reward': 0.025724246960650787, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent drove left instead of forward. (rewarded 0.03)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.637338441105644, 'left': -10.382675389058768, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: forward, reward: -10.0049554588
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -10.004955458761476, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.00)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.513288872267108, 'left': 1.1864242196153203, 'right': 1.7138782790864349, None: 0.47522944267498085}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: left, reward: 1.82835698686
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 1.8283569868645482, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.83)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.5269068092091707, 'left': 0.13193180851617986, 'right': 0.43840170455061556, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: right, reward: 1.67887221811
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.6788722181095668, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.68)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.82114694993356, 'left': -10.382675389058768, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: left, reward: -10.8866751927
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 17, 't': 8, 'action': 'left', 'reward': -10.886675192683319, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.89)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.11249816202689822, 'left': 0.9689580603068322, 'right': 0.9767349331213432, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: 1.82923832912
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 16, 't': 9, 'action': None, 'reward': 1.829238329115904, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.9894480298872794, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: left, reward: 0.624153194851
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', 'left'), 'deadline': 15, 't': 10, 'action': 'left', 'reward': 0.6241531948509847, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'left')
Agent drove left instead of right. (rewarded 0.62)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.21050752947987972, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -4.452428525386306}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: 0.915177730591
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 0.9151777305914336, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 0.92)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -19.906441819369782, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: -40.3717970644
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'forward'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -40.371797064404745, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.37)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: left, reward: -39.4444634592
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', 'left', None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': -39.44446345923475, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', 'left', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.44)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -20.287052385378907, 'left': 0.0, 'right': -10.185420721605979, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: 1.0620401183
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'right'), 'deadline': 11, 't': 14, 'action': None, 'reward': 1.062040118299722, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', 'right')
Agent properly idled at a red light. (rewarded 1.06)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -5.144768770141099, 'left': -5.316620902961126, 'right': 0.4939406876573365, None: 1.3439140078426994}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: -10.6815991437
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -10.681599143664197, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.68)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.576563711633343, 'left': 0.5354453186444723, 'right': 0.48750088769889083, None: 1.5031735307720893}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: 1.00351538722
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': 1.0035153872230542, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.00)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.312103483103256, 'left': 0.09219518175704206, 'right': 0.47515470021203554, None: 0.4845531306976972}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: right, reward: 0.755137090694
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 0.7551370906943005, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent drove right instead of forward. (rewarded 0.76)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.8591536693731, 'left': -10.314922914945843, 'right': 1.4171897850040787, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: forward, reward: -10.6406065181
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -10.640606518133541, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.64)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -8.290101299485517, 'left': -9.59961116239367, 'right': 0.6134089052498192, None: 1.0752316183858213}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: None, reward: 1.18765887302
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 6, 't': 19, 'action': None, 'reward': 1.1876588730188362, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.19)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': -9.949795710718272, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: -40.4785772389
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'right'), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -40.47857723891989, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.48)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.9856387018787541, 'left': 1.5782253223152045, 'right': 0.3469593229001174, None: -5.445811016493643}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: left, reward: 0.522515467378
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': 0.5225154673781374, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 0.52)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.9758657498167072, None: -2.2399954326267206}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: forward, reward: 1.8705446714
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'right'), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 1.8705446714008231, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'right')
Agent followed the waypoint forward. (rewarded 1.87)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.576563711633343, 'left': 0.5354453186444723, 'right': 0.48750088769889083, None: 1.2533444589975717}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: -0.600851875429
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 2, 't': 23, 'action': None, 'reward': -0.6008518754292398, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded -0.60)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.576563711633343, 'left': 0.5354453186444723, 'right': 0.48750088769889083, None: 0.326246291784166}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: left, reward: -0.0334350392719
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 1, 't': 24, 'action': 'left', 'reward': -0.033435039271908495, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded -0.03)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 55
\-------------------------

Environment.reset(): Trial set up with start = (5, 2), destination = (2, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: -10.6471095896
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', 'right', None), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': -10.647109589572748, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.65)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -10.24988009375332, 'left': -10.314922914945843, 'right': 1.4171897850040787, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: right, reward: 1.48480817308
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 1.484808173075597, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.48)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 0.8375260865679228, None: 2.0003048823190093}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 1.40914625924
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.4091462592398636, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.41)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: right, reward: 1.40471704115
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'right'), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 1.4047170411509138, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'right')
Agent drove right instead of left. (rewarded 1.40)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: forward, reward: 2.0919514802
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', 'forward', None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 2.0919514802028703, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'forward', None)
Agent followed the waypoint forward. (rewarded 2.09)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 1.123336172903893, None: 2.0003048823190093}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: None, reward: 2.71054694836
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 2.710546948361494, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -4.605589689186562, 'left': -4.78455040624589, 'right': 0.5764593467306396, None: 0.6632459972982245}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: None, reward: 1.60149682206
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 19, 't': 6, 'action': None, 'reward': 1.6014968220639114, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 1.60)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: forward, reward: 1.15165690543
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'left', None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': 1.1516569054306396, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'left', None)
Agent followed the waypoint forward. (rewarded 1.15)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 1.123336172903893, None: 2.355425915340252}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 2.85570644259
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.855706442585082, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.86)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -9.54635611164004, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.445165222771701}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: forward, reward: -9.02202250826
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -9.02202250826325, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.02)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.576563711633343, 'left': 0.2510051396862819, 'right': 0.48750088769889083, None: 0.326246291784166}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 1.52255019827
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 15, 't': 10, 'action': None, 'reward': 1.5225501982730276, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.52)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.6395179913662825, 'left': -10.225179724486539, 'right': 0.16065985993575088, None: -2.6876250480873725}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: forward, reward: 0.911519055438
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 0.9115190554375225, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent followed the waypoint forward. (rewarded 0.91)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -19.65953581119593, 'right': 0.0, None: 0.9549383128618697}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: right, reward: 2.15183415145
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 2.1518341514549473, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent followed the waypoint right. (rewarded 2.15)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': -10.171716032974075, 'right': 0.0, None: -2.247195820641637}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: right, reward: 0.0169484854223
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 0.01694848542229699, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent drove right instead of forward. (rewarded 0.02)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -8.290101299485517, 'left': -9.59961116239367, 'right': 0.6134089052498192, None: 1.1314452457023287}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: right, reward: -0.231479445366
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': -0.2314794453661425, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded -0.23)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.6933959083547354, 'left': -0.13611540802838895, 'right': 1.5015998462085305, None: -5.084061869916265}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: None, reward: -4.79947358113
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 10, 't': 15, 'action': None, 'reward': -4.799473581132724, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.80)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -7.485239648928357, 'left': -7.188380902122997, 'right': 1.7082831033198054, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: forward, reward: -9.10322552433
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -9.103225524332851, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.10)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -19.835655796651018, 'left': -20.400057737446087, 'right': -9.551100812334473, None: 0.5511311871385264}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: right, reward: -19.4213055612
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 8, 't': 17, 'action': 'right', 'reward': -19.42130556115041, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.42)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -8.294232586630605, 'left': -7.188380902122997, 'right': 1.7082831033198054, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: right, reward: 2.1367900727
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 2.136790072701804, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.14)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': -5.486949540726377, 'right': 1.0572247241579655, None: 0.42500909165756573}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: forward, reward: -9.11773499133
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -9.117734991330202, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.12)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.82114694993356, 'left': -10.634675290871044, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: left, reward: -10.9725944799
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -10.97259447985399, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.97)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.11249816202689822, 'left': 0.9689580603068322, 'right': 0.9767349331213432, None: 0.914619164557952}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: -0.336034030617
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 4, 't': 21, 'action': None, 'reward': -0.3360340306167793, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded -0.34)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.6933959083547354, 'left': -0.13611540802838895, 'right': 1.5015998462085305, None: -4.941767725524494}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: forward, reward: 1.01007363846
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 1.0100736384640778, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove forward instead of right. (rewarded 1.01)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.28374431582749343, 'left': 0.32293604945672666, 'right': 0.47953198892479065, None: -2.9332972625096314}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 0.347389963288
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.3473899632876818, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent followed the waypoint right. (rewarded 0.35)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': -19.65953581119593, 'right': 1.0759170757274736, None: 0.9549383128618697}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: forward, reward: -40.4141089334
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -40.414108933355415, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 56
\-------------------------

Environment.reset(): Trial set up with start = (1, 6), destination = (5, 3), deadline = 35
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': -10.20376779627894, None: 0.972698004472699}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: -39.9501587346
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 35, 't': 0, 'action': 'left', 'reward': -39.950158734576114, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.95)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': -19.975079367288057, 'right': -10.20376779627894, None: 0.972698004472699}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: forward, reward: -39.8484813291
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 34, 't': 1, 'action': 'forward', 'reward': -39.8484813291012, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.85)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.24988009375332, 'left': -10.314922914945843, 'right': 1.4509989790398379, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: forward, reward: -9.69919077434
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 33, 't': 2, 'action': 'forward', 'reward': -9.69919077433911, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.70)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 0.982676893784
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 32, 't': 3, 'action': None, 'reward': 0.9826768937837413, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent properly idled at a red light. (rewarded 0.98)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.2558035823336766, None: -3.7806246715003917}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: -5.76823620007
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 31, 't': 4, 'action': None, 'reward': -5.768236200074259, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.77)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.2558035823336766, None: -4.774430435787325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: right, reward: 1.43400673629
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 30, 't': 5, 'action': 'right', 'reward': 1.4340067362870543, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 1.43)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.652821636576652, 'left': -9.722162032851195, 'right': 1.123336172903893, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: -10.8002918105
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 29, 't': 6, 'action': 'forward', 'reward': -10.80029181050031, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.80)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.9848884446025539, 'left': 0.4008901216934816, 'right': 0.15353933489711996, None: -4.702960818661083}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: forward, reward: 2.16585206446
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 28, 't': 7, 'action': 'forward', 'reward': 2.1658520644575487, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 2.17)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -10.226556723538481, 'left': -9.722162032851195, 'right': 1.123336172903893, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: right, reward: 1.54728964821
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 27, 't': 8, 'action': 'right', 'reward': 1.5472896482060756, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.55)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -9.974535434046215, 'left': -10.314922914945843, 'right': 1.4509989790398379, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: left, reward: -9.34164095349
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 26, 't': 9, 'action': 'left', 'reward': -9.341640953488195, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.34)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.974535434046215, 'left': -9.828281934217019, 'right': 1.4509989790398379, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: left, reward: -9.26032625759
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 25, 't': 10, 'action': 'left', 'reward': -9.260326257593944, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.26)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.08841626537400932, 'left': -10.439104395112189, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: forward, reward: 1.45877379472
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 24, 't': 11, 'action': 'forward', 'reward': 1.458773794723411, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded 1.46)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.974535434046215, 'left': -9.544304095905481, 'right': 1.4509989790398379, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: left, reward: -9.76629300862
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 23, 't': 12, 'action': 'left', 'reward': -9.766293008621524, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.77)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.142502397782994, 'left': 0.9359977911346383, 'right': 0.8442332002116322, None: -4.174122094724185}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: None, reward: -5.03547997609
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 22, 't': 13, 'action': None, 'reward': -5.035479976093618, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.04)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.142502397782994, 'left': 0.9359977911346383, 'right': 0.8442332002116322, None: -4.604801035408901}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: right, reward: 1.72922191415
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 21, 't': 14, 'action': 'right', 'reward': 1.7292219141540262, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove right instead of left. (rewarded 1.73)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.4766816270830957, 'left': -18.96402258970014, 'right': 0.7099833028882607, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: left, reward: -20.7086541514
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 20, 't': 15, 'action': 'left', 'reward': -20.708654151381914, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.71)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.9856387018787541, 'left': 1.050370394846671, 'right': 0.3469593229001174, None: -5.445811016493643}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: None, reward: -5.07877457953
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 19, 't': 16, 'action': None, 'reward': -5.078774579529259, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.08)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.38216139270014016, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: forward, reward: -10.7027218429
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, 'right'), 'deadline': 18, 't': 17, 'action': 'forward', 'reward': -10.702721842864015, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.70)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 1.4509989790398379, None: 1.6479940398971238}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: None, reward: 2.20101170607
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 17, 't': 18, 'action': None, 'reward': 2.2010117060697496, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.9856387018787541, 'left': 1.050370394846671, 'right': 0.3469593229001174, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: forward, reward: -0.27323123981
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 16, 't': 19, 'action': 'forward', 'reward': -0.27323123980971686, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded -0.27)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: right, reward: 0.144163067392
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'right', None), 'deadline': 15, 't': 20, 'action': 'right', 'reward': 0.14416306739180818, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'right', None)
Agent drove right instead of forward. (rewarded 0.14)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.35620373103451863, 'left': 1.050370394846671, 'right': 0.3469593229001174, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: forward, reward: -0.0337347675394
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 14, 't': 21, 'action': 'forward', 'reward': -0.03373476753942617, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded -0.03)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': 0.48055691662313127, None: 0.7544217305059691}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: 1.44462028183
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 13, 't': 22, 'action': None, 'reward': 1.444620281827296, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.44)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.16123448174754623, 'left': 1.050370394846671, 'right': 0.3469593229001174, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: left, reward: 1.87697239546
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 12, 't': 23, 'action': 'left', 'reward': 1.8769723954630781, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.88)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.5664528426475384, 'left': 0.8566112200613123, 'right': 1.1103226782778806, None: -4.545506209287621}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 0.983999748553
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 11, 't': 24, 'action': 'right', 'reward': 0.9839997485530956, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.98)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': -19.942531269156774, 'left': -29.73932473240484, 'right': 0.8650352404523536, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: right, reward: 0.169630777724
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 10, 't': 25, 'action': 'right', 'reward': 0.16963077772423207, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 0.17)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -9.455517593438483, 'left': -7.469671947537388, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: left, reward: -10.547062877
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 9, 't': 26, 'action': 'left', 'reward': -10.547062877041194, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -10.55)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -7.917553356648497, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 1.0087402778226733}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: forward, reward: -10.4802001725
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 8, 't': 27, 'action': 'forward', 'reward': -10.480200172508683, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.48)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 1.4509989790398379, None: 1.9245028729834366}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: -0.342449925904
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 7, 't': 28, 'action': 'right', 'reward': -0.3424499259040219, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded -0.34)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.9051279624712755, 'left': 0.14494626684101308, 'right': 0.7599968239915724, None: -4.817319096940976}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: -5.87537682611
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 6, 't': 29, 'action': None, 'reward': -5.875376826111259, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.88)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Environment.step(): t = 30
state
{'forward': 0.9051279624712755, 'left': 0.14494626684101308, 'right': 0.7599968239915724, None: -5.346347961526117}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: -4.16603625809
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 5, 't': 30, 'action': None, 'reward': -4.166036258089638, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.17)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Environment.step(): t = 31
state
{'forward': -7.724768135081817, 'left': -7.533654060689663, 'right': 0.6289009916621053, None: 0.9063464871957148}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: 0.62793564121
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 4, 't': 31, 'action': None, 'reward': 0.6279356412102457, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 0.63)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Environment.step(): t = 32
state
{'forward': -19.835655796651018, 'left': -20.400057737446087, 'right': -14.486203186742443, None: 0.5511311871385264}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: -39.005471105
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 3, 't': 32, 'action': 'left', 'reward': -39.00547110504827, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.01)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Environment.step(): t = 33
state
{'forward': 0.0, 'left': -20.278999117446027, 'right': 0.0, None: 0.8000704033403672}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: right, reward: 1.93028918705
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 2, 't': 33, 'action': 'right', 'reward': 1.9302891870455723, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent followed the waypoint right. (rewarded 1.93)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Environment.step(): t = 34
state
{'forward': -19.53296645241421, 'left': -29.733978514866102, 'right': 0.5007637796767701, None: 1.364334499731736}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: -39.0913980225
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 1, 't': 34, 'action': 'forward', 'reward': -39.091398022547175, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.09)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 57
\-------------------------

Environment.reset(): Trial set up with start = (2, 7), destination = (6, 4), deadline = 35
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -19.507414548080114, 'left': 0.0, 'right': -9.691071904827892, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: left, reward: -39.734722851
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 35, 't': 0, 'action': 'left', 'reward': -39.734722851026355, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.73)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.55397234296628, 'left': -10.500621646120678, 'right': 0.7336526467627663, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: left, reward: -9.51902779358
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 34, 't': 1, 'action': 'left', 'reward': -9.519027793583874, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.52)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -19.507414548080114, 'left': -19.867361425513177, 'right': -9.691071904827892, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: None, reward: 2.48154072573
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 33, 't': 2, 'action': None, 'reward': 2.4815407257322093, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent properly idled at a red light. (rewarded 2.48)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.55397234296628, 'left': -10.009824719852276, 'right': 0.7336526467627663, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: left, reward: -10.755253033
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 32, 't': 3, 'action': 'left', 'reward': -10.755253033036517, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.76)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.55397234296628, 'left': -10.382538876444396, 'right': 0.7336526467627663, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: right, reward: 0.0843792181673
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 31, 't': 4, 'action': 'right', 'reward': 0.08437921816728622, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.08)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: -40.7795268161
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'forward', None), 'deadline': 30, 't': 5, 'action': 'forward', 'reward': -40.77952681613824, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.78)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: left, reward: -20.5206698896
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', 'right', None), 'deadline': 29, 't': 6, 'action': 'left', 'reward': -20.52066988957946, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'right', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.52)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.5664528426475384, 'left': 0.8566112200613123, 'right': 1.047161213415488, None: -4.545506209287621}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: left, reward: 0.797052970641
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 28, 't': 7, 'action': 'left', 'reward': 0.7970529706408752, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.80)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.19454077517710466, 'left': -10.12018347721062, 'right': 0.0, None: -2.2450553221476186}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: right, reward: 2.50839071043
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'left'), 'deadline': 27, 't': 8, 'action': 'right', 'reward': 2.5083907104313337, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, 'left')
Agent followed the waypoint right. (rewarded 2.51)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.576563711633343, 'left': 0.2510051396862819, 'right': 0.48750088769889083, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: right, reward: 0.933568089134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 26, 't': 9, 'action': 'right', 'reward': 0.9335680891337148, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.93)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.4931966083114683, None: 0.947934932373417}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: forward, reward: -39.5208845698
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'right', None, 'forward'), 'deadline': 25, 't': 10, 'action': 'forward', 'reward': -39.520884569750244, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.52)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.19887676457859, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 1.0087402778226733}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: None, reward: 0.907489743769
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 24, 't': 11, 'action': None, 'reward': 0.9074897437688823, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 0.91)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.19887676457859, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 0.9581150107957778}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: forward, reward: -10.4537466409
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 23, 't': 12, 'action': 'forward', 'reward': -10.453746640892557, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.45)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.16123448174754623, 'left': 1.4636713951548745, 'right': 0.3469593229001174, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: left, reward: 1.73048298169
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 22, 't': 13, 'action': 'left', 'reward': 1.7304829816906622, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.73)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.7590719014093665, 'left': -0.03953712785440244, 'right': 0.7471859553618118, None: -2.758206280159728}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: right, reward: 0.883977972993
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 21, 't': 14, 'action': 'right', 'reward': 0.8839779729933975, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent drove right instead of forward. (rewarded 0.88)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.4783397258548069, 'left': 1.9706879924385716, 'right': 1.1953234434716113, None: -5.6069980308293355}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 1.31498264478
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 20, 't': 15, 'action': 'left', 'reward': 1.3149826447802124, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.31)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -19.835655796651018, 'left': -29.70276442124718, 'right': -14.486203186742443, None: 0.5511311871385264}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: right, reward: -20.3092076498
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 19, 't': 16, 'action': 'right', 'reward': -20.309207649768165, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.31)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -10.321713714495857, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.0209243139558204}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: forward, reward: -9.57374863755
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 17, 'action': 'forward', 'reward': -9.57374863754686, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.57)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.6832708149928933}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: -39.489523144
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'right', 'right'), 'deadline': 17, 't': 18, 'action': 'left', 'reward': -39.48952314395552, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'right', 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.49)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.3291813507472994, 'left': 0.8387466365283431, 'right': 1.1727471922073036, None: -5.2411057470199545}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: None, reward: -5.33438442142
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 16, 't': 19, 'action': None, 'reward': -5.334384421420714, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.128355652982165, 'left': 0.0, 'right': 0.8925079238852455, None: -2.9548276019880584}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 7), heading: (0, -1), action: left, reward: 0.752322278859
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'right'), 'deadline': 15, 't': 20, 'action': 'left', 'reward': 0.7523222788587531, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'right')
Agent drove left instead of right. (rewarded 0.75)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.9051279624712755, 'left': 0.14494626684101308, 'right': 0.7599968239915724, None: -4.756192109807877}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: left, reward: 0.61528433764
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 14, 't': 21, 'action': 'left', 'reward': 0.6152843376397159, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove left instead of right. (rewarded 0.62)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.142502397782994, 'left': 0.9359977911346383, 'right': 1.2867275571828292, None: -4.604801035408901}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: 2.27567401547
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 13, 't': 22, 'action': 'left', 'reward': 2.2756740154686304, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 2.28)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -9.55416234204704, 'left': -9.380390237312291, 'right': 0.9536056534843269, None: 0.937017119313653}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: -9.41814093863
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 12, 't': 23, 'action': 'left', 'reward': -9.418140938625665, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.42)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.55416234204704, 'left': -9.399265587968978, 'right': 0.9536056534843269, None: 0.937017119313653}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: forward, reward: -10.0843762338
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 11, 't': 24, 'action': 'forward', 'reward': -10.0843762337876, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.08)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': -10.297785093259217, 'right': 0.0, None: -2.387809036530959}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: forward, reward: -0.00103956738804
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', None), 'deadline': 10, 't': 25, 'action': 'forward', 'reward': -0.0010395673880441159, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', None)
Agent drove forward instead of left. (rewarded -0.00)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.16123448174754623, 'left': 1.5970771884227684, 'right': 0.3469593229001174, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: forward, reward: 0.769515651809
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 9, 't': 26, 'action': 'forward', 'reward': 0.7695156518092543, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.77)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: 0.265672263505
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'left', 'right'), 'deadline': 8, 't': 27, 'action': 'right', 'reward': 0.26567226350476936, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'left', 'right')
Agent drove right instead of left. (rewarded 0.27)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.5987919255930934, 'left': -0.19810334064661483, 'right': 0.858053421636954, None: -3.613670831087952}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: None, reward: -5.94979501353
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 7, 't': 28, 'action': None, 'reward': -5.949795013529961, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.5987919255930934, 'left': -0.19810334064661483, 'right': 0.858053421636954, None: -4.781732922308956}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: -0.35591413302
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 6, 't': 29, 'action': 'left', 'reward': -0.35591413301990793, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent drove left instead of right. (rewarded -0.36)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Environment.step(): t = 30
state
{'forward': 0.7256171267558971, 'left': 1.8385702604823753, 'right': -0.016050334634434504, None: -2.5977189400569203}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: left, reward: 0.455074728837
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 5, 't': 30, 'action': 'left', 'reward': 0.45507472883654154, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent followed the waypoint left. (rewarded 0.46)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Environment.step(): t = 31
state
{'forward': 1.576563711633343, 'left': 0.2510051396862819, 'right': 0.7105344884163028, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: right, reward: 0.455564073459
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 4, 't': 31, 'action': 'right', 'reward': 0.45556407345869, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.46)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Environment.step(): t = 32
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': 0.9536056534843269, None: 0.937017119313653}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: right, reward: 0.0955960490301
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 3, 't': 32, 'action': 'right', 'reward': 0.09559604903012087, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.10)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Environment.step(): t = 33
state
{'forward': 0.7998693356494042, 'left': -0.0485734347393747, 'right': 0.7472727068412333, None: -2.2941735092357636}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: 0.377204266159
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 2, 't': 33, 'action': 'right', 'reward': 0.37720426615890235, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent followed the waypoint right. (rewarded 0.38)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Environment.step(): t = 34
state
{'forward': -4.975220594212877, 'left': -4.5639876857538715, 'right': 1.3211709831966352, None: 1.3916643087415634}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: 0.24867116419
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 1, 't': 34, 'action': None, 'reward': 0.24867116419034963, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 0.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 58
\-------------------------

Environment.reset(): Trial set up with start = (3, 6), destination = (7, 3), deadline = 35
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.745883956956815, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: left, reward: -19.1959543793
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', 'forward'), 'deadline': 35, 't': 0, 'action': 'left', 'reward': -19.19595437932283, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'left', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.20)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.5269068092091707, 'left': 0.13193180851617986, 'right': 1.0586369613300912, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: forward, reward: 0.109120625176
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 34, 't': 1, 'action': 'forward', 'reward': 0.10912062517611765, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove forward instead of right. (rewarded 0.11)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.226556723538481, 'left': -9.722162032851195, 'right': 1.3353129105549844, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: left, reward: -9.46931515523
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 33, 't': 2, 'action': 'left', 'reward': -9.469315155226727, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.47)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -10.226556723538481, 'left': -9.595738594038961, 'right': 1.3353129105549844, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: forward, reward: -10.3625470239
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 32, 't': 3, 'action': 'forward', 'reward': -10.362547023912494, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.36)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.7755185234019025, 'left': -10.225179724486539, 'right': 0.16065985993575088, None: -2.6876250480873725}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: forward, reward: 1.89174458533
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 31, 't': 4, 'action': 'forward', 'reward': 1.8917445853253774, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent followed the waypoint forward. (rewarded 1.89)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.576563711633343, 'left': 0.2510051396862819, 'right': 0.5830492809374964, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: forward, reward: 2.52399605361
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 30, 't': 5, 'action': 'forward', 'reward': 2.5239960536142085, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 2.52)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -10.294551873725489, 'left': -9.595738594038961, 'right': 1.3353129105549844, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: left, reward: -9.28128148201
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 29, 't': 6, 'action': 'left', 'reward': -9.281281482008783, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.28)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.085978102063929, 'left': 1.4799723763816648, 'right': 0.4589533924540907, None: -5.097396010542358}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: forward, reward: 1.10715407457
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 28, 't': 7, 'action': 'forward', 'reward': 1.1071540745707564, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 1.11)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.9051279624712755, 'left': 0.3801153022403645, 'right': 0.7599968239915724, None: -4.756192109807877}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: None, reward: -5.12194155523
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 27, 't': 8, 'action': None, 'reward': -5.121941555231096, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.12)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.9051279624712755, 'left': 0.3801153022403645, 'right': 0.7599968239915724, None: -4.939066832519487}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: 1.88015277415
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 26, 't': 9, 'action': 'forward', 'reward': 1.8801527741520745, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove forward instead of right. (rewarded 1.88)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.947731176021358, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.0209243139558204}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: 2.21047541755
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 25, 't': 10, 'action': None, 'reward': 2.2104754175478227, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.0143952663415368, 'left': 0.8773697728223144, 'right': 1.5173569771793467, None: -4.623101012165499}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: forward, reward: 0.721205139709
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 24, 't': 11, 'action': 'forward', 'reward': 0.7212051397086142, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.72)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -10.11330457205706, 'left': -8.667054633023076, 'right': 1.309253619358648, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: right, reward: 2.22621603348
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 23, 't': 12, 'action': 'right', 'reward': 2.226216033480462, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.23)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': -10.372266652618636, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: right, reward: 1.71798566322
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'right'), 'deadline': 22, 't': 13, 'action': 'right', 'reward': 1.7179856632158452, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'right')
Agent followed the waypoint right. (rewarded 1.72)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.5664528426475384, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -4.545506209287621}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: forward, reward: 2.77855082532
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 21, 't': 14, 'action': 'forward', 'reward': 2.778550825323519, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 2.78)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -5.165118412686815, 'left': -8.538905258779703, 'right': 0.39290176734632676, None: 1.2571843239247231}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: forward, reward: -9.28763629596
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 20, 't': 15, 'action': 'forward', 'reward': -9.287636295961999, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.29)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.55397234296628, 'left': -10.382538876444396, 'right': 0.40901593246502627, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: right, reward: 0.131733765166
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 19, 't': 16, 'action': 'right', 'reward': 0.13173376516645774, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.13)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.7590130592047367, 'left': -9.992381388078595, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: right, reward: 1.7069228138
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'forward'), 'deadline': 18, 't': 17, 'action': 'right', 'reward': 1.7069228138013697, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'forward')
Agent followed the waypoint right. (rewarded 1.71)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.947731176021358, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 2.1156998657518216}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: 1.62735962635
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 17, 't': 18, 'action': None, 'reward': 1.6273596263534318, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.5880469454065894, 'left': 0.3780471455299186, 'right': 0.5029408890303542, None: -2.2723477585706715}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: forward, reward: 0.847558871018
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 16, 't': 19, 'action': 'forward', 'reward': 0.8475588710175045, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove forward instead of right. (rewarded 0.85)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.8517347734094066, 'left': -0.13611540802838895, 'right': 1.5015998462085305, None: -4.941767725524494}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: right, reward: 1.34209319746
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 15, 't': 20, 'action': 'right', 'reward': 1.3420931974631602, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.34)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.3291813507472994, 'left': 0.8387466365283431, 'right': 1.1727471922073036, None: -5.287745084220335}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: left, reward: 0.798227116381
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 14, 't': 21, 'action': 'left', 'reward': 0.7982271163811347, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 0.80)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.591967914575033, None: -4.444318020142434}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: right, reward: 2.13394306192
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 13, 't': 22, 'action': 'right', 'reward': 2.133943061918467, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.13)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': -5.246695785050046, 'right': 0.7766342297453515, None: 1.0547657571588864}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: -9.64972054256
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 12, 't': 23, 'action': 'forward', 'reward': -9.649720542564813, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.65)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -4.8248602712824065, 'left': -5.246695785050046, 'right': 0.7766342297453515, None: 1.0547657571588864}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: left, reward: -9.24285594182
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 11, 't': 24, 'action': 'left', 'reward': -9.242855941820887, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent attempted driving left through a red light. (rewarded -9.24)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.14519273965189095, 'left': -9.64811268990952, 'right': 1.86295548824675, None: -4.444318020142434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: left, reward: -20.9794005947
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 10, 't': 25, 'action': 'left', 'reward': -20.979400594748014, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.98)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.7432517139351562, 'left': -9.657410605631151, 'right': 0.7100068876956095, None: -3.8069771162676878}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: 0.944767849701
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 9, 't': 26, 'action': 'forward', 'reward': 0.944767849700552, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent drove forward instead of right. (rewarded 0.94)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.9634319095607817, None: -3.7946263542830287}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 0.848616637832
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 8, 't': 27, 'action': 'right', 'reward': 0.8486166378319344, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 0.85)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 1.178577648505495, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 0.474605045569
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 7, 't': 28, 'action': 'right', 'reward': 0.47460504556923944, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.47)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.5628426300356566, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -4.452428525386306}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: -5.98193013615
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 6, 't': 29, 'action': None, 'reward': -5.981930136147632, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.98)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Environment.step(): t = 30
state
{'forward': 0.726002198685094, 'left': 1.4139011236151922, 'right': 0.4916810102821621, None: -4.046450134383133}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: forward, reward: 0.918691693578
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 5, 't': 30, 'action': 'forward', 'reward': 0.9186916935782694, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove forward instead of left. (rewarded 0.92)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Environment.step(): t = 31
state
{'forward': 0.46537506677840024, 'left': 1.5970771884227684, 'right': 0.3469593229001174, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: right, reward: 0.251164245186
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 4, 't': 31, 'action': 'right', 'reward': 0.25116424518580105, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.25)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Environment.step(): t = 32
state
{'forward': 0.3291813507472994, 'left': 0.8184868764547389, 'right': 1.1727471922073036, None: -5.287745084220335}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: 1.16212487505
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 3, 't': 32, 'action': 'forward', 'reward': 1.1621248750519597, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded 1.16)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Environment.step(): t = 33
state
{'forward': -9.947731176021358, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: -9.74084564297
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 2, 't': 33, 'action': 'forward', 'reward': -9.74084564297405, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.74)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Environment.step(): t = 34
state
{'forward': -9.844288409497704, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: -9.21210144892
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 34, 'action': 'forward', 'reward': -9.21210144891845, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.21)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 59
\-------------------------

Environment.reset(): Trial set up with start = (3, 5), destination = (7, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 2.0502798826237756, 'left': 0.2510051396862819, 'right': 0.5830492809374964, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: 0.352190853488
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 0.3521908534884399, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.35)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.455517593438483, 'left': -9.00836741228929, 'right': 0.14458206381830013, None: 1.0600988449670785}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 1.86860427242
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 24, 't': 1, 'action': None, 'reward': 1.8686042724234193, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 0.554274526567908, None: 1.9245028729834366}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: right, reward: 0.655025701748
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 0.6550257017482595, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.66)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -20.385878494423228, 'left': -20.415567100483972, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: 1.32753683512
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.3275368351221821, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.33)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -10.294551873725489, 'left': -9.438510038023871, 'right': 1.3353129105549844, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: left, reward: -9.11947781204
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -9.11947781203903, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.12)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -5.341285613501723, 'left': -4.9304179776708486, 'right': 0.8369463408474729, None: 1.282521002309312}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: 1.3395767902
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 20, 't': 5, 'action': None, 'reward': 1.339576790202412, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 2.1725018339855287, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -4.545506209287621}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: 1.80352655559
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 1.8035265555909765, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.80)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.7852957097207534}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: -10.3239073774
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'right'), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -10.323907377395528, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', 'right')
Agent attempted driving forward through a red light. (rewarded -10.32)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: right, reward: 1.15816127118
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.1581612711833627, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent drove right instead of forward. (rewarded 1.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -9.55397234296628, 'left': -10.382538876444396, 'right': 0.27037484881574203, None: 1.5761989240174057}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: None, reward: 1.88747804921
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 16, 't': 9, 'action': None, 'reward': 1.8874780492131962, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.89)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': 0.48055691662313127, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: right, reward: 0.834204832512
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 0.8342048325121593, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 0.83)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.46537506677840024, 'left': 1.5970771884227684, 'right': 0.29906178404295924, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: forward, reward: 1.45037350609
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.4503735060938077, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.45)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -10.294551873725489, 'left': -9.27899392503145, 'right': 1.3353129105549844, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: 0.572676640487
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 0.572676640486607, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.57)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.4506600629335881, 'left': 0.42291577324947194, 'right': 0.0, None: 0.685848835362945}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: right, reward: 1.00782205104
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'right'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 1.0078220510388602, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'right')
Agent drove right instead of left. (rewarded 1.01)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -4.605589689186562, 'left': -4.78455040624589, 'right': 0.5764593467306396, None: 1.132371409681068}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: -10.03240846
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -10.03240845997745, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.03)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -7.318999074582006, 'left': -4.78455040624589, 'right': 0.5764593467306396, None: 1.132371409681068}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: 0.693517020143
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 10, 't': 15, 'action': None, 'reward': 0.693517020142906, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 0.69)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.9880141947882526, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -4.545506209287621}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: -5.54905259864
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -5.549052598640035, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.4596626084991584, 'left': -15.392635291459799, 'right': 0.34560899567456205, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: left, reward: -19.8967120857
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -19.89671208570255, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.90)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 2.075370254530051, 'left': 0.4008901216934816, 'right': 0.15353933489711996, None: -4.702960818661083}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: -5.13357737725
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 7, 't': 18, 'action': None, 'reward': -5.133577377248609, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.13)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.7707211542480219, 'left': 0.20759633159991853, 'right': 0.0, None: 0.3743108077441025}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: right, reward: 1.23560540846
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 1.235605408463413, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent drove right instead of forward. (rewarded 1.24)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -4.9249023521925785, 'left': -4.560529175938716, 'right': 0.4169873267604316, None: 0.4268911878731215}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: -9.9470274494
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -9.947027449404803, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.95)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.7860201112177005}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: -20.9156971401
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -20.91569714012769, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.92)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: -19.9894267285
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', 'forward'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -19.98942672850543, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.99)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': -9.994713364252714, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: None, reward: -4.59709998747
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', 'forward'), 'deadline': 2, 't': 23, 'action': None, 'reward': -4.597099987469592, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.60)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.6814830777923697, 'left': 1.4937546672883477, 'right': 0.448890874629896, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: right, reward: -0.569055135881
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': -0.5690551358809888, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded -0.57)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 60
\-------------------------

Environment.reset(): Trial set up with start = (3, 5), destination = (1, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.9880141947882526, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -5.047279403963827}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: forward, reward: 1.12159450845
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 1.121594508446506, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.12)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: None, reward: -5.08922931999
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', 'left'), 'deadline': 19, 't': 1, 'action': None, 'reward': -5.089229319994948, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.544614659997474}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: right, reward: 1.91249251695
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', 'left'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 1.9124925169493419, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', 'left')
Agent drove right instead of forward. (rewarded 1.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.7023585205754569, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: -5.39120764544
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'right'), 'deadline': 17, 't': 3, 'action': None, 'reward': -5.391207645435775, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.39)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.957874286436104, 'left': 1.5970771884227684, 'right': 0.29906178404295924, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: 1.02410654277
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 16, 't': 4, 'action': 'left', 'reward': 1.0241065427702951, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.02)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -20.22047945123705, 'left': 0.0, 'right': -9.861974581816915, None: 1.0998975479105964}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: -40.3369992629
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'forward'), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -40.336999262927485, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.34)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.82114694993356, 'left': -10.803634885362516, 'right': 0.7772043591398556, None: 1.5090471076222443}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: None, reward: 2.17766453923
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 14, 't': 6, 'action': None, 'reward': 2.1776645392306695, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: -40.3378727126
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'right', 'forward'), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -40.33787271257784, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.34)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -20.207054466677707, 'left': -19.65953581119593, 'right': 1.0759170757274736, None: 0.9549383128618697}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: None, reward: 1.09612413643
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 12, 't': 8, 'action': None, 'reward': 1.096124136427999, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 1.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.513288872267108, 'left': 1.5073906032399342, 'right': 1.7138782790864349, None: 0.47522944267498085}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: forward, reward: 0.480429966878
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'forward', 'reward': 0.4804299668775991, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 0.48)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: right, reward: 0.971217813992
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 0.9712178139923331, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'left', None)
Agent followed the waypoint right. (rewarded 0.97)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -5.0133505645022485, 'left': 0.0, 'right': 0.0, None: 0.8703294085273404}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: left, reward: -9.90818983562
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -9.908189835621407, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -9.91)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.82114694993356, 'left': -10.803634885362516, 'right': 0.7772043591398556, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: left, reward: -9.5805503292
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -9.580550329196184, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.58)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.82114694993356, 'left': -10.19209260727935, 'right': 0.7772043591398556, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: right, reward: 1.38808080024
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 1.3880808002407388, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.39)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 61
\-------------------------

Environment.reset(): Trial set up with start = (2, 6), destination = (7, 3), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -9.528194929208077, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: -10.1196894534
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -10.119689453432777, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.12)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -29.694482379191278, 'left': -29.94481238143355, 'right': -14.998322753551045, None: 2.179853355677282}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: right, reward: -20.4354817566
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 29, 't': 1, 'action': 'right', 'reward': -20.435481756629358, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.44)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.823942191320427, 'left': -9.97706538984961, 'right': 1.0978046028116835, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: right, reward: 2.15956138454
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 2.1595613845350012, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.16)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -20.207054466677707, 'left': -19.65953581119593, 'right': 1.0759170757274736, None: 1.0255312246449344}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -40.63429692
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 27, 't': 3, 'action': 'forward', 'reward': -40.6342969200415, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.63)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.82114694993356, 'left': -10.19209260727935, 'right': 1.0826425796902972, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -9.5680058955
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': -9.568005895496944, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.57)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.69457642271525, 'left': -10.19209260727935, 'right': 1.0826425796902972, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -10.7790307258
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 25, 't': 5, 'action': 'forward', 'reward': -10.779030725838407, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.78)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4968594195723535, 'left': 1.5073906032399342, 'right': 1.7138782790864349, None: 0.47522944267498085}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: left, reward: 1.23324224929
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 24, 't': 6, 'action': 'left', 'reward': 1.2332422492941442, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.23)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.717802908212047, 'left': 0.3780471455299186, 'right': 0.5029408890303542, None: -2.2723477585706715}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: None, reward: -4.45398340824
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 23, 't': 7, 'action': None, 'reward': -4.453983408237931, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.45)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.8678002030250755, 'left': 0.8773697728223144, 'right': 1.5173569771793467, None: -4.623101012165499}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: right, reward: 1.01666457674
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 22, 't': 8, 'action': 'right', 'reward': 1.0166645767363955, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.02)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -9.823942191320427, 'left': -9.97706538984961, 'right': 1.6286829936733422, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: right, reward: 0.96424373816
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 21, 't': 9, 'action': 'right', 'reward': 0.9642437381595439, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 0.96)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.44863133736615, 'left': 0.9674211914728518, 'right': 0.18717668054371284, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: left, reward: 1.71980247097
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 20, 't': 10, 'action': 'left', 'reward': 1.7198024709670399, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 1.72)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': -4.631373817507978, 'right': 0.0, None: 1.003461754419426}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -9.20540939343
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'left'), 'deadline': 19, 't': 11, 'action': 'forward', 'reward': -9.205409393427313, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.21)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.8984010627443128, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -9.58751743759
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', 'left'), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': -9.587517437588556, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -9.59)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.14519273965189095, 'left': -15.313756642328768, 'right': 1.86295548824675, None: -4.444318020142434}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: 0.22088934942
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 17, 't': 13, 'action': 'forward', 'reward': 0.2208893494195292, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent drove forward instead of right. (rewarded 0.22)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.823942191320427, 'left': -9.97706538984961, 'right': 1.296463365916443, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: left, reward: -9.87867022165
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -9.878670221650546, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.88)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.8678002030250755, 'left': 0.8773697728223144, 'right': 1.2670107769578711, None: -4.623101012165499}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: forward, reward: 0.765054890886
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': 0.765054890885973, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.77)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.18304104453571007, 'left': -15.313756642328768, 'right': 1.86295548824675, None: -4.444318020142434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: -19.1547048732
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 14, 't': 16, 'action': 'left', 'reward': -19.154704873243006, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.15)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.2802365880189883, 'right': 0.3150230225378734, None: -0.31475107338701513}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: forward, reward: 0.824077835299
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', None), 'deadline': 13, 't': 17, 'action': 'forward', 'reward': 0.8240778352985862, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'right', None)
Agent drove forward instead of right. (rewarded 0.82)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -10.11330457205706, 'left': -8.667054633023076, 'right': 1.767734826419555, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: right, reward: 1.45532628212
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 12, 't': 18, 'action': 'right', 'reward': 1.4553262821234563, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 1.46)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -34.82762629762413, 'left': -29.99532306776711, 'right': -0.11811137397118186, None: 1.1346035314440637}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: None, reward: 1.29500638179
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 11, 't': 19, 'action': None, 'reward': 1.2950063817869648, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 1.30)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -10.294551873725489, 'left': -9.27899392503145, 'right': 0.9539947755207957, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: left, reward: -9.69751903626
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 10, 't': 20, 'action': 'left', 'reward': -9.697519036262594, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.70)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.284189309951646, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.445165222771701}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: None, reward: 2.44253575838
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 9, 't': 21, 'action': None, 'reward': 2.442535758384782, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.44)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.7707211542480219, 'left': 0.20759633159991853, 'right': 0.6178027042317065, None: 0.3743108077441025}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: left, reward: 1.36026145173
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 8, 't': 22, 'action': 'left', 'reward': 1.3602614517315863, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent drove left instead of forward. (rewarded 1.36)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.8164275469555242, 'left': 0.8773697728223144, 'right': 1.2670107769578711, None: -4.623101012165499}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: None, reward: -5.60636173003
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 23, 'action': None, 'reward': -5.606361730031311, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.61)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.8164275469555242, 'left': 0.8773697728223144, 'right': 1.2670107769578711, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: forward, reward: 0.318892556107
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 6, 't': 24, 'action': 'forward', 'reward': 0.31889255610733014, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.32)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': -10.236803574276829, 'left': -10.19209260727935, 'right': 1.0826425796902972, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 1.71403381289
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 25, 'action': 'right', 'reward': 1.7140338128930763, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.71)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: -2.555502637730306}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: forward, reward: 0.987164694012
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', 'forward', 'left'), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': 0.987164694011589, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'forward', 'left')
Agent followed the waypoint forward. (rewarded 0.99)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 0.6046501141580838, None: 1.9245028729834366}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: right, reward: -0.700578934993
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 3, 't': 27, 'action': 'right', 'reward': -0.7005789349932188, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded -0.70)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -4.602704696713657, 'left': -4.631373817507978, 'right': 0.0, None: 1.003461754419426}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: right, reward: 2.1250633742
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'left'), 'deadline': 2, 't': 28, 'action': 'right', 'reward': 2.125063374195552, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, 'left')
Agent followed the waypoint right. (rewarded 2.13)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.0, 'left': -20.30139091121564, 'right': 1.264822589743049, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: forward, reward: -10.3389860172
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 1, 't': 29, 'action': 'forward', 'reward': -10.338986017220147, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.34)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 62
\-------------------------

Environment.reset(): Trial set up with start = (3, 5), destination = (8, 7), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': 0.5246008512572239, None: 0.937017119313653}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: None, reward: 2.21636041434
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 25, 't': 0, 'action': None, 'reward': 2.2163604143390527, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': -5.490105629043586, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 1.88362159212
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', None), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 1.883621592117469, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'left', None)
Agent drove right instead of left. (rewarded 1.88)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.3120765974254924, 'right': 0.9894480298872794, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: forward, reward: 0.349763206526
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', 'left'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 0.3497632065262952, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'left')
Agent drove forward instead of right. (rewarded 0.35)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 0.8265913470373673, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: right, reward: 0.127760094771
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 0.1277600947705979, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.13)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -7.226377354324407, 'left': -8.538905258779703, 'right': 0.39290176734632676, None: 1.2571843239247231}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: right, reward: 0.115526701575
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 0.11552670157465939, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent drove right instead of left. (rewarded 0.12)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.8301721299516381, 'right': 0.17523106684103096, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: 2.85578579995
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': 2.855785799946386, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent followed the waypoint forward. (rewarded 2.86)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -7.377115111848962, 'left': -9.485295079824343, 'right': 0.7275926038090874, None: 1.543589404546968}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 2.03770455029
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 19, 't': 6, 'action': None, 'reward': 2.037704550293075, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.04)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -20.385878494423228, 'left': -20.415567100483972, 'right': 0.0, None: 0.6637684175610911}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: -20.149331502
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': -20.14933150204142, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.15)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -5.167738838026643, 'left': -5.173426484474621, 'right': 0.4903315088640395, None: 0.6927168675640749}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 1.74360690282
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 17, 't': 8, 'action': None, 'reward': 1.7436069028247392, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 1.74)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.7707211542480219, 'left': 0.7839288916657524, 'right': 0.6178027042317065, None: 0.3743108077441025}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 0.0444122488565
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 0.044412248856454006, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent drove right instead of forward. (rewarded 0.04)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -8.290101299485517, 'left': -9.59961116239367, 'right': 0.19096472994183833, None: 1.1314452457023287}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: left, reward: -9.85596979209
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 15, 't': 10, 'action': 'left', 'reward': -9.855969792087134, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.86)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.7735950300487101, 'left': -10.439104395112189, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: 1.39419591844
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.3941959184351709, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded 1.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -20.407072234619378, 'right': 0.10768731177632035, None: 0.8714150576196961}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: -40.6110786398
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -40.61107863976472, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.61)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.7023585205754569, None: -2.6956038227178873}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: 0.0192213713153
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'right'), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 0.019221371315266467, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'right')
Agent drove forward instead of left. (rewarded 0.02)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.142502397782994, 'left': 1.6058359033016343, 'right': 1.2867275571828292, None: -4.604801035408901}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: forward, reward: 1.42648070442
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': 1.4264807044150736, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 1.43)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.957874286436104, 'left': 1.3105918655965318, 'right': 0.29906178404295924, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: left, reward: 1.3782327122
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': 'left', 'reward': 1.37823271219641, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.38)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -10.294551873725489, 'left': -9.488256480647022, 'right': 0.9539947755207957, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: left, reward: -10.5103010424
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -10.510301042448479, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.51)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.0965660883173427, 'left': 1.4799723763816648, 'right': 0.4589533924540907, None: -5.097396010542358}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: left, reward: 0.927353779527
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 8, 't': 17, 'action': 'left', 'reward': 0.9273537795268925, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove left instead of forward. (rewarded 0.93)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.9060242736963581, None: -3.7946263542830287}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: -4.15551957341
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 7, 't': 18, 'action': None, 'reward': -4.155519573410364, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.16)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.9060242736963581, None: -3.975072963846696}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: -5.79315860231
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 6, 't': 19, 'action': None, 'reward': -5.793158602308273, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.79)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.251448841481128, 'left': 0.4506458242372434, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: -4.34343902976
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'right', 'forward'), 'deadline': 5, 't': 20, 'action': None, 'reward': -4.343439029759328, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.34)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -8.294232586630605, 'left': -7.188380902122997, 'right': 1.9225365880108047, None: 1.1878901121921879}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: 0.531701084045
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 4, 't': 21, 'action': None, 'reward': 0.5317010840454923, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 0.53)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -30.420675693359605, 'left': -19.65953581119593, 'right': 1.0759170757274736, None: 1.0255312246449344}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 0.956553677394
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 3, 't': 22, 'action': 'right', 'reward': 0.9565536773941714, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent followed the waypoint right. (rewarded 0.96)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 0.4771757209039826, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 0.40306565089
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.40306565088963064, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.40)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.6814830777923697, 'left': 1.4937546672883477, 'right': -0.060082130625546376, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: left, reward: 0.139046911509
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 1, 't': 24, 'action': 'left', 'reward': 0.13904691150891102, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent followed the waypoint left. (rewarded 0.14)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 63
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (3, 3), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -10.11330457205706, 'left': -8.667054633023076, 'right': 1.6115305542715057, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: right, reward: 1.14048471777
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 1.1404847177711213, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 1.14)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 2.0502798826237756, 'left': 0.2510051396862819, 'right': 0.46762006721296817, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: forward, reward: 1.36244617563
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 1.36244617562875, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 1.36)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.44863133736615, 'left': 1.3436118312199459, 'right': 0.18717668054371284, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: 0.319787097744
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': 0.3197870977440964, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.32)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.31801371719264415, 'left': 0.13193180851617986, 'right': 1.0586369613300912, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: left, reward: 0.904996151282
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 0.9049961512817879, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove left instead of right. (rewarded 0.90)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -4.975220594212877, 'left': -4.5639876857538715, 'right': 1.3211709831966352, None: 0.8201677364659565}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: left, reward: -9.6193890605
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -9.619389060495621, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.62)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -10.236803574276829, 'left': -10.19209260727935, 'right': 1.3983381962916868, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: right, reward: 1.77490379664
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 1.7749037966407728, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.77)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.823942191320427, 'left': -9.927867805750079, 'right': 1.296463365916443, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: forward, reward: -10.8126656748
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.812665674847159, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -10.318303933083794, 'left': -9.927867805750079, 'right': 1.296463365916443, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: right, reward: 2.29300955252
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 2.293009552523486, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.29)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.0965660883173427, 'left': 1.2036630779542787, 'right': 0.4589533924540907, None: -5.097396010542358}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: left, reward: 0.369186711945
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 0.3691867119448412, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent drove left instead of forward. (rewarded 0.37)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.9060242736963581, None: -4.884115783077484}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: None, reward: -5.61764157116
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 16, 't': 9, 'action': None, 'reward': -5.617641571159907, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.62)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -20.397068747535524, 'left': -30.278193086275486, 'right': 1.067865206031248, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: left, reward: -39.5436917214
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 15, 't': 10, 'action': 'left', 'reward': -39.54369172135989, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.54)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -20.202140757956546, 'left': 0.0, 'right': -9.578054242763955, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: left, reward: -39.6663327741
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -39.66633277408947, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.67)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -10.236803574276829, 'left': -10.19209260727935, 'right': 1.58662099646623, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: forward, reward: -9.28196159895
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -9.281961598947412, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.28)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -0.2367538979223509, 'left': 0.0, 'right': 0.5304258197177816, None: -0.11886393746254198}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: left, reward: 0.386121953009
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 0.38612195300934915, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent drove left instead of right. (rewarded 0.39)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -4.95562719072588, 'left': -4.676426717449235, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: 1.00428730312
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', 'right'), 'deadline': 11, 't': 14, 'action': None, 'reward': 1.0042873031242556, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'right')
Agent properly idled at a red light. (rewarded 1.00)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.6877294880707738, 'left': 1.1651622902728445, 'right': 0.9619268042883712, None: -4.023393744880231}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: right, reward: 1.47305305189
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 10, 't': 15, 'action': 'right', 'reward': 1.4730530518851086, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.47)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.5676600515314272, 'left': 0.8773697728223144, 'right': 1.2670107769578711, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: right, reward: 0.994552714978
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 0.994552714978072, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 0.99)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: None, reward: 2.370221044
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 8, 't': 17, 'action': None, 'reward': 2.3702210439956937, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent properly idled at a red light. (rewarded 2.37)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: left, reward: -10.858256986
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -10.858256986043116, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.86)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': -10.171716032974075, 'right': 0.008474242711148494, None: -2.247195820641637}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: 1.19223994167
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': 1.1922399416740834, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent followed the waypoint forward. (rewarded 1.19)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 1.0459757401014351, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: -20.508908319
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'right', 'forward', None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -20.50890831898185, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'forward', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.51)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.0, 'left': -10.288956745639254, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: None, reward: -4.48941274797
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', 'right'), 'deadline': 4, 't': 21, 'action': None, 'reward': -4.4894127479680535, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -4.49)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -20.38976340806912, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: right, reward: -20.9783652263
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', 'forward', 'forward', None), 'deadline': 3, 't': 22, 'action': 'right', 'reward': -20.978365226273674, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.98)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -5.437105507680951, 'left': -4.903094568391228, 'right': 0.44012068589680664, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: -10.8063834998
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': -10.806383499833379, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.81)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -8.121744503757165, 'left': -4.903094568391228, 'right': 0.44012068589680664, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: -10.4794542129
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -10.479454212889873, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.48)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 64
\-------------------------

Environment.reset(): Trial set up with start = (3, 3), destination = (6, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -8.290101299485517, 'left': -9.7277904772404, 'right': 0.19096472994183833, None: 1.1314452457023287}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 2.89621508064
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 25, 't': 0, 'action': None, 'reward': 2.896215080638771, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.90)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -7.226377354324407, 'left': -8.538905258779703, 'right': 0.25421423446049307, None: 1.2571843239247231}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 2.97938351888
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 24, 't': 1, 'action': None, 'reward': 2.979383518878052, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 2.98)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -4.837167337146998, 'left': 0.0, 'right': 0.6703735136509539, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: None, reward: 2.52573233231
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.525732332313342, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent properly idled at a red light. (rewarded 2.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.55397234296628, 'left': -10.382538876444396, 'right': 0.27037484881574203, None: 1.731838486615301}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: forward, reward: -9.46077615541
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': -9.4607761554064, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.46)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': 0.6573808745676453, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: right, reward: 0.302206344695
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 0.3022063446949793, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 0.30)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -10.318303933083794, 'left': -9.927867805750079, 'right': 1.7947364592199646, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: right, reward: 1.59153439322
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 1.5915343932194854, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.59)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.30059935832352, 'left': -4.903094568391228, 'right': 0.44012068589680664, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: forward, reward: -9.64035255497
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -9.640352554965187, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.64)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.5961199708370417, 'left': -10.171716032974075, 'right': 0.008474242711148494, None: -2.247195820641637}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: right, reward: 1.76685199293
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.7668519929328594, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent drove right instead of forward. (rewarded 1.77)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.50737424918634, 'left': -10.382538876444396, 'right': 0.27037484881574203, None: 1.731838486615301}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: -10.3869568088
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 17, 't': 8, 'action': 'left', 'reward': -10.386956808843767, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.39)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -4.837167337146998, 'left': 0.0, 'right': 0.6703735136509539, None: 1.262866166156671}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: -10.2142942798
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': -10.214294279787769, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.21)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.4838107402304995, 'left': 1.2994466021540823, 'right': 0.4797936096313123, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: left, reward: 2.36032615699
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 15, 't': 10, 'action': 'left', 'reward': 2.360326156994335, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 2.36)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5790806355916813, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: left, reward: -39.5581538408
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -39.55815384083132, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.56)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': -19.77907692041566, 'right': 0.5790806355916813, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: forward, reward: -39.5622080678
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -39.56220806783058, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.56)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -4.852981348741553, 'left': -7.328857101738256, 'right': 0.22813951525488635, None: 0.6764920457085781}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: None, reward: 2.58142870639
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 12, 't': 13, 'action': None, 'reward': 2.581428706394112, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 2.58)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -5.161953688697764, 'left': 0.0, 'right': 0.0, None: 0.7852957097207534}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: right, reward: 1.41110992761
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'right'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.4111099276126473, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', 'right')
Agent drove right instead of forward. (rewarded 1.41)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.6877294880707738, 'left': 1.1651622902728445, 'right': 1.21748992808674, None: -4.023393744880231}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: left, reward: 0.766622342754
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 10, 't': 15, 'action': 'left', 'reward': 0.7666223427538277, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent followed the waypoint left. (rewarded 0.77)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -7.724768135081817, 'left': -7.533654060689663, 'right': 0.6289009916621053, None: 0.7671410642029802}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: left, reward: -9.16289602513
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -9.162896025131484, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.16)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.5676600515314272, 'left': 0.8773697728223144, 'right': 1.1307817459679717, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: left, reward: -0.040685915665
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -0.04068591566499968, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded -0.04)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.8440097818178541, 'left': -9.657410605631151, 'right': 0.7100068876956095, None: -3.8069771162676878}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: forward, reward: -0.153323753292
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -0.15332375329214631, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent drove forward instead of right. (rewarded -0.15)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: right, reward: 0.466276371866
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'forward'), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 0.4662763718656987, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'forward')
Agent drove right instead of forward. (rewarded 0.47)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.3449051593103656, None: -4.774430435787325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: right, reward: 0.566712790953
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 0.566712790953346, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 0.57)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -20.397068747535524, 'left': -34.910942403817685, 'right': 1.067865206031248, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: right, reward: 1.44221686856
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 4, 't': 21, 'action': 'right', 'reward': 1.4422168685635957, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 1.44)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -30.08356539191574, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 1.8444774012754064}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: None, reward: 1.30942404593
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 3, 't': 22, 'action': None, 'reward': 1.3094240459341342, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 1.31)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.6814830777923697, 'left': 0.8164007893986294, 'right': -0.060082130625546376, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: 0.874991492975
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': 0.8749914929749933, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove forward instead of left. (rewarded 0.87)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.50737424918634, 'left': -10.384747842644082, 'right': 0.27037484881574203, None: 1.731838486615301}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: forward, reward: -10.7767466758
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -10.776746675765011, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.78)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 65
\-------------------------

Environment.reset(): Trial set up with start = (5, 6), destination = (1, 4), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.1851105219978468}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: right, reward: 1.52531071963
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 30, 't': 0, 'action': 'right', 'reward': 1.5253107196318814, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent drove right instead of forward. (rewarded 1.53)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.4783397258548069, 'left': 1.642835318609392, 'right': 1.1953234434716113, None: -5.6069980308293355}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: None, reward: -4.84866415087
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 29, 't': 1, 'action': None, 'reward': -4.848664150868183, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.85)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.4783397258548069, 'left': 1.642835318609392, 'right': 1.1953234434716113, None: -5.22783109084876}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: None, reward: -4.81254052187
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 28, 't': 2, 'action': None, 'reward': -4.812540521874153, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.81)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.3106204747259736, 'left': -9.813445973183716, 'right': 0.0, None: -2.0691990998302656}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: right, reward: 0.00443928755645
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 27, 't': 3, 'action': 'right', 'reward': 0.004439287556452842, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent drove right instead of left. (rewarded 0.00)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': -19.85217377853075, 'right': 0.0, None: 0.8808114996176329}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: right, reward: -20.6883206286
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'left'), 'deadline': 26, 't': 4, 'action': 'right', 'reward': -20.68832062856046, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.69)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -19.78110403391529, 'left': -19.77907692041566, 'right': 0.5790806355916813, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: None, reward: 2.0357583824
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 25, 't': 5, 'action': None, 'reward': 2.0357583824016254, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent properly idled at a red light. (rewarded 2.04)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.284189309951646, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.9438504905782414}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: forward, reward: -9.97359042624
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 24, 't': 6, 'action': 'forward', 'reward': -9.97359042623929, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.97)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.7063630291262628, 'left': 0.2510051396862819, 'right': 0.46762006721296817, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, -1), action: right, reward: 1.12056910277
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 1.1205691027698848, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 1.12)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.39721910935771626, 'left': 0.3587581722439006, 'right': 0.5284232638443994, None: -2.577849171873072}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: 1.75198440561
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 22, 't': 8, 'action': 'left', 'reward': 1.7519844056079363, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent followed the waypoint left. (rewarded 1.75)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.7063630291262628, 'left': 0.2510051396862819, 'right': 0.7940945849914265, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: 2.04328876852
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': 2.04328876851845, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 2.04)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': -19.646596381419585, 'right': -9.648374887274155, None: 1.3732820516067057}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: -40.3133331899
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': -40.31333318991118, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.31)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.628889868095468, 'left': -10.03048644729865, 'right': 1.0586079102212795, None: 1.9438504905782414}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: left, reward: -10.5151372304
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 19, 't': 11, 'action': 'left', 'reward': -10.515137230411192, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.52)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.07208153369590409, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: left, reward: -9.36869287661
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'right', None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': -9.36869287660867, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -9.37)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': 0.23313818593284935, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: left, reward: 0.131100114208
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'forward'), 'deadline': 17, 't': 13, 'action': 'left', 'reward': 0.13110011420760692, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'forward')
Agent drove left instead of forward. (rewarded 0.13)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.7456531128996295, 'left': 0.8184868764547389, 'right': 1.1727471922073036, None: -5.287745084220335}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: None, reward: -4.59205534304
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 16, 't': 14, 'action': None, 'reward': -4.592055343038796, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.59)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.5987919255930934, 'left': -0.2770087368332614, 'right': 0.858053421636954, None: -4.781732922308956}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: right, reward: 1.54985192463
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 15, 't': 15, 'action': 'right', 'reward': 1.5498519246294984, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent followed the waypoint right. (rewarded 1.55)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.957874286436104, 'left': 1.344412288896471, 'right': 0.29906178404295924, None: -5.262292798011451}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: None, reward: -5.78352527911
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 14, 't': 16, 'action': None, 'reward': -5.783525279107146, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.78)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -0.23202695925459793, 'left': 0.6503390950632522, 'right': 0.373341258972024, None: 0.41985379464947914}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: None, reward: -0.156250226269
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 13, 't': 17, 'action': None, 'reward': -0.1562502262689478, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded -0.16)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.5628426300356566, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -5.217179330766969}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: None, reward: -4.53754828777
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 12, 't': 18, 'action': None, 'reward': -4.537548287773054, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.54)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: forward, reward: -10.2147599157
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', 'left', 'right'), 'deadline': 11, 't': 19, 'action': 'forward', 'reward': -10.214759915693017, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -10.21)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: None, reward: 0.778978857532
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', 'left', None), 'deadline': 10, 't': 20, 'action': None, 'reward': 0.7789788575324714, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'left', None)
Agent properly idled at a red light. (rewarded 0.78)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': -0.04796441041756749, None: 1.9245028729834366}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: right, reward: 0.506881837335
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 0.5068818373353864, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.51)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.4596626084991584, 'left': -17.644673688581175, 'right': 0.34560899567456205, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: -19.5779252953
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 8, 't': 22, 'action': 'left', 'reward': -19.57792529525325, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.58)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.5548043516173793, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -5.047279403963827}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: None, reward: -5.95427576883
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 7, 't': 23, 'action': None, 'reward': -5.954275768834125, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.5548043516173793, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -5.500777586398977}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: forward, reward: 1.50394630127
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 6, 't': 24, 'action': 'forward', 'reward': 1.5039463012683723, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.50)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 1.5293753264428758, 'left': 0.8268320953510937, 'right': 1.047161213415488, None: -5.500777586398977}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: right, reward: -0.0410745036355
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 25, 'action': 'right', 'reward': -0.041074503635538884, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded -0.04)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 0.4838107402304995, 'left': 1.8298863795742086, 'right': 0.4797936096313123, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: forward, reward: 0.877029757925
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': 0.8770297579247857, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 0.88)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -19.942531269156774, 'left': -29.73932473240484, 'right': 0.5173330090882928, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: left, reward: -39.2886480423
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 3, 't': 27, 'action': 'left', 'reward': -39.288648042343326, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.29)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 0.22945871345890945, None: 1.9245028729834366}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: None, reward: 0.392426313612
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 2, 't': 28, 'action': None, 'reward': 0.3924263136115411, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.39)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.957874286436104, 'left': 1.344412288896471, 'right': 0.29906178404295924, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: left, reward: 1.7291434896
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 29, 'action': 'left', 'reward': 1.7291434896026123, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.73)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 66
\-------------------------

Environment.reset(): Trial set up with start = (3, 5), destination = (6, 4), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -10.318303933083794, 'left': -9.927867805750079, 'right': 1.6931354262197251, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: left, reward: -10.2607617733
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 20, 't': 0, 'action': 'left', 'reward': -10.260761773337908, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -7.724768135081817, 'left': -8.348275042910574, 'right': 0.6289009916621053, None: 0.7671410642029802}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: left, reward: -10.9140910032
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 19, 't': 1, 'action': 'left', 'reward': -10.914091003181912, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -10.91)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.318303933083794, 'left': -10.094314789543994, 'right': 1.6931354262197251, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 1.99113665866
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 1.9911366586622887, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.99)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.470475956644353, 'left': -4.903094568391228, 'right': 0.44012068589680664, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: left, reward: -10.0454515818
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -10.045451581767692, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -10.05)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.470475956644353, 'left': -7.474273075079459, 'right': 0.44012068589680664, None: 0.8298691618452592}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: None, reward: 2.12946066598
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 2.1294606659846504, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.4596626084991584, 'left': -18.611299491917215, 'right': 0.34560899567456205, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: right, reward: 0.305450395245
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 0.3054503952449905, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.31)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4783397258548069, 'left': 1.642835318609392, 'right': 1.1953234434716113, None: -5.020185806361456}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: left, reward: 2.7485794365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 2.7485794364969376, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.75)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.5961199708370417, 'left': -10.171716032974075, 'right': 0.887663117822004, None: -2.247195820641637}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: None, reward: -4.94722546727
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 13, 't': 7, 'action': None, 'reward': -4.947225467268311, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.95)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.838727297326762, 'left': 0.6877511845619184, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: forward, reward: 1.64556858637
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 1.6455685863665765, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 1.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.21647576783341127, None: -0.06986981125610517}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (0, 1), action: right, reward: 0.912785327635
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 0.9127853276346556, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent drove right instead of left. (rewarded 0.91)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.5293753264428758, 'left': 0.8268320953510937, 'right': 0.5030433548899745, None: -5.500777586398977}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: right, reward: 0.727360022147
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 0.7273600221467865, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.73)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.7049590726603364, 'left': 0.0, 'right': 0.5928150765757904, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: -5.19375470177
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'right', None), 'deadline': 9, 't': 11, 'action': None, 'reward': -5.193754701768921, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.19)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.957874286436104, 'left': 1.5367778892495416, 'right': 0.29906178404295924, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: 2.15000053651
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': 2.1500005365139003, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.957874286436104, 'left': 1.8433892128817209, 'right': 0.29906178404295924, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 0.812725072476
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': 0.8127250724760622, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 0.81)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.11249816202689822, 'left': 0.9689580603068322, 'right': 0.9767349331213432, None: 0.28929256697058636}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 1.26484833801
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 1.2648483380079323, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent followed the waypoint right. (rewarded 1.26)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.294551873725489, 'left': -9.99927876154775, 'right': 0.9539947755207957, None: 2.605566178962667}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: None, reward: 0.796492947016
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': 0.7964929470161675, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.80)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.470475956644353, 'left': -7.474273075079459, 'right': 0.44012068589680664, None: 1.479664913914955}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: right, reward: -0.130639875098
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': -0.1306398750984703, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded -0.13)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.837483406711816, None: 2.398077030131604}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: None, reward: 2.21805441057
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 3, 't': 17, 'action': None, 'reward': 2.218054410574519, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.22)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -10.142060462475676, 'left': -10.384747842644082, 'right': 0.27037484881574203, None: 1.731838486615301}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: right, reward: 0.627924930922
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 0.6279249309219412, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.63)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.18304104453571007, 'left': -17.234230757785888, 'right': 1.86295548824675, None: -4.444318020142434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: left, reward: -19.6583954705
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -19.658395470520283, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.66)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 67
\-------------------------

Environment.reset(): Trial set up with start = (1, 5), destination = (3, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.717802908212047, 'left': 0.3780471455299186, 'right': 0.5029408890303542, None: -3.3631655834043013}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (-1, 0), action: left, reward: 0.46219021684
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 20, 't': 0, 'action': 'left', 'reward': 0.46219021684032, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove left instead of right. (rewarded 0.46)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 0.22945871345890945, None: 1.1584645932974889}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: right, reward: 0.648438126831
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.6484381268306542, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.65)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.8517347734094066, 'left': -0.13611540802838895, 'right': 1.4218465218358454, None: -4.941767725524494}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: 1.50794303672
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 18, 't': 2, 'action': 'left', 'reward': 1.5079430367218705, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove left instead of right. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.6990550835836937, None: 1.153837155726019}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: forward, reward: -10.0054467812
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'right'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -10.00544678118261, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.01)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.35308188479542246, 'right': 0.0, None: -0.06799419960188136}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: forward, reward: 1.73103872849
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 1.731038728487413, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent drove forward instead of right. (rewarded 1.73)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: 1.45739640943
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'right'), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 1.4573964094254535, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'right')
Agent drove left instead of forward. (rewarded 1.46)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.7590130592047367, 'left': -9.992381388078595, 'right': 0.8534614069006848, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: -4.32402732533
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', None, 'forward'), 'deadline': 14, 't': 6, 'action': None, 'reward': -4.32402732533349, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.32)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -20.397068747535524, 'left': -34.910942403817685, 'right': 1.255041037297422, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -40.5978467831
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -40.597846783138756, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.60)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -5.186739908100557, 'left': -20.13555444050785, 'right': 0.5235464148227659, None: 1.067255681203651}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: left, reward: -39.4254266155
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -39.42542661551696, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -10.318303933083794, 'left': -10.094314789543994, 'right': 1.842136042441007, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 2.0162612209
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 2.0162612209042297, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.02)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -20.15666659495559, 'left': -19.646596381419585, 'right': -9.648374887274155, None: 1.3732820516067057}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: 1.29375867965
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 10, 't': 10, 'action': None, 'reward': 1.2937586796499763, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent properly idled at a red light. (rewarded 1.29)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -5.341285613501723, 'left': -4.9304179776708486, 'right': 0.8369463408474729, None: 1.311048896255862}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: left, reward: -10.5514357174
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -10.551435717397672, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -10.55)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.63407448875485, 'left': 0.0, 'right': 0.14932788298831878, None: 0.14449714315918194}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: left, reward: -39.8584446344
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -39.858444634376866, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.86)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.8748258988223565, 'left': 0.2510051396862819, 'right': 0.7940945849914265, None: 0.9243982450285968}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: -0.170259226282
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': -0.17025922628248724, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded -0.17)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.8748258988223565, 'left': 0.2510051396862819, 'right': 0.7940945849914265, None: 0.3770695093730548}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 1.13985210711
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 1.1398521071110377, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 1.14)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.4596626084991584, 'left': -18.611299491917215, 'right': 0.3255296954597763, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: left, reward: -19.9932621311
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -19.993262131095726, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.99)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.7421479418466692, 'left': 0.6877511845619184, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: left, reward: 0.921473817941
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 4, 't': 16, 'action': 'left', 'reward': 0.9214738179412072, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded 0.92)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.5676600515314272, 'left': 0.4183419285786574, 'right': 1.1307817459679717, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: left, reward: 0.13401923999
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': 0.13401923999037735, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.13)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.18304104453571007, 'left': -18.446313114153085, 'right': 1.86295548824675, None: -4.444318020142434}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: None, reward: -4.26829232514
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': -4.268292325143573, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.27)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -5.186739908100557, 'left': -29.780490528012407, 'right': 0.5235464148227659, None: 1.067255681203651}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: left, reward: -39.4031221956
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -39.40312219556832, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.40)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 68
\-------------------------

Environment.reset(): Trial set up with start = (5, 2), destination = (3, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -4.895726971810477, 'left': 0.0, 'right': 0.46038862309963297, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: left, reward: -9.90000188754
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 25, 't': 0, 'action': 'left', 'reward': -9.90000188754137, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.90)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': 0.0, 'right': 0.7626553598159407, None: 1.1851105219978468}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: forward, reward: -10.1532984826
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -10.153298482642214, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.15)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -4.895726971810477, 'left': -4.950000943770685, 'right': 0.46038862309963297, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: None, reward: 1.38245454685
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 23, 't': 2, 'action': None, 'reward': 1.3824545468477305, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent properly idled at a red light. (rewarded 1.38)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': -20.334621587219544, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: None, reward: 1.56217100425
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'forward'), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.5621710042542323, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 1.56)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.4596626084991584, 'left': -19.30228081150647, 'right': 0.3255296954597763, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: right, reward: 0.344730387289
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 0.3447303872888744, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.957874286436104, 'left': 1.3280571426788916, 'right': 0.29906178404295924, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: right, reward: 0.0639504470096
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 0.06395044700955166, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.06)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.974535434046215, 'left': -9.655298552263503, 'right': 0.4389484201447818, None: 1.1584645932974889}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: forward, reward: -9.47873467469
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -9.478734674691703, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.48)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.957874286436104, 'left': 1.3280571426788916, 'right': 0.18150611552625545, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: right, reward: 0.406202878076
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 0.4062028780763689, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -10.318303933083794, 'left': -10.094314789543994, 'right': 1.9291986316726182, None: 1.8715297460526266}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: None, reward: 2.63382848622
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.6338284862185635, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.63)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.262876599231653, 'left': -5.387928055003375, 'right': 1.0606258023337287, None: 1.0216675216986189}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: None, reward: 1.01029350479
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 16, 't': 9, 'action': None, 'reward': 1.010293504786564, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.01)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.5676600515314272, 'left': 0.27618058428451736, 'right': 1.1307817459679717, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: right, reward: 1.87379812703
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.8737981270269979, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.87)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.5293753264428758, 'left': 0.8268320953510937, 'right': 0.6152016885183805, None: -5.500777586398977}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: right, reward: 1.76193717835
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 1.7619371783464612, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.76)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.826311702735573, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 0.9581150107957778}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: -10.9823144822
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -10.982314482151434, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.98)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': -10.457848570063845, 'right': 0.0, None: -2.7860201112177005}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: forward, reward: 1.23929266432
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 1.2392926643187374, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', None)
Agent drove forward instead of left. (rewarded 1.24)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.7844915510990338, 'left': 1.6058359033016343, 'right': 1.2867275571828292, None: -4.604801035408901}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: None, reward: -4.33469344448
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 11, 't': 14, 'action': None, 'reward': -4.334693444480144, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.33)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -20.239288619459945, 'right': -9.949795710718272, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: forward, reward: -40.1068641755
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'right'), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -40.106864175480204, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.11)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.726635054368959, 'left': -9.655298552263503, 'right': 0.4389484201447818, None: 1.1584645932974889}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: None, reward: 0.673412774595
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': 0.6734127745953526, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.67)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -0.23202695925459793, 'left': 0.6503390950632522, 'right': 0.373341258972024, None: 0.13180178419026567}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: left, reward: 2.51191449635
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': 2.5119144963467805, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent followed the waypoint left. (rewarded 2.51)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -7.377115111848962, 'left': -9.485295079824343, 'right': 0.7275926038090874, None: 1.7906469774200215}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: -9.34770352515
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -9.347703525147754, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.35)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.2996712678479763, 'left': 0.0, 'right': 0.0, None: -2.7317545282608426}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: right, reward: 0.0861403017148
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'left'), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 0.08614030171483533, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'left')
Agent drove right instead of forward. (rewarded 0.09)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 0.9558089751318558, None: -4.774430435787325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: right, reward: 1.37491289779
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 1.3749128977882517, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 1.37)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.4120389176492931, 'left': 0.2802365880189883, 'right': 0.3150230225378734, None: -0.31475107338701513}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: None, reward: 0.937973980683
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', None), 'deadline': 4, 't': 21, 'action': None, 'reward': 0.9379739806830814, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.94)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -8.294232586630605, 'left': -7.188380902122997, 'right': 1.9225365880108047, None: 0.8597955981188401}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: None, reward: 1.13265780621
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 3, 't': 22, 'action': None, 'reward': 1.1326578062132824, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.13)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -10.11330457205706, 'left': -8.667054633023076, 'right': 1.3760076360213134, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: forward, reward: -10.0045083153
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': -10.00450831526657, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.00)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: left, reward: -39.1046530139
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 1, 't': 24, 'action': 'left', 'reward': -39.10465301393219, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.10)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 69
\-------------------------

Environment.reset(): Trial set up with start = (3, 2), destination = (8, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.1531639796171882, 'left': 0.7176682277388182, 'right': 0.4254638151155375, None: -4.199422231553411}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: forward, reward: 1.9418400218
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': 1.941840021799663, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent followed the waypoint forward. (rewarded 1.94)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.7421479418466692, 'left': 0.8046125012515628, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: left, reward: 1.97733280397
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 24, 't': 1, 'action': 'left', 'reward': 1.9773328039724594, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded 1.98)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.75938258661212, 'left': -10.19209260727935, 'right': 1.58662099646623, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 1.17787787706
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.1778778770571765, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 1.18)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.9562462584746709, None: -2.544614659997474}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: left, reward: 1.91154341843
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', 'left'), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 1.9115434184323843, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', 'left')
Agent drove left instead of forward. (rewarded 1.91)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.7998693356494042, 'left': -0.0485734347393747, 'right': 0.5622384865000678, None: -2.2941735092357636}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: None, reward: -5.67393648614
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 21, 't': 4, 'action': None, 'reward': -5.6739364861355135, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': 0.0, 'right': 1.3809421552020318, None: 1.3442985093696806}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: forward, reward: -10.4238048514
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', 'left'), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -10.423804851419208, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -10.42)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -10.318303933083794, 'left': -10.094314789543994, 'right': 1.9291986316726182, None: 2.252679116135595}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: left, reward: -9.15077160993
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -9.150771609933587, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.15)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -10.058906443661815, 'left': -8.667054633023076, 'right': 1.3760076360213134, None: 1.0678138377994275}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: None, reward: 2.21571167288
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': 2.215711672884586, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -10.058906443661815, 'left': -8.667054633023076, 'right': 1.3760076360213134, None: 1.6417627553420067}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: forward, reward: -10.75863631
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -10.758636310018309, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.18304104453571007, 'left': -18.446313114153085, 'right': 1.86295548824675, None: -4.356305172643004}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: right, reward: 2.77538005003
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 2.7753800500314263, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 2.78)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.7844915510990338, 'left': 1.6058359033016343, 'right': 1.2867275571828292, None: -4.469747239944523}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: None, reward: -5.10330713134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 15, 't': 10, 'action': None, 'reward': -5.103307131342843, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.10)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -0.26461059568637424, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: None, reward: 1.39094186811
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', None), 'deadline': 14, 't': 11, 'action': None, 'reward': 1.3909418681120056, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', None)
Agent idled at a green light with oncoming traffic. (rewarded 1.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.7844915510990338, 'left': 1.6058359033016343, 'right': 1.2867275571828292, None: -4.786527185643683}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: left, reward: 1.56409238721
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': 1.5640923872108254, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.56)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -19.78110403391529, 'left': -19.77907692041566, 'right': 0.5790806355916813, None: 1.0178791912008127}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: 0.903355049731
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 0.9033550497312341, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent drove right instead of forward. (rewarded 0.90)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.726635054368959, 'left': -9.655298552263503, 'right': 0.4389484201447818, None: 0.9159386839464208}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: forward, reward: -10.0123785231
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -10.012378523085875, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.01)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -5.0036806375679035, 'left': -4.792915590347111, 'right': 0.7074505672721305, None: 0.952465482329112}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: None, reward: 1.16527178127
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.165271781271704, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 1.17)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.1653609364600537, None: -4.774430435787325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: right, reward: 0.973687567495
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 0.9736875674948108, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 0.97)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -10.318303933083794, 'left': -9.62254319973879, 'right': 1.9291986316726182, None: 2.252679116135595}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: forward, reward: -9.29478670534
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': -9.294786705335405, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.29)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.75938258661212, 'left': -10.19209260727935, 'right': 1.382249436761703, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: right, reward: 2.32926092326
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 2.329260923258451, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 2.33)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': -20.278999117446027, 'right': 0.9651445935227861, None: 0.8000704033403672}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: forward, reward: -39.6733122183
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -39.6733122183209, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.67)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -5.186739908100557, 'left': -34.59180636179036, 'right': 0.5235464148227659, None: 1.067255681203651}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: None, reward: 2.16157005098
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 5, 't': 20, 'action': None, 'reward': 2.1615700509837286, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 2.16)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.806545319209599, 'left': -9.62254319973879, 'right': 1.9291986316726182, None: 2.252679116135595}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: None, reward: 0.49056524189
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 0.4905652418904711, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.49)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.5676600515314272, 'left': 0.27618058428451736, 'right': 1.5022899364974847, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: forward, reward: -0.476694399566
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': -0.4766943995656858, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded -0.48)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.3453430142628539, 'left': -9.657410605631151, 'right': 0.7100068876956095, None: -3.8069771162676878}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: left, reward: -20.4424364907
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -20.442436490651392, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.44)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.4968594195723535, 'left': 1.3703164262670393, 'right': 1.7138782790864349, None: 0.47522944267498085}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: -0.6076664048
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 1, 't': 24, 'action': None, 'reward': -0.6076664047999278, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded -0.61)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 70
\-------------------------

Environment.reset(): Trial set up with start = (7, 6), destination = (1, 3), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.06767306846036047, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: right, reward: 0.574624336933
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', 'forward'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 0.5746243369325696, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', 'forward')
Agent drove right instead of left. (rewarded 0.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -0.008390135160448176, 'left': 0.9632697118369385, 'right': 0.7674870214244467, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: forward, reward: 1.61806167967
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 1.618061679672046, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove forward instead of left. (rewarded 1.62)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.869506788727417, 'left': -9.655298552263503, 'right': 0.4389484201447818, None: 0.9159386839464208}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: right, reward: 0.940839809025
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 0.9408398090245231, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.94)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.186739908100557, 'left': -34.59180636179036, 'right': 0.5235464148227659, None: 1.6144128660936898}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: 2.94551183628
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.945511836284764, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 2.95)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.7456531128996295, 'left': 0.8184868764547389, 'right': 1.1727471922073036, None: -4.939900213629565}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: left, reward: 0.815637517857
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': 0.8156375178571441, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 0.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.7707211542480219, 'left': 0.7839288916657524, 'right': 0.33110747654408024, None: 0.3743108077441025}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: forward, reward: 1.02327265508
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': 1.0232726550754392, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'left', None)
Agent followed the waypoint forward. (rewarded 1.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.5293753264428758, 'left': 0.8268320953510937, 'right': 1.1885694334324208, None: -5.500777586398977}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: left, reward: 0.812417622669
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 0.8124176226691685, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -10.408771376840061, 'left': -8.667054633023076, 'right': 1.3760076360213134, None: 1.6417627553420067}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: forward, reward: -9.38271281477
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -9.382712814767231, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.38)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -4.8248602712824065, 'left': -7.244775863435466, 'right': 0.7766342297453515, None: 1.0547657571588864}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: forward, reward: -9.05011986532
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -9.050119865324843, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.05)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': -9.830913699771962, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: None, reward: -4.19139780139
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'forward'), 'deadline': 16, 't': 9, 'action': None, 'reward': -4.1913978013868265, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.19)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 0.9060242736963581, None: -5.2508786771186955}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: right, reward: 2.01965394546
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 2.01965394545511, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.02)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.8897394586209985, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: right, reward: 0.0737178144031
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', 'left', None), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 0.0737178144030638, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'left', None)
Agent drove right instead of forward. (rewarded 0.07)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.869506788727417, 'left': -9.655298552263503, 'right': 0.6898941145846524, None: 0.9159386839464208}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: -9.80646119404
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -9.806461194038036, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.81)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -8.290101299485517, 'left': -9.7277904772404, 'right': 0.19096472994183833, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: -10.2075157208
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': -10.207515720812545, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.21)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.404313092443504, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 0.9581150107957778}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: None, reward: 0.791171783914
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 0.7911717839137116, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 0.79)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.837983991382727, 'left': -9.655298552263503, 'right': 0.6898941145846524, None: 0.9159386839464208}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -10.0471128929
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': 'left', 'reward': -10.047112892891024, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.05)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.6877294880707738, 'left': 0.9658923165133361, 'right': 1.21748992808674, None: -4.023393744880231}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: 1.64153724836
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 9, 't': 16, 'action': 'left', 'reward': 1.6415372483618207, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent followed the waypoint left. (rewarded 1.64)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -7.724768135081817, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 0.7671410642029802}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 1.54885183496
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 8, 't': 17, 'action': None, 'reward': 1.5488518349610292, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.55)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -6.9374900683036245, 'left': -7.244775863435466, 'right': 0.7766342297453515, None: 1.0547657571588864}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: -9.36948791711
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -9.369487917107385, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent attempted driving left through a red light. (rewarded -9.37)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.895742095803646, 'left': -8.667054633023076, 'right': 1.3760076360213134, None: 1.6417627553420067}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: right, reward: 1.09626046483
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 1.0962604648339178, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 1.10)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 1.5475020007084255, 'left': 0.7176682277388182, 'right': 0.4254638151155375, None: -4.199422231553411}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: 0.527238472902
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': 0.5272384729021684, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent followed the waypoint forward. (rewarded 0.53)
16% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 71
\-------------------------

Environment.reset(): Trial set up with start = (6, 2), destination = (2, 5), deadline = 35
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: forward, reward: -9.02981377444
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', 'left', 'left'), 'deadline': 35, 't': 0, 'action': 'forward', 'reward': -9.029813774443408, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -9.03)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -19.83665610916045, 'left': -20.278999117446027, 'right': 0.9651445935227861, None: 0.8000704033403672}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: right, reward: 1.74348564627
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 34, 't': 1, 'action': 'right', 'reward': 1.7434856462654522, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent followed the waypoint right. (rewarded 1.74)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.7421479418466692, 'left': 1.3909726526120112, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: 1.61799179015
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 33, 't': 2, 'action': 'left', 'reward': 1.6179917901532794, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded 1.62)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.806545319209599, 'left': -9.62254319973879, 'right': 1.9291986316726182, None: 1.3716221790130332}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: -9.92645191746
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 32, 't': 3, 'action': 'left', 'reward': -9.926451917463698, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.93)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.806545319209599, 'left': -9.774497558601244, 'right': 1.9291986316726182, None: 1.3716221790130332}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: forward, reward: -10.5439789688
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 31, 't': 4, 'action': 'forward', 'reward': -10.543978968791736, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.54)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.4968594195723535, 'left': 1.3703164262670393, 'right': 1.7138782790864349, None: -0.0662184810624735}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: forward, reward: 1.73713965893
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 30, 't': 5, 'action': 'forward', 'reward': 1.7371396589291381, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove forward instead of right. (rewarded 1.74)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -20.05096751997749, 'left': 0.0, 'right': -9.50853753760335, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: None, reward: 2.49004367866
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'left'), 'deadline': 29, 't': 6, 'action': None, 'reward': 2.4900436786636737, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'left')
Agent properly idled at a red light. (rewarded 2.49)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.116999539250746, 'left': 1.3703164262670393, 'right': 1.7138782790864349, None: -0.0662184810624735}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: left, reward: 0.921710002497
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 28, 't': 7, 'action': 'left', 'reward': 0.921710002497332, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.92)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.75938258661212, 'left': -10.19209260727935, 'right': 1.855755180010077, None: 1.8433558234264569}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: None, reward: 1.40064053916
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 27, 't': 8, 'action': None, 'reward': 1.4006405391625367, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.40)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.116999539250746, 'left': 1.1460132143821857, 'right': 1.7138782790864349, None: -0.0662184810624735}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 7), heading: (0, 1), action: left, reward: 1.12350940635
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 26, 't': 9, 'action': 'left', 'reward': 1.1235094063478275, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.12)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.7782372853836815, 'left': 0.8164007893986294, 'right': -0.060082130625546376, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: right, reward: 0.563924790482
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 25, 't': 10, 'action': 'right', 'reward': 0.5639247904818637, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 0.56)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: left, reward: -39.6605208927
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', 'forward'), 'deadline': 24, 't': 11, 'action': 'left', 'reward': -39.660520892726346, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.66)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.470475956644353, 'left': -7.474273075079459, 'right': 0.15474040539916817, None: 1.479664913914955}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: forward, reward: -10.1355881698
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 23, 't': 12, 'action': 'forward', 'reward': -10.135588169762862, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.14)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.4596626084991584, 'left': -19.30228081150647, 'right': 0.33513004137432534, None: -3.639794921791757}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: -4.05720232458
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 22, 't': 13, 'action': None, 'reward': -4.05720232458033, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.06)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.5293753264428758, 'left': 0.8196248590101312, 'right': 1.1885694334324208, None: -5.500777586398977}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: -5.54439915016
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 21, 't': 14, 'action': None, 'reward': -5.544399150163523, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.54)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.33363155436364, 'left': -10.225179724486539, 'right': 0.16065985993575088, None: -2.6876250480873725}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: forward, reward: 1.84117822922
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 20, 't': 15, 'action': 'forward', 'reward': 1.841178229218085, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent followed the waypoint forward. (rewarded 1.84)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.5293753264428758, 'left': 0.8196248590101312, 'right': 1.1885694334324208, None: -5.52258836828125}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: forward, reward: 1.96901027565
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 19, 't': 16, 'action': 'forward', 'reward': 1.9690102756480512, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.97)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: forward, reward: 2.12380175628
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'left', 'right'), 'deadline': 18, 't': 17, 'action': 'forward', 'reward': 2.123801756275225, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'left', 'right')
Agent followed the waypoint forward. (rewarded 2.12)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': -19.97062010159411, 'right': 0.3826796009968656, None: 0.5045356804597947}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: forward, reward: -40.8014354795
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 17, 't': 18, 'action': 'forward', 'reward': -40.80143547946152, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.80)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -10.175262144000667, 'left': -9.774497558601244, 'right': 1.9291986316726182, None: 1.3716221790130332}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: forward, reward: -9.50992133976
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 16, 't': 19, 'action': 'forward', 'reward': -9.509921339764313, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.51)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -4.975220594212877, 'left': -7.091688373124747, 'right': 1.3211709831966352, None: 0.8201677364659565}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (-1, 0), action: left, reward: -10.6387867606
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 15, 't': 20, 'action': 'left', 'reward': -10.638786760649952, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -10.64)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.116999539250746, 'left': 1.1347613103650067, 'right': 1.7138782790864349, None: -0.0662184810624735}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: left, reward: 0.13837491847
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 14, 't': 21, 'action': 'left', 'reward': 0.1383749184696852, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.14)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: -39.5465515926
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', None), 'deadline': 13, 't': 22, 'action': 'forward', 'reward': -39.54655159261889, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.55)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.18304104453571007, 'left': -18.446313114153085, 'right': 2.319167769139088, None: -4.356305172643004}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: left, reward: -19.1521441943
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 12, 't': 23, 'action': 'left', 'reward': -19.15214419432142, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.15)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.04548282598287068, 'left': 0.27618058428451736, 'right': 1.5022899364974847, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: 0.760764545825
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 11, 't': 24, 'action': 'forward', 'reward': 0.7607645458254872, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.76)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 1.427892899973193, 'left': 0.8301721299516381, 'right': 0.17523106684103096, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: 0.180492772041
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 10, 't': 25, 'action': None, 'reward': 0.18049277204076208, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.18)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': 1.44863133736615, 'left': 0.8316994644820211, 'right': 0.18717668054371284, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 0.373736191652
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 9, 't': 26, 'action': 'right', 'reward': 0.3737361916521289, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.37)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -10.142060462475676, 'left': -10.384747842644082, 'right': 0.44914988986884163, None: 1.731838486615301}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 2.27241971005
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 8, 't': 27, 'action': None, 'reward': 2.272419710052266, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.27)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -10.142060462475676, 'left': -10.384747842644082, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -9.26644315406
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 7, 't': 28, 'action': 'left', 'reward': -9.266443154058289, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.27)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': -4.837167337146998, 'left': -5.107147139893884, 'right': 0.6703735136509539, None: 1.262866166156671}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -10.2399359261
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 6, 't': 29, 'action': 'left', 'reward': -10.239935926090519, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.24)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Environment.step(): t = 30
state
{'forward': -7.226377354324407, 'left': -8.538905258779703, 'right': 0.25421423446049307, None: 2.1182839214013875}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: forward, reward: -9.1145924097
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 5, 't': 30, 'action': 'forward', 'reward': -9.11459240970044, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.11)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Environment.step(): t = 31
state
{'forward': 0.6804202490776425, 'left': 1.8298863795742086, 'right': 0.4797936096313123, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -0.562522162864
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 4, 't': 31, 'action': 'forward', 'reward': -0.5625221628644467, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded -0.56)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Environment.step(): t = 32
state
{'forward': 0.957874286436104, 'left': 1.3280571426788916, 'right': 0.29385449680131215, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: 1.88704260526
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 3, 't': 32, 'action': 'left', 'reward': 1.887042605255643, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.89)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Environment.step(): t = 33
state
{'forward': 0.5628426300356566, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -4.877363809270012}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: forward, reward: 0.735541858166
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 2, 't': 33, 'action': 'forward', 'reward': 0.7355418581660325, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 0.74)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Environment.step(): t = 34
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.3894894287662357}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: -0.800152708811
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', 'left', None), 'deadline': 1, 't': 34, 'action': 'right', 'reward': -0.8001527088108449, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'right', 'left', None)
Agent drove right instead of left. (rewarded -0.80)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 72
\-------------------------

Environment.reset(): Trial set up with start = (1, 4), destination = (6, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: -4.72179231298
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'left', 'forward'), 'deadline': 25, 't': 0, 'action': None, 'reward': -4.721792312975202, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'left', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.7782372853836815, 'left': 0.8164007893986294, 'right': 0.2519213299281587, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: right, reward: 1.05333532347
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 1.0533353234682887, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 1.05)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.84259174188249, 'left': -9.774497558601244, 'right': 1.9291986316726182, None: 1.3716221790130332}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: left, reward: -9.41242877769
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': -9.412428777685466, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.41)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -7.724768135081817, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 1.1579964495820048}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: None, reward: 2.84957274786
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.8495727478561115, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 2.85)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.84259174188249, 'left': -9.593463168143355, 'right': 1.9291986316726182, None: 1.3716221790130332}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: None, reward: 1.0062687966
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 1.0062687966019828, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -4.550706824261862, 'left': -5.099080327924715, 'right': 1.0846468711775872, None: 0.39667569564981675}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: -9.2604625346
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -9.260462534603434, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.26)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.40312368590417896, 'left': 0.27618058428451736, 'right': 1.5022899364974847, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: 1.91194074057
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 1.9119407405661013, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.895742095803646, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.6417627553420067}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: forward, reward: -9.59639610346
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -9.596396103462343, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.60)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: forward, reward: -39.562623875
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'right', 'forward'), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -39.562623874974086, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'right', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.56)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.002723390591305, 'left': 0.0, 'right': 0.6990550835836937, None: 1.153837155726019}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: -9.72482170523
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'right'), 'deadline': 16, 't': 9, 'action': 'left', 'reward': -9.724821705228953, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'right')
Agent attempted driving left through a red light. (rewarded -9.72)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.75938258661212, 'left': -10.19209260727935, 'right': 1.855755180010077, None: 1.6219981812944968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: right, reward: 0.851399084691
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 0.8513990846910855, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 0.85)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 1.462839109575734, None: -5.2508786771186955}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: right, reward: 2.58917935491
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 2.589179354905563, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.59)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -8.362409318498358, 'left': -9.485295079824343, 'right': 0.7275926038090874, None: 1.7906469774200215}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: right, reward: 1.35507572186
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 1.3550757218565872, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.36)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -19.9242406645506, 'left': -19.975079367288057, 'right': -10.20376779627894, None: 0.972698004472699}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: None, reward: 2.40007922993
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 12, 't': 13, 'action': None, 'reward': 2.4000792299320253, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.40)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -19.9242406645506, 'left': -19.975079367288057, 'right': -10.20376779627894, None: 1.686388617202362}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: None, reward: 1.8382477069
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 11, 't': 14, 'action': None, 'reward': 1.8382477068973886, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.84)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.142060462475676, 'left': -9.825595498351184, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: forward, reward: -9.54928917049
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -9.549289170489782, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.55)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': 0.4797936096313123, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: right, reward: 0.216625329003
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 0.21662532900329767, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 0.22)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.75938258661212, 'left': -10.19209260727935, 'right': 1.3535771323505812, None: 1.6219981812944968}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: None, reward: 2.47093669255
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 8, 't': 17, 'action': None, 'reward': 2.470936692552085, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.47)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.75938258661212, 'left': -10.19209260727935, 'right': 1.3535771323505812, None: 2.046467436923291}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: -10.8580513047
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -10.858051304695605, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.86)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.75938258661212, 'left': -10.525071955987478, 'right': 1.3535771323505812, None: 2.046467436923291}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: forward, reward: -9.7040234223
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -9.704023422296475, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.70)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -4.558867495665101, 'left': -5.486949540726377, 'right': 1.0572247241579655, None: 0.42500909165756573}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: -9.3659291178
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -9.365929117803008, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -9.37)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.8655193642437065, 'left': 0.35308188479542246, 'right': 0.0, None: -0.06799419960188136}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: right, reward: 0.82655375762
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 4, 't': 21, 'action': 'right', 'reward': 0.826553757619537, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent followed the waypoint right. (rewarded 0.83)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.7456531128996295, 'left': 0.8170621971559415, 'right': 1.1727471922073036, None: -4.939900213629565}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: 0.93723791599
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 0.9372379159903323, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded 0.94)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -39.9674526276
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', 'right'), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': -39.96745262759474, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'forward', 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.97)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.226607096779238}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: left, reward: -39.0034229207
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 1, 't': 24, 'action': 'left', 'reward': -39.00342292066792, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.00)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 73
\-------------------------

Environment.reset(): Trial set up with start = (6, 3), destination = (3, 2), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.07435280118752369, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: left, reward: -20.9685557455
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', 'forward'), 'deadline': 20, 't': 0, 'action': 'left', 'reward': -20.968555745514387, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'left', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.97)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.7782372853836815, 'left': 0.8164007893986294, 'right': 0.6526283266982237, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: forward, reward: 1.65680017445
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 1.6568001744491436, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove forward instead of left. (rewarded 1.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': -5.382477278141294, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: None, reward: 2.32455099373
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'left'), 'deadline': 18, 't': 2, 'action': None, 'reward': 2.3245509937307567, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', 'left')
Agent properly idled at a red light. (rewarded 2.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.1933779785915037}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: right, reward: 0.520590587871
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'right'), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 0.520590587871376, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'right')
Agent drove right instead of forward. (rewarded 0.52)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': 0.348209469317305, None: 1.0995210061666325}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: None, reward: 1.49012480218
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.4901248021806714, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.957874286436104, 'left': 1.6075498739672673, 'right': 0.29385449680131215, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: left, reward: 2.88336354563
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 2.8833635456273923, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': -20.334621587219544, 'right': 0.0, None: 0.7810855021271161}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: right, reward: 0.534261187468
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'forward'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 0.5342611874684862, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'forward')
Agent drove right instead of forward. (rewarded 0.53)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.4783397258548069, 'left': 2.195707377553165, 'right': 1.1953234434716113, None: -5.020185806361456}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: None, reward: -4.83391313636
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 13, 't': 7, 'action': None, 'reward': -4.833913136363746, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.83)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.3106204747259736, 'left': -9.813445973183716, 'right': 0.002219643778226421, None: -2.0691990998302656}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: forward, reward: -0.0390102555683
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -0.03901025556830107, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent drove forward instead of left. (rewarded -0.04)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.0838954742419404, 'left': -10.439104395112189, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: left, reward: -20.1156552154
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -20.115655215383008, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.12)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.957874286436104, 'left': 2.2454567097973297, 'right': 0.29385449680131215, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: left, reward: 0.938003766858
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 0.9380037668584533, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 0.94)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -7.913183956902648, 'left': -5.316620902961126, 'right': 0.4939406876573365, None: 1.3439140078426994}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: None, reward: 2.16995689286
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 9, 't': 11, 'action': None, 'reward': 2.169956892864837, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 2.17)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.167738838026643, 'left': -5.173426484474621, 'right': 0.4903315088640395, None: 1.218161885194407}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: right, reward: 0.853010804672
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.8530108046723373, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent drove right instead of forward. (rewarded 0.85)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': 0.348209469317305, None: 1.2948229041736519}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: None, reward: -0.0720362309394
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': -0.07203623093935174, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded -0.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': 0.348209469317305, None: 0.61139333661715}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: right, reward: -0.423775516307
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': -0.42377551630697485, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded -0.42)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: forward, reward: -40.2708291285
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -40.270829128455325, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.27)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': 0.5246008512572239, None: 1.576688766826353}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: right, reward: -0.569070748789
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': -0.5690707487891171, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded -0.57)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.392640368311675, 'left': 0.3801153022403645, 'right': 0.7599968239915724, None: -4.939066832519487}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: forward, reward: 0.962213688091
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.9622136880905777, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove forward instead of right. (rewarded 0.96)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -5.186739908100557, 'left': -34.59180636179036, 'right': 0.5235464148227659, None: 2.279962351189227}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: forward, reward: -9.010948131
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': -9.01094813099673, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -9.01)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.84259174188249, 'left': -9.593463168143355, 'right': 1.9291986316726182, None: 1.188945487807508}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: right, reward: 1.98351731578
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 1.9835173157844856, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.98)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 74
\-------------------------

Environment.reset(): Trial set up with start = (7, 7), destination = (5, 4), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': -20.37826037399169, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: -19.144676704
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', 'right', 'forward', 'forward'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': -19.144676704041522, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.14)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -4.63407448875485, 'left': -19.929222317188433, 'right': 0.14932788298831878, None: 0.14449714315918194}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: None, reward: 1.01952924316
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 24, 't': 1, 'action': None, 'reward': 1.0195292431617324, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent properly idled at a red light. (rewarded 1.02)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.294551873725489, 'left': -9.99927876154775, 'right': 0.9539947755207957, None: 1.7010295629894172}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: None, reward: 2.09315512536
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.093155125359514, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.7491928010454636, 'left': 0.8196248590101312, 'right': 1.1885694334324208, None: -5.52258836828125}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: forward, reward: 2.34098108787
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': 2.3409810878729953, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 2.34)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 2.0450869444592294, 'left': 0.8196248590101312, 'right': 1.1885694334324208, None: -5.52258836828125}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: None, reward: -5.04403896841
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': -5.044038968412911, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.04)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -10.294551873725489, 'left': -9.99927876154775, 'right': 0.9539947755207957, None: 1.8970923441744656}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 0.291601149585
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 0.29160114958470096, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.29)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.84567481648273, 'left': -9.825595498351184, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: forward, reward: -10.338746516
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.338746516029468, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.34)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.5924839481068156, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: left, reward: 2.82803353383
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'left', None), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 2.8280335338325076, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'left', None)
Agent followed the waypoint left. (rewarded 2.83)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.18304104453571007, 'left': -18.799228654237254, 'right': 2.319167769139088, None: -4.356305172643004}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: None, reward: -5.70710961071
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': -5.707109610709006, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -29.694482379191278, 'left': -29.94481238143355, 'right': -17.7169022550902, None: 2.179853355677282}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: None, reward: 1.76952688645
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 16, 't': 9, 'action': None, 'reward': 1.76952688644987, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.77)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.746069099632994, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.6417627553420067}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: forward, reward: -9.25711374285
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': -9.257113742852054, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.26)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.18304104453571007, 'left': -18.799228654237254, 'right': 2.319167769139088, None: -5.0317073916760044}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: 1.41763375316
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.4176337531585497, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent drove forward instead of right. (rewarded 1.42)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.0259685319686296, 'left': 0.4823217858378716, 'right': 2.026009232240648, None: -5.2508786771186955}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: 1.16030295246
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': 1.1603029524647432, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent drove forward instead of right. (rewarded 1.16)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -30.420675693359605, 'left': -19.65953581119593, 'right': 1.0162353765608225, None: 1.0255312246449344}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: -39.4845955891
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': -39.48459558912099, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.48)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.731703004454296, 'left': -10.525071955987478, 'right': 1.3535771323505812, None: 2.046467436923291}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 1.54549103706
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 1.545491037063119, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -5.0133505645022485, 'left': -4.954094917810703, 'right': 0.0, None: 0.8703294085273404}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 2.41576828586
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 10, 't': 15, 'action': 'right', 'reward': 2.4157682858644978, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent followed the waypoint right. (rewarded 2.42)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: None, reward: -4.98742804864
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', 'right', 'forward'), 'deadline': 9, 't': 16, 'action': None, 'reward': -4.987428048641148, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'right', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.99)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -20.40071773973076, 'left': -19.97062010159411, 'right': 0.3826796009968656, None: 0.5045356804597947}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: left, reward: -39.2092767413
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -39.20927674130258, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.21)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 2.17010154953
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', 'forward'), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 2.170101549527137, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'left', 'forward')
Agent followed the waypoint right. (rewarded 2.17)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.44863133736615, 'left': 0.8316994644820211, 'right': 0.28045643609792087, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: 0.0634323778232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 0.06343237782317457, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.06)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.1748816032631476, 'left': 0.3120765974254924, 'right': 0.9894480298872794, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: -5.82458141077
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'right', 'left'), 'deadline': 5, 't': 20, 'action': None, 'reward': -5.824581410765817, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.82)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -8.294232586630605, 'left': -7.188380902122997, 'right': 1.9225365880108047, None: 0.9962267021660612}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: -10.4066260785
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -10.406626078476407, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.41)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -7.724768135081817, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 2.003784598719058}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: forward, reward: -10.108587418
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': -10.108587418046564, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.11)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -8.294232586630605, 'left': -8.797503490299702, 'right': 1.9225365880108047, None: 0.9962267021660612}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: -9.65852046137
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -9.658520461372984, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.66)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.84259174188249, 'left': -9.593463168143355, 'right': 1.956357973728552, None: 1.188945487807508}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: None, reward: 0.153746121336
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': None, 'reward': 0.15374612133581023, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.15)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 75
\-------------------------

Environment.reset(): Trial set up with start = (5, 5), destination = (2, 2), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -8.362409318498358, 'left': -9.485295079824343, 'right': 1.0413341628328374, None: 1.7906469774200215}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: -10.6680363181
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -10.66803631812013, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.67)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -5.341285613501723, 'left': -7.74092684753426, 'right': 0.8369463408474729, None: 1.311048896255862}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: -10.0966119399
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 29, 't': 1, 'action': 'forward', 'reward': -10.09661193987361, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.10)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.294551873725489, 'left': -9.99927876154775, 'right': 0.6227979625527483, None: 1.8970923441744656}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: -10.376520246
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 28, 't': 2, 'action': 'forward', 'reward': -10.376520245987198, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.38)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.628889868095468, 'left': -10.27281183885492, 'right': 1.0586079102212795, None: 1.9438504905782414}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: -9.36376786623
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 27, 't': 3, 'action': 'forward', 'reward': -9.363767866229006, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.36)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.507339002966697, 'left': 0.2510051396862819, 'right': 0.7940945849914265, None: 0.3770695093730548}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 1.22777499303
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 26, 't': 4, 'action': 'forward', 'reward': 1.2277749930319082, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 1.23)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': -19.85217377853075, 'right': -10.34416031428023, None: 0.8808114996176329}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: -39.3902765448
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'left'), 'deadline': 25, 't': 5, 'action': 'forward', 'reward': -39.39027654480277, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.39)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -20.385878494423228, 'left': -20.415567100483972, 'right': -10.07466575102071, None: 0.6637684175610911}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: None, reward: 2.27062463377
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 24, 't': 6, 'action': None, 'reward': 2.2706246337671736, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.27)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.515222818309244, 'left': -9.485295079824343, 'right': 1.0413341628328374, None: 1.7906469774200215}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: 0.803371753461
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 0.8033717534613852, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.80)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.957874286436104, 'left': 1.5917302383278915, 'right': 0.29385449680131215, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: right, reward: 1.63987429849
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 22, 't': 8, 'action': 'right', 'reward': 1.639874298487137, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 1.64)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.7844915510990338, 'left': 1.58496414525623, 'right': 1.2867275571828292, None: -4.786527185643683}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: None, reward: -4.62543083484
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 21, 't': 9, 'action': None, 'reward': -4.625430834840387, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.63)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.7844915510990338, 'left': 1.58496414525623, 'right': 1.2867275571828292, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: forward, reward: 1.288189061
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': 1.2881890609976268, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 1.29)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 2.075370254530051, 'left': 0.4008901216934816, 'right': 0.15353933489711996, None: -4.9182690979548465}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: -4.58819299474
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 19, 't': 11, 'action': None, 'reward': -4.588192994738212, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.59)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.44863133736615, 'left': 0.44756592115259786, 'right': 0.28045643609792087, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: right, reward: 0.334681585744
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 18, 't': 12, 'action': 'right', 'reward': 0.3346815857442559, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.33)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -0.14657522251285554, 'left': 1.1217904247490815, 'right': 0.0, None: -2.624086194271513}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 0.918483658296
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'right'), 'deadline': 17, 't': 13, 'action': 'right', 'reward': 0.9184836582956006, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'right')
Agent drove right instead of left. (rewarded 0.92)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.335536059856343, 'left': -9.99927876154775, 'right': 0.6227979625527483, None: 1.8970923441744656}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: left, reward: -9.20858372545
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -9.208583725451152, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.21)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.335536059856343, 'left': -9.603931243499451, 'right': 0.6227979625527483, None: 1.8970923441744656}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: 2.63216208932
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 15, 't': 15, 'action': None, 'reward': 2.632162089320091, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.63)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.7421479418466692, 'left': 1.5044822213826454, 'right': 0.9194383928146641, None: -3.8409705144450648}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: None, reward: -4.04769163795
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 14, 't': 16, 'action': None, 'reward': -4.047691637950983, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.05)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.7421479418466692, 'left': 1.5044822213826454, 'right': 0.9194383928146641, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: forward, reward: 1.87781000558
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 13, 't': 17, 'action': 'forward', 'reward': 1.8778100055755065, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 1.88)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 2.0450869444592294, 'left': 0.8196248590101312, 'right': 1.1885694334324208, None: -5.2833136683470805}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: left, reward: 0.0223287731588
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 12, 't': 18, 'action': 'left', 'reward': 0.022328773158805104, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.02)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.11249816202689822, 'left': 0.9689580603068322, 'right': 1.1207916355646377, None: 0.28929256697058636}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: 0.583517882998
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 11, 't': 19, 'action': None, 'reward': 0.5835178829984983, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.58)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: forward, reward: 0.348542015385
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', 'left'), 'deadline': 10, 't': 20, 'action': 'forward', 'reward': 0.34854201538547236, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', 'left')
Agent drove forward instead of right. (rewarded 0.35)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.84259174188249, 'left': -9.593463168143355, 'right': 1.956357973728552, None: 0.6713458045716592}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: left, reward: -9.36447048584
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 21, 'action': 'left', 'reward': -9.364470485835193, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.36)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.40312368590417896, 'left': 1.0940606624253093, 'right': 1.5022899364974847, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 2), heading: (0, 1), action: forward, reward: 1.21047074085
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': 1.2104707408489042, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 1.21)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -30.49745776533714, 'left': -34.910942403817685, 'right': 1.255041037297422, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (0, 1), action: left, reward: -40.4083675854
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 7, 't': 23, 'action': 'left', 'reward': -40.40836758535908, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.41)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -5.262876599231653, 'left': -5.387928055003375, 'right': 1.0606258023337287, None: 1.0159805132425914}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (-1, 0), action: right, reward: 0.533415138174
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 6, 't': 24, 'action': 'right', 'reward': 0.5334151381739429, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent followed the waypoint right. (rewarded 0.53)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': -4.852981348741553, 'left': -7.328857101738256, 'right': 0.22813951525488635, None: 1.628960376051345}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (-1, 0), action: left, reward: -9.49579815353
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -9.495798153530767, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.50)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -10.335536059856343, 'left': -9.603931243499451, 'right': 0.6227979625527483, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (-1, 0), action: left, reward: -9.55177211958
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 4, 't': 26, 'action': 'left', 'reward': -9.551772119578176, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.55)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -10.335536059856343, 'left': -9.577851681538814, 'right': 0.6227979625527483, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 7), heading: (0, -1), action: right, reward: 0.418069505219
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 27, 'action': 'right', 'reward': 0.41806950521872066, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.42)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 0.6491922441008445, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -4.877363809270012}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (0, -1), action: forward, reward: 1.03604955442
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 2, 't': 28, 'action': 'forward', 'reward': 1.0360495544228536, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 1.04)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': -0.10930905482674763, 'left': 1.328633988566433, 'right': 0.18628544098594924, None: 0.16294635437428795}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: left, reward: 0.393590199971
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'forward'), 'deadline': 1, 't': 29, 'action': 'left', 'reward': 0.39359019997123545, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'left', None, 'forward')
Agent followed the waypoint left. (rewarded 0.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 76
\-------------------------

Environment.reset(): Trial set up with start = (2, 2), destination = (4, 6), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': -0.40007635440542244, None: 0.3894894287662357}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: -10.753987744
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', 'left', None), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -10.753987744013404, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -4.837167337146998, 'left': -7.673541532992202, 'right': 0.6703735136509539, None: 1.262866166156671}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: right, reward: 0.72476539556
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.7247653955602752, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent drove right instead of left. (rewarded 0.72)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.31801371719264415, 'left': 0.5184639798989839, 'right': 1.0586369613300912, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: left, reward: 0.278048176539
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 18, 't': 2, 'action': 'left', 'reward': 0.2780481765392965, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove left instead of right. (rewarded 0.28)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -20.30553931988236, 'left': -20.407072234619378, 'right': 0.10768731177632035, None: 0.8714150576196961}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: forward, reward: -39.811080518
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -39.81108051797529, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.81)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': -0.022234948765946616, None: 1.576688766826353}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: None, reward: 1.27967627379
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.2796762737945377, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.28)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': -0.022234948765946616, None: 1.4281825203104455}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: None, reward: 2.49058102408
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 15, 't': 5, 'action': None, 'reward': 2.4905810240821813, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.49)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4506600629335881, 'left': 0.42291577324947194, 'right': 0.5039110255194301, None: 0.685848835362945}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: forward, reward: 0.779751492921
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'right'), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': 0.779751492921084, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'right')
Agent drove forward instead of left. (rewarded 0.78)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': -0.022234948765946616, None: 1.9593817721963134}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: right, reward: 0.392058871216
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 0.3920588712160662, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.39)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.455517593438483, 'left': -9.00836741228929, 'right': 0.14458206381830013, None: 1.464351558695249}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: left, reward: -9.19811713039
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -9.19811713039029, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.20)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -8.170484882012424, 'left': -8.538905258779703, 'right': 0.25421423446049307, None: 2.1182839214013875}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: right, reward: 0.478024209662
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 0.47802420966161097, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent drove right instead of left. (rewarded 0.48)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.28374431582749343, 'left': 0.32293604945672666, 'right': 0.4134609761062362, None: -2.9332972625096314}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: forward, reward: 0.39300224223
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': 0.3930022422303251, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent drove forward instead of right. (rewarded 0.39)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.84259174188249, 'left': -9.478966826989275, 'right': 1.956357973728552, None: 0.6713458045716592}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 1.33232820786
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': None, 'reward': 1.3323282078625904, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -29.694482379191278, 'left': -29.94481238143355, 'right': -17.7169022550902, None: 1.9746901210635759}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: -39.0395891983
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 8, 't': 12, 'action': 'left', 'reward': -39.0395891983114, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.04)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.8067972133765415, 'left': 1.0940606624253093, 'right': 1.5022899364974847, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: right, reward: 2.04148308762
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 2.0414830876173857, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.04)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.5961199708370417, 'left': -10.171716032974075, 'right': 0.887663117822004, None: -3.597210643954974}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: left, reward: -19.2916012161
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -19.291601216143615, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.29)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.8099789737110878, 'left': 1.5044822213826454, 'right': 0.9194383928146641, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: forward, reward: 1.74773154788
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': 1.7477315478848685, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 1.75)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -4.852981348741553, 'left': -8.412327627634511, 'right': 0.22813951525488635, None: 1.628960376051345}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: right, reward: -0.10502691918
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': -0.1050269191804849, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent drove right instead of forward. (rewarded -0.11)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -19.9242406645506, 'left': -19.975079367288057, 'right': -10.20376779627894, None: 1.7623181620498753}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -40.8643449981
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': -40.86434499814443, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.86)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.4913384468918707}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: -0.0223927875653
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': -0.022392787565267414, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent drove right instead of left. (rewarded -0.02)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -9.94001791824
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'right'), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -9.94001791824125, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'forward', None, 'right')
Agent attempted driving left through a red light. (rewarded -9.94)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 77
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (3, 2), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -30.49745776533714, 'left': -37.65965499458838, 'right': 1.255041037297422, None: 0.3751813753296742}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: forward, reward: -40.8571734635
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -40.85717346354733, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.86)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.84259174188249, 'left': -9.478966826989275, 'right': 1.956357973728552, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: -9.18398069681
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 29, 't': 1, 'action': 'left', 'reward': -9.183980696811501, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.18)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.84259174188249, 'left': -9.331473761900387, 'right': 1.956357973728552, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: -10.7811756825
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 28, 't': 2, 'action': 'left', 'reward': -10.781175682527964, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.78)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -4.6602596948445765, 'left': 0.0, 'right': 0.8218453983250407, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: left, reward: -9.32438464166
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', 'right'), 'deadline': 27, 't': 3, 'action': 'left', 'reward': -9.324384641657526, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'right')
Agent attempted driving left through a red light. (rewarded -9.32)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.1774270282011263, 'left': 0.3801153022403645, 'right': 0.7599968239915724, None: -4.939066832519487}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: left, reward: 1.76329459693
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 26, 't': 4, 'action': 'left', 'reward': 1.7632945969291058, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove left instead of right. (rewarded 1.76)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -7.517503405689789, 'left': -4.803714150969578, 'right': 0.837483406711816, None: 2.3080657203530617}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: None, reward: 1.92073390488
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'left'), 'deadline': 25, 't': 5, 'action': None, 'reward': 1.9207339048789724, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'left')
Agent properly idled at a red light. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: 2.35870737844
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', 'left'), 'deadline': 24, 't': 6, 'action': 'left', 'reward': 2.358707378441165, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', 'left')
Agent followed the waypoint left. (rewarded 2.36)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.4596626084991584, 'left': -19.30228081150647, 'right': 0.33513004137432534, None: -3.8484986231860434}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: -19.1211152523
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 23, 't': 7, 'action': 'left', 'reward': -19.121115252332835, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.12)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.5874048917908623, 'left': -10.225179724486539, 'right': 0.16065985993575088, None: -2.6876250480873725}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: None, reward: -5.29294700206
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 22, 't': 8, 'action': None, 'reward': -5.292947002055618, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.29)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -4.852981348741553, 'left': -8.412327627634511, 'right': 0.061556298037200724, None: 1.628960376051345}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: right, reward: 1.61145020789
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 21, 't': 9, 'action': 'right', 'reward': 1.6114502078882256, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent drove right instead of forward. (rewarded 1.61)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.957874286436104, 'left': 1.5917302383278915, 'right': 0.9668643976442246, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: forward, reward: 1.68257724224
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': 1.68257724224386, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.68)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: None, reward: -4.76884062028
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', None), 'deadline': 19, 't': 11, 'action': None, 'reward': -4.768840620276421, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.77)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.0363403060483303, 'left': 1.58496414525623, 'right': 1.2867275571828292, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: 0.406602830329
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 18, 't': 12, 'action': 'right', 'reward': 0.4066028303287069, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove right instead of left. (rewarded 0.41)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.4766816270830957, 'left': -19.83633837054103, 'right': 0.7099833028882607, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: forward, reward: 0.0339625482588
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 17, 't': 13, 'action': 'forward', 'reward': 0.03396254825884837, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove forward instead of left. (rewarded 0.03)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.7788552607979782, 'left': 1.5044822213826454, 'right': 0.9194383928146641, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 0.663997724455
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 16, 't': 14, 'action': 'right', 'reward': 0.6639977244554005, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.66)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.248808510149031, 'left': -9.7277904772404, 'right': 0.19096472994183833, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: right, reward: 1.2223923507
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 15, 't': 15, 'action': 'right', 'reward': 1.2223923507046064, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 1.22)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 0.7940945849914265, None: 0.3770695093730548}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: right, reward: 1.19950293816
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 14, 't': 16, 'action': 'right', 'reward': 1.1995029381602196, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 1.20)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -19.842559854117678, 'left': 0.0, 'right': 0.5137739820301053, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: left, reward: -40.0357429326
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'forward'), 'deadline': 13, 't': 17, 'action': 'left', 'reward': -40.03574293264678, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.04)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -10.092210666256099, 'left': -9.825595498351184, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: forward, reward: -9.90501267425
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -9.905012674252362, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.91)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': -0.03778302349483492, None: 0.61139333661715}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: None, reward: 1.08403129548
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 11, 't': 19, 'action': None, 'reward': 1.0840312954794245, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.08)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 1.0363403060483303, 'left': 1.58496414525623, 'right': 0.8466651937557681, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: left, reward: 2.19721806474
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 10, 't': 20, 'action': 'left', 'reward': 2.197218064735925, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 2.20)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.44863133736615, 'left': 0.44756592115259786, 'right': 0.30756901092108835, None: -3.903171108799849}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: None, reward: -5.32700813034
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 9, 't': 21, 'action': None, 'reward': -5.3270081303408094, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.44863133736615, 'left': 0.44756592115259786, 'right': 0.30756901092108835, None: -4.6150896195703295}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (0, 1), action: right, reward: 0.231245423621
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 8, 't': 22, 'action': 'right', 'reward': 0.23124542362080192, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.23)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (0, 1), action: None, reward: -5.98453765646
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'left', 'left'), 'deadline': 7, 't': 23, 'action': None, 'reward': -5.9845376564634885, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.98)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.8048357722557988, 'left': 0.9632697118369385, 'right': 0.7674870214244467, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: right, reward: 0.215608134699
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 6, 't': 24, 'action': 'right', 'reward': 0.215608134698708, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove right instead of left. (rewarded 0.22)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': -19.835655796651018, 'left': -29.70276442124718, 'right': -17.397705418255306, None: 0.5511311871385264}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: left, reward: -39.0101400723
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -39.010140072252504, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.01)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -4.602704696713657, 'left': -4.631373817507978, 'right': 1.062531687097776, None: 1.003461754419426}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: left, reward: -10.530440095
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'left'), 'deadline': 4, 't': 26, 'action': 'left', 'reward': -10.530440094981714, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, 'left')
Agent attempted driving left through a red light. (rewarded -10.53)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -9.501591421242523, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.6417627553420067}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: None, reward: 0.603039990257
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 3, 't': 27, 'action': None, 'reward': 0.6030399902572665, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 0.60)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -9.501591421242523, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.1224013727996365}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: forward, reward: -10.4130388336
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 2, 't': 28, 'action': 'forward', 'reward': -10.413038833637923, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.41)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.8003373988471298, 'left': -18.799228654237254, 'right': 2.319167769139088, None: -5.0317073916760044}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: None, reward: -5.80241097728
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 1, 't': 29, 'action': None, 'reward': -5.802410977276711, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.80)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 78
\-------------------------

Environment.reset(): Trial set up with start = (1, 2), destination = (8, 5), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: right, reward: 1.41390592751
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'left', 'forward'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.4139059275124046, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'left', 'forward')
Agent drove right instead of forward. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.0363403060483303, 'left': 1.8910911049960775, 'right': 0.8466651937557681, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: right, reward: 0.801076322544
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.8010763225441883, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove right instead of left. (rewarded 0.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.0838954742419404, 'left': -15.277379805247598, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: 0.564659857885
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': 0.5646598578854175, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded 0.56)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.0036806375679035, 'left': -4.792915590347111, 'right': 0.7074505672721305, None: 1.058868631800408}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: -9.85283031829
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -9.852830318288339, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.85)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.837983991382727, 'left': -9.851205722577262, 'right': 0.6898941145846524, None: 0.9159386839464208}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: None, reward: 1.47659810905
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.4765981090540288, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -5.351360921432008, 'left': 0.0, 'right': 0.38216139270014016, None: 0.0}
learning, learn new
zeroAction: ['left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: left, reward: -39.037925926
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'right', None, 'right'), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -39.03792592601897, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.0363403060483303, 'left': 1.8910911049960775, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (1, 0), action: forward, reward: -0.0159493537367
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': -0.015949353736665728, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded -0.02)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -0.23202695925459793, 'left': 1.5811267957050164, 'right': 0.373341258972024, None: 0.13180178419026567}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (1, 0), action: None, reward: 0.546875210718
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', None), 'deadline': 13, 't': 7, 'action': None, 'reward': 0.5468752107176114, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.55)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -0.26461059568637424, 'left': 0.0, 'right': 0.0, None: 0.6954709340560028}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (0, -1), action: left, reward: 1.09677181608
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': 1.0967718160788862, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', None)
Agent followed the waypoint left. (rewarded 1.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.824277666063679, 'left': -15.277379805247598, 'right': 0.26981588929192235, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: right, reward: 0.28585011404
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 0.2858501140400167, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove right instead of left. (rewarded 0.29)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.44863133736615, 'left': 0.44756592115259786, 'right': 0.26940721727094513, None: -4.6150896195703295}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: left, reward: 0.0378441672974
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 0.03784416729738205, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.04)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.84259174188249, 'left': -10.056324722214175, 'right': 1.956357973728552, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: forward, reward: -9.17765350727
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': -9.177653507272934, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.18)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.510122624577711, 'left': -10.056324722214175, 'right': 1.956357973728552, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: right, reward: 2.4052130757
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 2.405213075700798, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.41)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -20.385878494423228, 'left': -20.415567100483972, 'right': -10.07466575102071, None: 1.4671965256641324}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: left, reward: -40.1463288399
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': -40.14632883994715, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.15)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.335536059856343, 'left': -9.577851681538814, 'right': 0.5204337338857346, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: left, reward: -9.1851734584
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -9.185173458404702, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.19)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -20.385878494423228, 'left': -30.280947970215564, 'right': -10.07466575102071, None: 1.4671965256641324}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: right, reward: -20.6810629515
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': -20.681062951503467, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.68)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 2.0450869444592294, 'left': 0.42097681608446813, 'right': 1.1885694334324208, None: -5.2833136683470805}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: forward, reward: 1.27566324368
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': 1.275663243679209, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.28)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.1389649098260681, 'left': 0.39010044204178734, 'right': 0.6909158725853318, None: -2.482602067478505}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: forward, reward: 0.790635609305
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.7906356093050717, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent followed the waypoint forward. (rewarded 0.79)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 79
\-------------------------

Environment.reset(): Trial set up with start = (5, 4), destination = (1, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -30.394292831347514, 'left': -19.975079367288057, 'right': -10.20376779627894, None: 1.7623181620498753}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: forward, reward: -40.6506101661
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': -40.650610166114824, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.65)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -5.108565627573413, 'left': -4.740624278078626, 'right': -0.0321859633613375, None: 1.3087762246935766}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: forward, reward: -10.3430621061
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'right'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -10.343062106085615, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.34)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.998611670254231, 'left': -9.825595498351184, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: forward, reward: -9.6299493583
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': -9.629949358303437, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.63)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.814280514278835, 'left': -9.825595498351184, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: forward, reward: -9.59410279829
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': -9.594102798290375, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.704191656284605, 'left': -9.825595498351184, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: left, reward: -10.4135665798
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -10.413566579807618, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -4.837167337146998, 'left': -7.673541532992202, 'right': 0.6975694546056146, None: 1.262866166156671}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: left, reward: -10.3299705449
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': -10.329970544899275, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.33)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.5646305477340334, None: -0.06986981125610517}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: None, reward: 1.83434706232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 19, 't': 6, 'action': None, 'reward': 1.8343470623156088, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.83)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: left, reward: 2.21453998157
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', 'left'), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 2.21453998157072, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', 'left')
Agent followed the waypoint left. (rewarded 2.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.837983991382727, 'left': -9.851205722577262, 'right': 0.6898941145846524, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (-1, 0), action: right, reward: 1.85546094681
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.85546094681049, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.86)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.5005213790246086, 'left': -9.658790871308987, 'right': 0.32269982454676027, None: -3.9368993957668015}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: right, reward: 0.831179390092
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 0.8311793900919757, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent drove right instead of forward. (rewarded 0.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.2175187299164125, 'left': 0.8164007893986294, 'right': 0.6526283266982237, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: right, reward: 0.95468490466
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 0.9546849046596063, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 0.95)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -19.78110403391529, 'left': -19.77907692041566, 'right': 0.7412178426614577, None: 1.0178791912008127}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: -39.2739169089
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': -39.27391690894332, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.27)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.852981348741553, 'left': -8.412327627634511, 'right': 0.8365032529627131, None: 1.628960376051345}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: right, reward: 0.804741589115
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 0.8047415891151919, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent drove right instead of forward. (rewarded 0.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 0.0, 'right': -9.564243218599621, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: forward, reward: -40.708021106
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', 'forward'), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': -40.7080211059602, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.71)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.704191656284605, 'left': -10.119581039079401, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: forward, reward: -9.11827842155
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -9.118278421545487, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.12)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.411235038915045, 'left': -10.119581039079401, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: forward, reward: -9.67350711238
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -9.673507112378264, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.67)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: left, reward: 1.4630211281
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', 'right'), 'deadline': 9, 't': 16, 'action': 'left', 'reward': 1.4630211280989467, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', 'right')
Agent followed the waypoint left. (rewarded 1.46)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -10.335536059856343, 'left': -9.381512569971758, 'right': 0.5204337338857346, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: 0.667708431789
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 0.6677084317887885, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.67)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.3787996367460704, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: -5.86228872977
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'left'), 'deadline': 7, 't': 18, 'action': None, 'reward': -5.862288729772903, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.86)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': -0.011196393782633707, None: 0.4913384468918707}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: left, reward: -10.2212131337
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -10.221213133684646, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent attempted driving left through a red light. (rewarded -10.22)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -4.9249023521925785, 'left': -7.253778312671759, 'right': 0.4169873267604316, None: 0.4268911878731215}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: right, reward: 0.29904940883
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 0.29904940883036946, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent drove right instead of left. (rewarded 0.30)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.841445514444981, 'left': 0.8170621971559415, 'right': 1.1727471922073036, None: -4.939900213629565}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: -0.25838425801
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -0.25838425801033615, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded -0.26)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: left, reward: -40.7383599642
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'left'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -40.738359964196796, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.74)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -10.335536059856343, 'left': -9.381512569971758, 'right': 0.5940710828372615, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: left, reward: -9.58153669512
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -9.581536695118132, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.58)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -34.82762629762413, 'left': -29.99532306776711, 'right': -0.11811137397118186, None: 1.2148049566155144}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: forward, reward: -40.9459256234
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -40.94592562342294, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.95)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 80
\-------------------------

Environment.reset(): Trial set up with start = (2, 4), destination = (5, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -20.05096751997749, 'left': 0.0, 'right': -9.50853753760335, None: 1.2450218393318369}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: left, reward: -40.7917256314
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'left'), 'deadline': 25, 't': 0, 'action': 'left', 'reward': -40.79172563137597, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.79)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -4.975220594212877, 'left': -8.865237566887348, 'right': 1.3211709831966352, None: 0.8201677364659565}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: -9.07241040411
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -9.072410404111773, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.731703004454296, 'left': -10.525071955987478, 'right': 1.3535771323505812, None: 1.795979236993205}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: None, reward: 2.05387840656
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.05387840655663, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.05)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.8655193642437065, 'left': 0.35308188479542246, 'right': 0.4132768788097685, None: -0.06799419960188136}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: left, reward: 1.88925998197
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 1.8892599819673548, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent drove left instead of right. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.320225764339982, 'left': 1.5917302383278915, 'right': 0.9668643976442246, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: forward, reward: 0.120667444376
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 0.1206674443761685, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.12)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.837983991382727, 'left': -9.851205722577262, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: left, reward: -9.23120195599
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': -9.231201955991232, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.23)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -5.437513937771278, 'left': -7.941395844926093, 'right': 0.7265790522270998, None: 0.8854295764789993}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 1.5195423781
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 19, 't': 6, 'action': None, 'reward': 1.5195423780955046, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.52)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': -0.03778302349483492, None: 0.8477123160482872}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 0.782891626579
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': 0.7828916265787487, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.78)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.8426208992618491, 'left': 2.2901405895030593, 'right': -0.020402637991021932, None: -4.877363809270012}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: right, reward: 0.46853405714
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 0.4685340571398052, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove right instead of left. (rewarded 0.47)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -4.895726971810477, 'left': -4.950000943770685, 'right': 0.46038862309963297, None: 0.6912272734238653}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: forward, reward: -9.08762693188
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -9.087626931880335, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.09)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.4596626084991584, 'left': -19.211698031919653, 'right': 0.33513004137432534, None: -3.8484986231860434}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: None, reward: -4.28460221082
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 15, 't': 10, 'action': None, 'reward': -4.284602210822046, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.28)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.427892899973193, 'left': 0.8301721299516381, 'right': 0.17523106684103096, None: 0.09024638602038104}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: left, reward: 0.348672970292
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 0.3486729702915011, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent drove left instead of forward. (rewarded 0.35)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.8067972133765415, 'left': 1.0940606624253093, 'right': 1.7718865120574352, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: left, reward: 1.56074984007
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': 1.5607498400662065, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.56)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -20.185898532202373, 'left': -19.906441819369782, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: None, reward: 1.03029777782
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'forward'), 'deadline': 12, 't': 13, 'action': None, 'reward': 1.0302977778245959, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, 'forward')
Agent properly idled at a red light. (rewarded 1.03)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -20.185898532202373, 'left': -19.906441819369782, 'right': 0.0, None: 0.5151488889122979}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: right, reward: 1.37101229214
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'forward'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.3710122921417365, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, 'forward')
Agent drove right instead of forward. (rewarded 1.37)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.248808510149031, 'left': -9.7277904772404, 'right': 0.7066785403232223, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: -9.10618773208
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -9.106187732076247, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.11)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -5.437513937771278, 'left': -7.941395844926093, 'right': 0.7265790522270998, None: 1.202485977287252}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: left, reward: -9.64321253889
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -9.643212538886091, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -9.64)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.7204466043580753, 'left': 1.5917302383278915, 'right': 0.9668643976442246, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: right, reward: 0.670784187701
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 0.6707841877012521, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.67)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 2.075370254530051, 'left': 0.4008901216934816, 'right': 0.15353933489711996, None: -4.75323104634653}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: left, reward: 0.176076066731
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': 0.1760760667307193, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 0.18)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.731703004454296, 'left': -10.525071955987478, 'right': 1.3535771323505812, None: 1.9249288217749174}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: left, reward: -10.1508514668
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -10.150851466768524, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -10.15)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.731703004454296, 'left': -10.337961711378, 'right': 1.3535771323505812, None: 1.9249288217749174}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: left, reward: -9.83407152106
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': -9.834071521063809, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.83)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -0.2367538979223509, 'left': 0.19306097650467458, 'right': 0.5304258197177816, None: -0.11886393746254198}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: right, reward: 1.8672415662
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 4, 't': 21, 'action': 'right', 'reward': 1.8672415661985184, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent followed the waypoint right. (rewarded 1.87)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -10.335536059856343, 'left': -9.481524632544945, 'right': 0.5940710828372615, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -9.24252441552
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': -9.242524415516112, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.24)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -20.385878494423228, 'left': -30.280947970215564, 'right': -15.377864351262089, None: 1.4671965256641324}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -40.2098831751
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': -40.209883175097346, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.21)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.789030237686227, 'left': -9.481524632544945, 'right': 0.5940710828372615, None: 2.2646272167472783}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: None, reward: 0.274315443323
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': None, 'reward': 0.2743154433227266, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.27)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 81
\-------------------------

Environment.reset(): Trial set up with start = (2, 4), destination = (3, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -20.40071773973076, 'left': -29.589948421448348, 'right': 0.3826796009968656, None: 0.5045356804597947}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: right, reward: 1.50855046336
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.5085504633566886, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.51)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.31801371719264415, 'left': 0.39825607821914016, 'right': 1.0586369613300912, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: 0.469225278864
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 19, 't': 1, 'action': 'left', 'reward': 0.46922527886434306, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove left instead of right. (rewarded 0.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -5.0133505645022485, 'left': -4.954094917810703, 'right': 1.2078841429322489, None: 0.8703294085273404}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 1.31918370638
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 18, 't': 2, 'action': None, 'reward': 1.3191837063779719, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent properly idled at a red light. (rewarded 1.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -4.558867495665101, 'left': -7.426439329264692, 'right': 1.0572247241579655, None: 0.42500909165756573}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -10.9812237494
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -10.981223749394763, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving left through a red light. (rewarded -10.98)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -5.002723390591305, 'left': -4.8624108526144765, 'right': 0.6990550835836937, None: 1.153837155726019}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 2.3170577346
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, 'right'), 'deadline': 16, 't': 4, 'action': None, 'reward': 2.317057734602645, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'right')
Agent properly idled at a red light. (rewarded 2.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -5.0133505645022485, 'left': -4.954094917810703, 'right': 1.2078841429322489, None: 1.094756557452656}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -10.9616309199
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -10.961630919903108, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.96)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 0.410640155292
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', 'forward'), 'deadline': 14, 't': 6, 'action': None, 'reward': 0.41064015529173314, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'right', 'forward')
Agent idled at a green light with oncoming traffic. (rewarded 0.41)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 1.326415198
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'left', 'forward'), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.3264151980006977, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.33)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -20.40071773973076, 'left': -29.589948421448348, 'right': 0.9456150321767771, None: 0.5045356804597947}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: forward, reward: -40.6109976586
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -40.610997658556016, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.61)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -8.91667777656419, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 2.003784598719058}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: 2.43108886337
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 11, 't': 9, 'action': None, 'reward': 2.431088863368459, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 2.43)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -8.294232586630605, 'left': -9.228011975836342, 'right': 1.9225365880108047, None: 0.9962267021660612}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: forward, reward: -9.81548377071
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -9.81548377070912, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.82)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.3453430142628539, 'left': -15.049923548141273, 'right': 0.7100068876956095, None: -3.8069771162676878}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: right, reward: 1.79752808837
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 1.7975280883738851, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', None, None)
Agent followed the waypoint right. (rewarded 1.80)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.6603750940692192, 'left': 0.42097681608446813, 'right': 1.1885694334324208, None: -5.2833136683470805}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: None, reward: -4.31903494443
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 8, 't': 12, 'action': None, 'reward': -4.319034944433899, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.32)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.789030237686227, 'left': -9.481524632544945, 'right': 0.5940710828372615, None: 1.2694713300350025}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: forward, reward: -10.3810023624
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': -10.38100236243074, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.38)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.5501485411908174, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: left, reward: -9.77878310543
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'right', 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -9.77878310543167, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'right', 'left')
Agent attempted driving left through a red light. (rewarded -9.78)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.1499730318081505, 'left': 0.14861671747760768, 'right': 0.5885716837887962, None: 0.9821489084995745}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: forward, reward: 0.809112335855
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': 0.809112335855035, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'left')
Agent followed the waypoint forward. (rewarded 0.81)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -19.942531269156774, 'left': -34.51398638737408, 'right': 0.5173330090882928, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: left, reward: -39.8021122278
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 4, 't': 16, 'action': 'left', 'reward': -39.80211222775434, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.80)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.819269287917319, 'left': -9.399265587968978, 'right': 0.1849119612250598, None: 1.9593817721963134}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: forward, reward: -9.15802615176
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': -9.158026151763535, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.16)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.488647719840426, 'left': -9.399265587968978, 'right': 0.1849119612250598, None: 1.9593817721963134}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: forward, reward: -9.89351258738
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': -9.893512587376527, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.89)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.25532208767097203, 'left': -19.83633837054103, 'right': 0.7099833028882607, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: left, reward: -20.8910767002
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': -20.89107670015929, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.89)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 82
\-------------------------

Environment.reset(): Trial set up with start = (8, 7), destination = (6, 5), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': -20.37229212154966, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: 1.53498995283
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'right'), 'deadline': 20, 't': 0, 'action': None, 'reward': 1.5349899528260693, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'right')
Agent properly idled at a red light. (rewarded 1.53)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -10.085016300058484, 'left': -9.481524632544945, 'right': 0.5940710828372615, None: 1.2694713300350025}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: right, reward: 0.228976290537
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.22897629053683988, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.23)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.837983991382727, 'left': -9.541203839284247, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: -10.3300476327
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.330047632659289, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 0.25532208767097203, 'left': -20.36370753535016, 'right': 0.7099833028882607, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: right, reward: 1.59487002742
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 1.594870027416198, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.05894904310659793, 'left': 1.8298863795742086, 'right': -0.03778302349483492, None: 0.8153019713135179}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: left, reward: 2.24292145386
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 16, 't': 4, 'action': 'left', 'reward': 2.242921453862971, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 2.24)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -10.084015812021008, 'left': -9.541203839284247, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, -1), action: left, reward: -9.44127817927
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -9.441278179270991, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.44)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.7204466043580753, 'left': 1.5917302383278915, 'right': 0.8188242926727384, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: right, reward: 1.69408087214
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.6940808721387595, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 1.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.731703004454296, 'left': -10.086016616220904, 'right': 1.3535771323505812, None: 1.9249288217749174}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: None, reward: 2.18097750314
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': 2.1809775031409053, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.18)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.731703004454296, 'left': -10.086016616220904, 'right': 1.3535771323505812, None: 2.0529531624579116}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: forward, reward: -10.4573575808
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -10.45735758082755, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -7.023815499162325, 'left': -8.865237566887348, 'right': 1.3211709831966352, None: 0.8201677364659565}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: -9.94792215173
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -9.947922151728223, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.95)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -10.094530292640922, 'left': -10.086016616220904, 'right': 1.3535771323505812, None: 2.0529531624579116}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: right, reward: 2.0240389371
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 2.02403893710123, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 2.02)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.510122624577711, 'left': -10.056324722214175, 'right': 2.180785524714675, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: left, reward: -10.2445067068
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -10.244506706837825, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.24)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -34.9526356412403, 'left': -19.65953581119593, 'right': 1.0162353765608225, None: 1.0255312246449344}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: forward, reward: -40.4432320159
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -40.44323201593221, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.44)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.8117664406292029, 'left': 0.4225708722263659, 'right': 1.0327289680605438, None: 0.3499343815737973}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 0.749128045517
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 7, 't': 13, 'action': None, 'reward': 0.7491280455170463, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.75)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.1748816032631476, 'left': 0.3120765974254924, 'right': 0.9894480298872794, None: -2.9122907053829086}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: -4.10929052379
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'right', 'left'), 'deadline': 6, 't': 14, 'action': None, 'reward': -4.109290523792215, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.11)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.7998693356494042, 'left': -0.0485734347393747, 'right': 0.5622384865000678, None: -3.9840549976856385}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: -5.74761313194
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 5, 't': 15, 'action': None, 'reward': -5.747613131937372, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.75)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -30.505857699143387, 'left': -29.589948421448348, 'right': 0.9456150321767771, None: 0.5045356804597947}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: right, reward: 1.64604374282
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 1.6460437428175116, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.65)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.6603750940692192, 'left': 0.42097681608446813, 'right': 1.1885694334324208, None: -4.8011743063904895}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: left, reward: -0.62358574981
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -0.6235857498095078, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded -0.62)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.2915306282173224, 'left': 0.8170621971559415, 'right': 1.1727471922073036, None: -4.939900213629565}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: None, reward: -4.26564251809
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 2, 't': 18, 'action': None, 'reward': -4.265642518086832, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.27)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -19.835655796651018, 'left': -34.35645224674984, 'right': -17.397705418255306, None: 0.5511311871385264}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: right, reward: -20.3459564079
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 1, 't': 19, 'action': 'right', 'reward': -20.34595640793986, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.35)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 83
\-------------------------

Environment.reset(): Trial set up with start = (3, 6), destination = (8, 5), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: right, reward: 1.30051534772
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'forward'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.3005153477240403, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'forward')
Agent drove right instead of left. (rewarded 1.30)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -7.023815499162325, 'left': -9.406579859307787, 'right': 1.3211709831966352, None: 0.8201677364659565}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: left, reward: -9.70860570164
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'left'), 'deadline': 19, 't': 1, 'action': 'left', 'reward': -9.708605701638671, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.71)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -4.558867495665101, 'left': -9.203831539329727, 'right': 1.0572247241579655, None: 0.42500909165756573}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: forward, reward: -10.5662519883
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.566251988258822, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.57)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.0133505645022485, 'left': -7.957862918856906, 'right': 1.2078841429322489, None: 1.094756557452656}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: left, reward: -10.3457711351
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -10.345771135149691, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -10.094530292640922, 'left': -10.086016616220904, 'right': 1.6888080347259056, None: 2.0529531624579116}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: None, reward: 2.88589918004
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 2.8858991800387273, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -7.5625597419619615, 'left': -9.203831539329727, 'right': 1.0572247241579655, None: 0.42500909165756573}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: forward, reward: -9.25101916787
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': -9.251019167867728, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.25)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4120389176492931, 'left': 0.2802365880189883, 'right': 0.3150230225378734, None: 0.3116114536480331}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: None, reward: 0.823912890617
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'right', None), 'deadline': 14, 't': 6, 'action': None, 'reward': 0.8239128906174942, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded 0.82)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.5785292480664365, 'left': 0.605134434256815, 'right': 1.5796761582831478, None: 0.21291611379519323}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: forward, reward: 0.939443088452
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 0.9394430884516374, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove forward instead of right. (rewarded 0.94)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -20.202140757956546, 'left': -19.833166387044734, 'right': -9.578054242763955, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: None, reward: 2.61577712818
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', None), 'deadline': 12, 't': 8, 'action': None, 'reward': 2.615777128178251, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', None)
Agent properly idled at a red light. (rewarded 2.62)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -10.094530292640922, 'left': -10.086016616220904, 'right': 1.6888080347259056, None: 2.4694261712483194}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: right, reward: 0.864767657045
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 0.8647676570454881, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 0.86)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -20.15666659495559, 'left': -19.646596381419585, 'right': -9.648374887274155, None: 1.333520365628341}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: forward, reward: -40.9869654082
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -40.98696540819165, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.99)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.6603750940692192, 'left': -0.10130446686251982, 'right': 1.1885694334324208, None: -4.8011743063904895}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: left, reward: -0.153593218022
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -0.15359321802223613, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded -0.15)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.128355652982165, 'left': 0.37616113942937657, 'right': 0.8925079238852455, None: -2.9548276019880584}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: 0.671364009853
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'right'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': 0.6713640098534377, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'right')
Agent drove forward instead of right. (rewarded 0.67)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.013142629497748193, 'left': 0.0, 'right': 0.0, None: -2.1031435407286274}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: right, reward: 1.52340039573
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'forward'), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 1.5234003957308135, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'forward', 'forward')
Agent followed the waypoint right. (rewarded 1.52)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.515222818309244, 'left': -9.485295079824343, 'right': 0.9223529581471113, None: 1.7906469774200215}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: forward, reward: -9.06923221182
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': -9.069232211822372, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.07)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -9.803032063203608, 'left': -7.474273075079459, 'right': 0.15474040539916817, None: 1.479664913914955}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: -9.15781213698
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -9.157812136982198, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.16)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.15474040539916817, None: 1.479664913914955}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: None, reward: 1.89136928611
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.8913692861089613, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.89)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.4596626084991584, 'left': -19.211698031919653, 'right': 0.33513004137432534, None: -4.066550417004045}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: forward, reward: 0.664004937285
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.664004937284997, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent followed the waypoint forward. (rewarded 0.66)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.6877294880707738, 'left': 1.3037147824375783, 'right': 1.21748992808674, None: -4.023393744880231}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: left, reward: 1.53737494179
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 1.537374941786655, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent followed the waypoint left. (rewarded 1.54)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 84
\-------------------------

Environment.reset(): Trial set up with start = (1, 5), destination = (5, 3), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.4935823470057945, 'left': 0.0, 'right': 0.0, None: -2.555502637730306}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: right, reward: 1.97712601285
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', 'forward', 'left'), 'deadline': 30, 't': 0, 'action': 'right', 'reward': 1.9771260128548962, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'right', 'forward', 'left')
Agent drove right instead of forward. (rewarded 1.98)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.691080153608477, 'left': -9.399265587968978, 'right': 0.1849119612250598, None: 1.9593817721963134}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: forward, reward: -9.48060850601
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 29, 't': 1, 'action': 'forward', 'reward': -9.480608506008492, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.48)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.585844329808484, 'left': -9.399265587968978, 'right': 0.1849119612250598, None: 1.9593817721963134}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: forward, reward: -9.16987747605
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 28, 't': 2, 'action': 'forward', 'reward': -9.16987747605192, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.17)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.377860902930202, 'left': -9.399265587968978, 'right': 0.1849119612250598, None: 1.9593817721963134}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: None, reward: 1.93558659641
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 27, 't': 3, 'action': None, 'reward': 1.9355865964128294, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.94)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: None, reward: 2.11966843543
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', 'left'), 'deadline': 26, 't': 4, 'action': None, 'reward': 2.1196684354323443, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'left', 'left')
Agent properly idled at a red light. (rewarded 2.12)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.25532208767097203, 'left': -20.36370753535016, 'right': 1.1524266651522295, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: 0.681226390943
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 25, 't': 5, 'action': 'forward', 'reward': 0.681226390942682, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove forward instead of left. (rewarded 0.68)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.4783397258548069, 'left': 2.195707377553165, 'right': 1.1953234434716113, None: -4.927049471362601}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: left, reward: 1.20917974829
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 24, 't': 6, 'action': 'left', 'reward': 1.2091797482889246, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.21)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -10.085016300058484, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 1.2694713300350025}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: None, reward: 1.17509622584
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 23, 't': 7, 'action': None, 'reward': 1.1750962258385245, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -10.085016300058484, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 1.2222837779367635}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -10.9606070998
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': -10.960607099792385, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.96)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -10.522811699925434, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 1.2222837779367635}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -9.39882652399
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': -9.398826523994734, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.40)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -39.537220764
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', 'forward'), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': -39.537220763992224, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.54)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': -10.26033494478973, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: right, reward: 0.971740757724
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'right', None), 'deadline': 19, 't': 11, 'action': 'right', 'reward': 0.9717407577241092, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'right', None)
Agent drove right instead of forward. (rewarded 0.97)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.7204466043580753, 'left': 1.5917302383278915, 'right': 1.2564525824057489, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: left, reward: 2.03936037714
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': 2.039360377142736, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.04)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -4.63407448875485, 'left': -19.929222317188433, 'right': 0.14932788298831878, None: 0.5820131931604572}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: right, reward: 0.698476987725
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 17, 't': 13, 'action': 'right', 'reward': 0.6984769877246473, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent drove right instead of forward. (rewarded 0.70)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.13580510957883626, 'left': -9.813445973183716, 'right': 0.002219643778226421, None: -2.0691990998302656}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: -20.1155476788
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -20.115547678770355, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.12)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -5.110606566842323, 'right': -0.011196393782633707, None: 0.4913384468918707}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: forward, reward: -10.3672540164
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': -10.367254016443534, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent attempted driving forward through a red light. (rewarded -10.37)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.177498121112638, 'left': -9.7277904772404, 'right': 0.7066785403232223, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: right, reward: -0.168131093534
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 14, 't': 16, 'action': 'right', 'reward': -0.1681310935338136, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded -0.17)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.054858178669862, 'left': -9.228011975836342, 'right': 1.9225365880108047, None: 0.9962267021660612}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (0, 1), action: right, reward: 1.13890923789
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 13, 't': 17, 'action': 'right', 'reward': 1.1389092378871102, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.14)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.2915306282173224, 'left': 0.8170621971559415, 'right': 1.1727471922073036, None: -4.602771365858199}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: left, reward: 1.06181038899
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 12, 't': 18, 'action': 'left', 'reward': 1.0618103889856767, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 1.06)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.957315127440223, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.1224013727996365}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: forward, reward: -9.49518365149
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 11, 't': 19, 'action': 'forward', 'reward': -9.495183651489969, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.50)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.726249389465096, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.1224013727996365}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: None, reward: 1.08118983391
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 10, 't': 20, 'action': None, 'reward': 1.0811898339130748, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.08)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.8003373988471298, 'left': -18.799228654237254, 'right': 2.319167769139088, None: -5.417059184476358}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: left, reward: -20.4403831205
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 9, 't': 21, 'action': 'left', 'reward': -20.440383120542602, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.44)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.8517347734094066, 'left': 0.6859138143467407, 'right': 1.4218465218358454, None: -4.941767725524494}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: None, reward: -4.20550429022
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 8, 't': 22, 'action': None, 'reward': -4.205504290216086, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.21)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.33837327902890924, 'left': 0.32293604945672666, 'right': 0.4134609761062362, None: -2.9332972625096314}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: left, reward: 0.526266841828
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 7, 't': 23, 'action': 'left', 'reward': 0.5262668418284738, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent drove left instead of right. (rewarded 0.53)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.5101954761558323, 'left': 1.8910911049960775, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: left, reward: 1.93274246031
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 6, 't': 24, 'action': 'left', 'reward': 1.9327424603050638, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.93)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': -9.496328867162237, 'left': -10.27281183885492, 'right': 1.0586079102212795, None: 1.9438504905782414}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: forward, reward: -10.3566114619
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 5, 't': 25, 'action': 'forward', 'reward': -10.356611461913088, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.36)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -9.926470164537662, 'left': -10.27281183885492, 'right': 1.0586079102212795, None: 1.9438504905782414}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: 1.81862218448
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 4, 't': 26, 'action': None, 'reward': 1.8186221844773653, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.82)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 0.996798761575823, None: 0.3770695093730548}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: -0.389111697815
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 3, 't': 27, 'action': None, 'reward': -0.38911169781492405, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded -0.39)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': 1.312103483103256, 'left': 0.09219518175704206, 'right': 0.615145895453168, None: 0.4845531306976972}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: -0.444994339626
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 2, 't': 28, 'action': None, 'reward': -0.44499433962594515, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent idled at a green light with oncoming traffic. (rewarded -0.44)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.0, 'left': -20.37229212154966, 'right': 0.0, None: 0.7674949764130347}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: right, reward: -20.4078863488
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'right'), 'deadline': 1, 't': 29, 'action': 'right', 'reward': -20.407886348849335, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, 'forward', 'right')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 85
\-------------------------

Environment.reset(): Trial set up with start = (4, 4), destination = (7, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.0965660883173427, 'left': 0.78642489494956, 'right': 0.4589533924540907, None: -5.097396010542358}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: forward, reward: 1.84868511452
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': 1.8486851145232437, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 1.85)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -29.527510471429306, 'left': -19.77907692041566, 'right': 0.7412178426614577, None: 1.0178791912008127}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: None, reward: 1.40240046435
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 24, 't': 1, 'action': None, 'reward': 1.402400464353876, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent properly idled at a red light. (rewarded 1.40)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: None, reward: 2.12930833139
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', 'forward', None), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.1293083313891157, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', 'forward', None)
Agent properly idled at a red light. (rewarded 2.13)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.960819111960085, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 1.2222837779367635}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: None, reward: 2.55251263539
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.5525126353901406, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.55)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -37.886775960523536, 'left': -29.99532306776711, 'right': -0.11811137397118186, None: 1.2148049566155144}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: left, reward: -39.8820858172
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -39.88208581716343, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.88)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.960819111960085, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 1.887398206663452}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: None, reward: 2.15605625031
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 2.1560562503128793, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.2996712678479763, 'left': 0.0, 'right': 0.043070150857417666, None: -2.7317545282608426}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: left, reward: 0.478637750232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'left'), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 0.4786377502318505, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'right', 'left')
Agent drove left instead of forward. (rewarded 0.48)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -6.905584679432648, 'left': -5.099080327924715, 'right': 1.0846468711775872, None: 0.39667569564981675}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: forward, reward: -9.8219562669
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -9.821956266899459, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.82)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -29.694482379191278, 'left': -34.49220078987247, 'right': -17.7169022550902, None: 1.9746901210635759}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: right, reward: -20.9232757298
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': -20.923275729759176, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.92)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.262876599231653, 'left': -5.387928055003375, 'right': 0.7970204702538358, None: 1.0159805132425914}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: forward, reward: -10.3835618482
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -10.383561848200978, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.38)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -8.91667777656419, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 2.217436731043758}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: forward, reward: -9.9238484761
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': -9.923848476103815, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.92)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.2915306282173224, 'left': 0.9394362930708091, 'right': 1.1727471922073036, None: -4.602771365858199}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: 1.33022041281
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.330220412808254, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded 1.33)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.8067972133765415, 'left': 1.327405251245758, 'right': 1.7718865120574352, None: -5.114731371098404}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: None, reward: -4.17471378321
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 13, 't': 12, 'action': None, 'reward': -4.17471378321134, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.17)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.510122624577711, 'left': -10.150415714526, 'right': 2.180785524714675, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: left, reward: -10.3651487023
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': -10.365148702336324, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.37)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.726249389465096, 'left': -8.667054633023076, 'right': 1.2361340504276157, None: 1.1017956033563556}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: left, reward: -9.93930977484
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 11, 't': 14, 'action': 'left', 'reward': -9.939309774842524, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.94)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -6.9374900683036245, 'left': -8.307131890271425, 'right': 0.7766342297453515, None: 1.0547657571588864}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: None, reward: 0.870533594908
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', None), 'deadline': 10, 't': 15, 'action': None, 'reward': 0.8705335949077919, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', None)
Agent properly idled at a red light. (rewarded 0.87)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.8003373988471298, 'left': -19.61980588738993, 'right': 2.319167769139088, None: -5.417059184476358}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: None, reward: -4.08534439272
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -4.0853443927246325, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.09)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.8067972133765415, 'left': 1.327405251245758, 'right': 1.7718865120574352, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: left, reward: 1.30978715589
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': 1.309787155888752, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.31)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.510122624577711, 'left': -10.257782208431163, 'right': 2.180785524714675, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: forward, reward: -9.37669660094
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -9.37669660094102, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.38)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.8067972133765415, 'left': 1.318596203567255, 'right': 1.7718865120574352, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: 0.765094315594
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 0.7650943155943034, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.77)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -10.084015812021008, 'left': -9.49124100927762, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: forward, reward: -10.1213039557
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -10.121303955708552, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.12)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -10.10265988386478, 'left': -9.49124100927762, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, 1), action: left, reward: -9.79242730672
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -9.792427306722097, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.79)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.8223469461316817, 'left': 1.4139011236151922, 'right': 0.4916810102821621, None: -4.046450134383133}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: left, reward: 0.517757449794
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 3, 't': 22, 'action': 'left', 'reward': 0.5177574497943536, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent followed the waypoint left. (rewarded 0.52)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -30.297880834760285, 'left': -30.280947970215564, 'right': -15.377864351262089, None: 1.4671965256641324}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: None, reward: 2.17545938235
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 2, 't': 23, 'action': None, 'reward': 2.175459382349638, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.18)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.15474040539916817, None: 1.6855171000119582}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: right, reward: 0.280856012781
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.2808560127811608, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.28)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 86
\-------------------------

Environment.reset(): Trial set up with start = (8, 3), destination = (6, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.8048357722557988, 'left': 0.9632697118369385, 'right': 0.49154757806157734, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: forward, reward: 0.635622016005
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 0.635622016005408, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove forward instead of left. (rewarded 0.64)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -19.507414548080114, 'left': -19.867361425513177, 'right': -9.691071904827892, None: 1.2407703628661046}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 2.93546268698
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 19, 't': 1, 'action': None, 'reward': 2.935462686978818, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent properly idled at a red light. (rewarded 2.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -19.507414548080114, 'left': -19.867361425513177, 'right': -9.691071904827892, None: 2.0881165249224614}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: right, reward: -19.0350556845
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': -19.03505568448313, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.542371075646654, 'left': -10.119581039079401, 'right': 0.44914988986884163, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: right, reward: 0.868844196998
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 0.8688441969978878, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.87)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.377860902930202, 'left': -9.399265587968978, 'right': 0.1849119612250598, None: 1.9474841843045714}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: left, reward: -9.24634580507
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 16, 't': 4, 'action': 'left', 'reward': -9.246345805069323, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.25)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.377860902930202, 'left': -9.32280569651915, 'right': 0.1849119612250598, None: 1.9474841843045714}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: left, reward: -9.38112564849
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -9.381125648488025, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.38)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.468274239306827, 'left': -20.36370753535016, 'right': 1.1524266651522295, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: right, reward: 0.205769741952
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 0.2057697419521196, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.21)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': -4.8634945402505485, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: forward, reward: -9.0362249028
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', 'right'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -9.036224902802239, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', 'right')
Agent attempted driving forward through a red light. (rewarded -9.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.443409612759366, 'left': -10.257782208431163, 'right': 2.180785524714675, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: right, reward: 1.74756742246
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 1.7475674224589632, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.75)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.44863133736615, 'left': 0.24270504422498995, 'right': 0.26940721727094513, None: -4.6150896195703295}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: None, reward: -5.31125711895
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 11, 't': 9, 'action': None, 'reward': -5.311257118949607, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.31)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.8966413576550227, 'left': 0.8282445588900252, 'right': 0.8967147634292605, None: -2.068857563296201}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: None, reward: -4.92749925648
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 10, 't': 10, 'action': None, 'reward': -4.927499256482567, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -30.57181600157362, 'left': -19.646596381419585, 'right': -9.648374887274155, None: 1.333520365628341}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -39.0041289831
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': -39.00412898305569, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.00)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -4.852981348741553, 'left': -8.412327627634511, 'right': 0.8206224210389526, None: 1.628960376051345}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: -10.7032269068
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -10.703226906830967, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.70)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.292227515065807, 'left': -9.485295079824343, 'right': 0.9223529581471113, None: 1.7906469774200215}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: left, reward: -9.58099955414
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 7, 't': 13, 'action': 'left', 'reward': -9.58099955414197, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -9.58)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.926470164537662, 'left': -10.27281183885492, 'right': 1.0586079102212795, None: 1.8812363375278034}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: right, reward: -0.189891499024
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': -0.18989149902379887, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded -0.19)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.7202288941306034, 'left': 0.9632697118369385, 'right': 0.49154757806157734, None: -2.9031521222410213}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: 1.02505957332
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': 1.0250595733207408, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'left')
Agent drove forward instead of left. (rewarded 1.03)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.377860902930202, 'left': -9.351965672503589, 'right': 0.1849119612250598, None: 1.9474841843045714}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: -10.1311251817
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': -10.131125181721941, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.13)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.009610685657633233, 'left': 0.0, 'right': 0.7023585205754569, None: -2.6956038227178873}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: left, reward: -19.69333599
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'right'), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -19.69333598996242, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.69)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.05894904310659793, 'left': 2.03640391671859, 'right': -0.03778302349483492, None: 0.8153019713135179}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: left, reward: 1.50267621347
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 1.5026762134719935, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 1.50)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.427892899973193, 'left': 0.5894225501215696, 'right': 0.17523106684103096, None: 0.09024638602038104}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: right, reward: -0.642625515827
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': -0.6426255158266904, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent drove right instead of forward. (rewarded -0.64)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 87
\-------------------------

Environment.reset(): Trial set up with start = (8, 2), destination = (4, 5), deadline = 35
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.7590719014093665, 'left': -0.03953712785440244, 'right': 0.8155819641776046, None: -2.758206280159728}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (0, 1), action: right, reward: 1.67975754106
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 35, 't': 0, 'action': 'right', 'reward': 1.6797575410553265, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent drove right instead of forward. (rewarded 1.68)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -9.177498121112638, 'left': -9.7277904772404, 'right': 0.26927372339470435, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (0, 1), action: forward, reward: -10.3109556111
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 34, 't': 1, 'action': 'forward', 'reward': -10.310955611122717, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.31)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -10.10265988386478, 'left': -9.641834157999858, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (0, 1), action: forward, reward: -9.29051573007
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 33, 't': 2, 'action': 'forward', 'reward': -9.290515730068106, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.29)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -5.437513937771278, 'left': -8.792304191906092, 'right': 0.7265790522270998, None: 1.202485977287252}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: right, reward: 1.86485673921
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 32, 't': 3, 'action': 'right', 'reward': 1.8648567392102144, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent drove right instead of left. (rewarded 1.86)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.8966413576550227, 'left': 0.8282445588900252, 'right': 0.8967147634292605, None: -3.498178409889384}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: None, reward: -5.00525560735
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'forward'), 'deadline': 31, 't': 4, 'action': None, 'reward': -5.005255607353936, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.01)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -34.78797249231466, 'left': -19.646596381419585, 'right': -9.648374887274155, None: 1.333520365628341}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: left, reward: -40.8827612176
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 30, 't': 5, 'action': 'left', 'reward': -40.88276121759671, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.88)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -9.960819111960085, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 2.0217272284881656}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: None, reward: 1.77458245839
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 29, 't': 6, 'action': None, 'reward': 1.7745824583944667, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -7.318999074582006, 'left': -4.78455040624589, 'right': 0.5764593467306396, None: 0.912944214911987}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: forward, reward: -10.4603114831
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 28, 't': 7, 'action': 'forward', 'reward': -10.460311483053548, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.46)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.960819111960085, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 1.8981548434413162}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: None, reward: 2.65470481076
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 27, 't': 8, 'action': None, 'reward': 2.654704810762566, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.65)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 1.6603750940692192, 'left': -0.12744884244237797, 'right': 1.1885694334324208, None: -4.8011743063904895}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: None, reward: -5.44409650393
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 26, 't': 9, 'action': None, 'reward': -5.444096503926654, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.44)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.06555005710380346, 'right': 0.23313818593284935, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: forward, reward: 2.34727586365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'forward'), 'deadline': 25, 't': 10, 'action': 'forward', 'reward': 2.3472758636460025, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, 'forward')
Agent followed the waypoint forward. (rewarded 2.35)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: left, reward: -40.2729811791
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', 'forward', 'right'), 'deadline': 24, 't': 11, 'action': 'left', 'reward': -40.272981179076126, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', 'forward', 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.27)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.960819111960085, 'left': -9.481524632544945, 'right': 0.4115236866870507, None: 2.276429827101941}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: left, reward: -10.6993869709
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 23, 't': 12, 'action': 'left', 'reward': -10.699386970874489, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.70)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.7788552607979782, 'left': 1.5044822213826454, 'right': 0.7917180586350323, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: -0.0737120006857
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 22, 't': 13, 'action': 'left', 'reward': -0.07371200068570627, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded -0.07)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.443409612759366, 'left': -10.257782208431163, 'right': 1.9641764735868192, None: 1.0018370062171247}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: None, reward: 1.46945659614
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 21, 't': 14, 'action': None, 'reward': 1.4694565961360324, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -29.694482379191278, 'left': -34.49220078987247, 'right': -19.320088992424687, None: 1.9746901210635759}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: -19.232233357
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 20, 't': 15, 'action': 'right', 'reward': -19.232233356962606, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.23)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.443409612759366, 'left': -10.257782208431163, 'right': 1.9641764735868192, None: 1.2356468011765784}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: right, reward: 0.830372490793
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 16, 'action': 'right', 'reward': 0.8303724907929164, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 0.83)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.960819111960085, 'left': -10.090455801709716, 'right': 0.4115236866870507, None: 2.276429827101941}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: right, reward: 0.454180394154
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 18, 't': 17, 'action': 'right', 'reward': 0.45418039415404, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.45)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.6877294880707738, 'left': 1.4205448621121166, 'right': 1.21748992808674, None: -4.023393744880231}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: None, reward: -5.82555416846
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 17, 't': 18, 'action': None, 'reward': -5.825554168461656, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.83)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.6877294880707738, 'left': 1.4205448621121166, 'right': 1.21748992808674, None: -4.924473956670943}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: 0.425847521196
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 16, 't': 19, 'action': 'forward', 'reward': 0.4258475211958904, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove forward instead of left. (rewarded 0.43)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.744226866117678, 'left': -9.7277904772404, 'right': 0.26927372339470435, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: -10.4791553871
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 15, 't': 20, 'action': 'forward', 'reward': -10.47915538712748, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.48)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -9.754493042326072, 'left': -9.351965672503589, 'right': 0.1849119612250598, None: 1.9474841843045714}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: -9.82946781219
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 14, 't': 21, 'action': 'forward', 'reward': -9.829467812185863, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.83)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.468274239306827, 'left': -20.36370753535016, 'right': 0.6790982035521745, None: -4.507316180043739}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: None, reward: -4.99970308953
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 13, 't': 22, 'action': None, 'reward': -4.999703089526521, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.00)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 0.5567885046333321, 'left': 1.4205448621121166, 'right': 1.21748992808674, None: -4.924473956670943}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: None, reward: -5.0895436467
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 12, 't': 23, 'action': None, 'reward': -5.089543646700797, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.39721910935771626, 'left': 1.0553712889259184, 'right': 0.5284232638443994, None: -2.577849171873072}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: 0.568376168656
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 11, 't': 24, 'action': 'forward', 'reward': 0.5683761686558051, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent drove forward instead of left. (rewarded 0.57)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.5007671309352947}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: -40.9901436077
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'left', 'forward'), 'deadline': 10, 't': 25, 'action': 'forward', 'reward': -40.990143607719624, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.99)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
state
{'forward': -9.455517593438483, 'left': -9.103242271339791, 'right': 0.14458206381830013, None: 1.464351558695249}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: -10.1543783463
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 9, 't': 26, 'action': 'forward', 'reward': -10.154378346269896, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.15)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
state
{'forward': -20.495071803859812, 'left': 0.0, 'right': 0.0, None: 0.5007671309352947}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: left, reward: -39.1630045253
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'left', 'forward'), 'deadline': 8, 't': 27, 'action': 'left', 'reward': -39.16300452527305, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.16)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
state
{'forward': -9.80494796985419, 'left': -9.103242271339791, 'right': 0.14458206381830013, None: 1.464351558695249}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: None, reward: 0.576865939412
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 7, 't': 28, 'action': None, 'reward': 0.5768659394124376, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 0.58)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
state
{'forward': 0.7204466043580753, 'left': 1.8155453077353139, 'right': 1.2564525824057489, None: -5.522909038559298}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: None, reward: -5.07223389227
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 6, 't': 29, 'action': None, 'reward': -5.072233892274362, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.07)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Environment.step(): t = 30
state
{'forward': 0.13580510957883626, 'left': -14.964496825977037, 'right': 0.002219643778226421, None: -2.0691990998302656}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: left, reward: -20.7048345656
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 5, 't': 30, 'action': 'left', 'reward': -20.704834565593565, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.70)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Environment.step(): t = 31
state
{'forward': 0.4783397258548069, 'left': 1.7024435629210446, 'right': 1.1953234434716113, None: -4.927049471362601}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: right, reward: 0.313701580472
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 4, 't': 31, 'action': 'right', 'reward': 0.31370158047238295, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded 0.31)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Environment.step(): t = 32
state
{'forward': -4.95562719072588, 'left': -4.676426717449235, 'right': 0.0, None: 0.5021436515621278}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: right, reward: -0.0257050364624
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', 'right'), 'deadline': 3, 't': 32, 'action': 'right', 'reward': -0.025705036462430053, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'right')
Agent drove right instead of left. (rewarded -0.03)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Environment.step(): t = 33
state
{'forward': 0.31801371719264415, 'left': 0.4337406785417416, 'right': 1.0586369613300912, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: 0.932108765043
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 2, 't': 33, 'action': 'left', 'reward': 0.9321087650431936, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent drove left instead of right. (rewarded 0.93)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Environment.step(): t = 34
state
{'forward': -9.80494796985419, 'left': -9.103242271339791, 'right': 0.14458206381830013, None: 1.0206087490538431}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: left, reward: -9.85383535539
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 1, 't': 34, 'action': 'left', 'reward': -9.853835355393318, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.85)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 88
\-------------------------

Environment.reset(): Trial set up with start = (4, 6), destination = (8, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.7204466043580753, 'left': 1.8155453077353139, 'right': 1.2564525824057489, None: -5.29757146541683}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 6), heading: (1, 0), action: None, reward: -4.79378886329
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 25, 't': 0, 'action': None, 'reward': -4.7937888632946954, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.79)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.7204466043580753, 'left': 1.8155453077353139, 'right': 1.2564525824057489, None: -5.045680164355763}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: forward, reward: 0.935154442378
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 0.9351544423783478, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.94)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.926470164537662, 'left': -10.27281183885492, 'right': 0.43435820559874033, None: 1.8812363375278034}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: forward, reward: -10.5671841309
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': -10.567184130853601, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 0.996798761575823, None: -0.006021094220934636}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (1, 0), action: None, reward: 1.77968303028
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.7796830302806896, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.78)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 0.996798761575823, None: 0.8868309680298775}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: right, reward: 1.84422640455
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 1.844226404551308, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 1.84)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.5567885046333321, 'left': 1.4205448621121166, 'right': 1.21748992808674, None: -5.0070088016858705}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 7), heading: (0, 1), action: None, reward: -4.37212835749
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 20, 't': 5, 'action': None, 'reward': -4.372128357490631, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.37)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.2175187299164125, 'left': 0.8164007893986294, 'right': 0.803656615678915, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: right, reward: 1.04389392884
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 19, 't': 6, 'action': 'right', 'reward': 1.043893928841514, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 1.04)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.21779820909016448, None: 1.6855171000119582}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: None, reward: 2.38381693278
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': 2.383816932777665, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.38)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -4.63407448875485, 'left': -19.929222317188433, 'right': 0.42390243535648303, None: 0.5820131931604572}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: forward, reward: -9.21441109736
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -9.214411097361799, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -9.21)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': -5.429128493021558, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: forward, reward: -9.55115303312
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -9.551153033118936, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.55)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.0618337728920777, 'left': -19.211698031919653, 'right': 0.33513004137432534, None: -4.066550417004045}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (0, -1), action: right, reward: 0.981498791142
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 0.9814987911417267, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.98)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.8223469461316817, 'left': 0.9658292867047729, 'right': 0.4916810102821621, None: -4.046450134383133}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: left, reward: 1.23919698354
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 1.2391969835429535, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'right', None)
Agent followed the waypoint left. (rewarded 1.24)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -5.21216226674209, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: 1.26140817084
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'right'), 'deadline': 13, 't': 12, 'action': None, 'reward': 1.2614081708431129, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'right')
Agent properly idled at a red light. (rewarded 1.26)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 1.4205125830635654, None: 0.8868309680298775}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 0.324697369086
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 0.3246973690859445, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.32)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -4.9249023521925785, 'left': -7.253778312671759, 'right': 0.3580183677954005, None: 0.4268911878731215}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 1.44629993524
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.4462999352419694, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent drove right instead of left. (rewarded 1.45)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.8067972133765415, 'left': 1.0418452595807792, 'right': 1.7718865120574352, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: forward, reward: 0.245859116985
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': 0.24585911698539364, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.25)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -34.78797249231466, 'left': -30.264678799508147, 'right': -9.648374887274155, None: 1.333520365628341}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: left, reward: -39.767992798
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'forward'), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -39.76799279796101, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.77)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.960819111960085, 'left': -10.090455801709716, 'right': 0.43285204042054537, None: 2.276429827101941}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: forward, reward: -10.3402625836
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': -10.340262583567545, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.34)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -10.150540847763814, 'left': -10.090455801709716, 'right': 0.43285204042054537, None: 2.276429827101941}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: right, reward: 1.16133391359
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 1.1613339135872094, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.16)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -35.52245149873117, 'left': -19.975079367288057, 'right': -10.20376779627894, None: 1.7623181620498753}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: left, reward: -39.036791666
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -39.03679166602757, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.04)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.696587806966443, 'left': -9.641834157999858, 'right': 1.272677530697571, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 6), heading: (-1, 0), action: right, reward: 0.528576623051
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 0.5285766230513562, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.53)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 2.075370254530051, 'left': 0.28848309421210044, 'right': 0.15353933489711996, None: -4.75323104634653}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: forward, reward: 1.0949978869
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': 1.0949978869024382, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 1.09)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.5851840707162448, 'left': 0.28848309421210044, 'right': 0.15353933489711996, None: -4.75323104634653}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: None, reward: -4.5766638299
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 3, 't': 22, 'action': None, 'reward': -4.576663829898614, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.58)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.5851840707162448, 'left': 0.28848309421210044, 'right': 0.15353933489711996, None: -4.664947438122572}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: 0.412677338304
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.4126773383038911, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove right instead of forward. (rewarded 0.41)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.8278005233682115, 'left': 1.8155453077353139, 'right': 1.2564525824057489, None: -5.045680164355763}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: left, reward: 0.513718914659
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 24, 'action': 'left', 'reward': 0.5137189146592798, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 0.51)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 89
\-------------------------

Environment.reset(): Trial set up with start = (4, 3), destination = (8, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.8278005233682115, 'left': 1.1646321111972968, 'right': 1.2564525824057489, None: -5.045680164355763}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: left, reward: 2.48051027422
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 20, 't': 0, 'action': 'left', 'reward': 2.4805102742182767, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.48)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.6603750940692192, 'left': -0.12744884244237797, 'right': 1.1885694334324208, None: -5.122635405158572}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: right, reward: 0.428596409403
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 0.4285964094028013, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.791980427255968, 'left': -9.351965672503589, 'right': 0.1849119612250598, None: 1.9474841843045714}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: 0.979311642802
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 18, 't': 2, 'action': None, 'reward': 0.9793116428016155, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 0.98)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.791980427255968, 'left': -9.351965672503589, 'right': 0.1849119612250598, None: 1.4633979135530935}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: forward, reward: -9.21113493431
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -9.211134934307532, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -9.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.5797471502419209, 'left': -9.908660606911534, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: -4.94348602237
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', None), 'deadline': 16, 't': 4, 'action': None, 'reward': -4.943486022371191, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.94)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.8426208992618491, 'left': 2.2901405895030593, 'right': 0.22406570957439165, None: -4.877363809270012}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 2), heading: (0, -1), action: None, reward: -4.08980356558
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 15, 't': 5, 'action': None, 'reward': -4.089803565575085, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.2175187299164125, 'left': 0.8164007893986294, 'right': 0.9237752722602146, None: -2.944634633464579}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: right, reward: 1.13321205854
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'forward'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.133212058540075, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', 'forward')
Agent drove right instead of left. (rewarded 1.13)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.116999539250746, 'left': 0.636568114417346, 'right': 1.7138782790864349, None: -0.0662184810624735}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: None, reward: 1.46456873792
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': 1.464568737924257, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.46)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.5263281651809676, 'left': 1.0418452595807792, 'right': 1.7718865120574352, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: left, reward: 1.45057095
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': 1.4505709499980317, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.45)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.48279763900676065, 'left': 1.0553712889259184, 'right': 0.5284232638443994, None: -2.577849171873072}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: right, reward: 0.843112712245
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 0.8431127122454014, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent drove right instead of left. (rewarded 0.84)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 1.6603750940692192, 'left': -0.12744884244237797, 'right': 0.808582921417611, None: -5.122635405158572}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: left, reward: 0.405207751085
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 0.40520775108540186, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.41)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: right, reward: -20.6245548052
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', 'forward'), 'deadline': 9, 't': 11, 'action': 'right', 'reward': -20.624554805184683, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.62)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -19.835655796651018, 'left': -34.35645224674984, 'right': -18.871830913097583, None: 0.5511311871385264}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 6), heading: (0, -1), action: None, reward: 2.06036761482
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'left'), 'deadline': 8, 't': 12, 'action': None, 'reward': 2.06036761482307, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 2.06)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.8517347734094066, 'left': 0.6859138143467407, 'right': 1.4218465218358454, None: -4.57363600787029}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: right, reward: 0.814443048044
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.8144430480437661, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 0.81)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: None, reward: -5.2076773016
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', 'left'), 'deadline': 6, 't': 14, 'action': None, 'reward': -5.207677301603867, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.21)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -19.695138272401383, 'left': -19.85217377853075, 'right': -10.34416031428023, None: 0.8808114996176329}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: None, reward: 1.79290404425
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', 'left'), 'deadline': 5, 't': 15, 'action': None, 'reward': 1.7929040442483457, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', 'left')
Agent properly idled at a red light. (rewarded 1.79)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.0, 'left': 0.0, 'right': 0.260295293935688, None: 1.1933779785915037}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 6), heading: (1, 0), action: left, reward: -9.59591243516
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'right'), 'deadline': 4, 't': 16, 'action': 'left', 'reward': -9.595912435158175, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'right')
Agent attempted driving left through a red light. (rewarded -9.60)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.6603750940692192, 'left': 0.13887945432151194, 'right': 0.808582921417611, None: -5.122635405158572}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: forward, reward: 1.6462510402
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 1.64625104020368, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.65)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.7590719014093665, 'left': -0.03953712785440244, 'right': 1.2476697526164655, None: -2.758206280159728}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: 2.07087568704
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': 2.07087568704366, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent followed the waypoint forward. (rewarded 2.07)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -29.694482379191278, 'left': -34.49220078987247, 'right': -19.276161174693648, None: 1.9746901210635759}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: 1.07071445858
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 1, 't': 19, 'action': None, 'reward': 1.070714458579772, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.07)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 90
\-------------------------

Environment.reset(): Trial set up with start = (4, 6), destination = (1, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.5263281651809676, 'left': 1.2462081047894054, 'right': 1.7718865120574352, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: left, reward: 0.555420560197
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 25, 't': 0, 'action': 'left', 'reward': 0.5554205601970397, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 0.56)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.0695242519774322, None: -4.774430435787325}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 5), heading: (0, -1), action: None, reward: -4.12017097134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 24, 't': 1, 'action': None, 'reward': -4.120170971336464, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.0695242519774322, None: -4.447300703561894}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (1, 0), action: right, reward: 1.19477207363
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.194772073631127, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove right instead of left. (rewarded 1.19)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.7788552607979782, 'left': 0.7153851103484696, 'right': 0.7917180586350323, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: left, reward: 1.85743272452
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 1.857432724517392, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded 1.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 1.0931357422166865, 'left': 0.4823217858378716, 'right': 2.026009232240648, None: -5.2508786771186955}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: None, reward: -5.00240137803
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 21, 't': 4, 'action': None, 'reward': -5.002401378030813, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.00)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.0931357422166865, 'left': 0.4823217858378716, 'right': 2.026009232240648, None: -5.126640027574754}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: right, reward: 2.70787341154
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 2.707873411535533, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.71)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.6533130671364495, 'left': 0.13887945432151194, 'right': 0.808582921417611, None: -5.122635405158572}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: right, reward: 0.814807238228
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 19, 't': 6, 'action': 'right', 'reward': 0.8148072382281847, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -19.942531269156774, 'left': -37.15804930756421, 'right': 0.5173330090882928, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: -40.638039794
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -40.63803979395169, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.64)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -5.437513937771278, 'left': -8.792304191906092, 'right': 1.2957178957186573, None: 1.202485977287252}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: None, reward: 0.933309803681
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 17, 't': 8, 'action': None, 'reward': 0.9333098036807492, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 0.93)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -30.290285531554233, 'left': -37.15804930756421, 'right': 0.5173330090882928, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: right, reward: 0.963053804953
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 0.9630538049527393, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 0.96)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -30.290285531554233, 'left': -37.15804930756421, 'right': 0.7401934070205161, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 5), heading: (-1, 0), action: forward, reward: -40.220104468
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': -40.22010446802528, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.22)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.696587806966443, 'left': -9.641834157999858, 'right': 0.9006270768744636, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: right, reward: -0.0900139630938
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 14, 't': 11, 'action': 'right', 'reward': -0.09001396309377452, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded -0.09)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.0, 'left': 0.0, 'right': 0.6632075990003489, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: None, reward: -4.5186240351
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', 'left', 'forward'), 'deadline': 13, 't': 12, 'action': None, 'reward': -4.518624035100255, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'left', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -4.52)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.31801371719264415, 'left': 0.6829247217924677, 'right': 1.0586369613300912, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: right, reward: 1.62166794444
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 1.6216679444385507, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.62)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 0.872604976074755, None: 0.8868309680298775}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: 0.636258120998
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 0.6362581209977013, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.64)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 1.6533130671364495, 'left': 0.13887945432151194, 'right': 0.8116950798228979, None: -5.122635405158572}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: -4.96055719645
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': -4.9605571964491935, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.96)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.6533130671364495, 'left': 0.13887945432151194, 'right': 0.8116950798228979, None: -5.041596300803883}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: -4.43234234061
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -4.432342340607688, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.43)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -10.150540847763814, 'left': -10.090455801709716, 'right': 0.7970929770038774, None: 2.276429827101941}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: forward, reward: -10.0280414906
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': -10.028041490631898, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.03)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -37.886775960523536, 'left': -34.938704442465266, 'right': -0.11811137397118186, None: 1.2148049566155144}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: 2.08633395008
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 7, 't': 18, 'action': None, 'reward': 2.086333950077993, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.09)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -10.089291169197857, 'left': -10.090455801709716, 'right': 0.7970929770038774, None: 2.276429827101941}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: 1.23058857138
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 19, 'action': None, 'reward': 1.2305885713846478, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.23)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -4.775576516559468, 'left': -5.429128493021558, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: None, reward: 0.730175008227
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', None), 'deadline': 5, 't': 20, 'action': None, 'reward': 0.7301750082273284, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', None)
Agent properly idled at a red light. (rewarded 0.73)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.5005213790246086, 'left': -9.658790871308987, 'right': 0.576939607319368, None: -3.9368993957668015}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: left, reward: -20.4714565247
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': -20.47145652473546, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.47)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.44863133736615, 'left': 0.24270504422498995, 'right': 0.26940721727094513, None: -4.963173369259968}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: forward, reward: 0.28217666258
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 0.282176662580067, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent followed the waypoint forward. (rewarded 0.28)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.7788552607979782, 'left': 1.2864089174329307, 'right': 0.7917180586350323, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: right, reward: -0.101404513522
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 2, 't': 23, 'action': 'right', 'reward': -0.10140451352168589, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove right instead of forward. (rewarded -0.10)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': -35.25519499978976, 'left': -37.15804930756421, 'right': 0.7401934070205161, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: forward, reward: -39.6178275484
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -39.6178275484132, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.62)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 91
\-------------------------

Environment.reset(): Trial set up with start = (8, 3), destination = (5, 2), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': -4.5515163183738645, 'right': 0.0, None: 0.8239559534400744}
learning, learn new
zeroAction: ['forward', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: -9.61688783583
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'right'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -9.616887835830934, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -9.62)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -5.470557812601779, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: left, reward: -9.3558613666
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'left'), 'deadline': 19, 't': 1, 'action': 'left', 'reward': -9.35586136659947, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'left', 'left')
Agent attempted driving left through a red light. (rewarded -9.36)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.542371075646654, 'left': -10.119581039079401, 'right': 0.6589970434333647, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: right, reward: 0.148932226122
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 0.14893222612201928, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 0.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.0931357422166865, 'left': 0.4823217858378716, 'right': 2.3669413218880906, None: -5.126640027574754}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: left, reward: 0.4878914527
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 17, 't': 3, 'action': 'left', 'reward': 0.4878914527002146, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent drove left instead of right. (rewarded 0.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.696587806966443, 'left': -9.641834157999858, 'right': 0.40530655689034456, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: forward, reward: -10.0681374621
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': -10.068137462093725, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.07)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.5101954761558323, 'left': 1.9119167826505707, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: 1.22893235192
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 1.2289323519169952, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.23)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -20.160948053227965, 'left': -20.16518434097718, 'right': -14.936053166962726, None: 0.6185419632928775}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: right, reward: -20.208689625
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': -20.20868962502938, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.21)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.882362634530084, 'left': -9.641834157999858, 'right': 0.40530655689034456, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: -9.16579627218
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -9.165796272176706, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.17)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -19.844789135561125, 'left': -19.552627706033697, 'right': -15.365687364910093, None: 1.8938245651212773}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: right, reward: -20.0226335598
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': -20.022633559846327, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.02)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -4.9249023521925785, 'left': -7.253778312671759, 'right': 0.902159151518685, None: 0.4268911878731215}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: -9.44232301647
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -9.442323016466768, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent attempted driving left through a red light. (rewarded -9.44)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.501557680781751, 'left': -9.351965672503589, 'right': 0.1849119612250598, None: 1.4633979135530935}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: -10.4355774947
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': -10.435577494703214, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -10.44)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.468274239306827, 'left': -20.36370753535016, 'right': 0.6790982035521745, None: -4.7535096347851304}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: None, reward: -4.78618078404
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 9, 't': 11, 'action': None, 'reward': -4.786180784039288, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.79)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -0.0005197836940220579, 'left': -10.297785093259217, 'right': 0.0, None: -2.387809036530959}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: right, reward: 0.397621273016
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.397621273015508, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', None)
Agent drove right instead of left. (rewarded 0.40)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -29.31218223748069, 'left': -29.733978514866102, 'right': 0.5007637796767701, None: 1.364334499731736}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: right, reward: -0.299172249066
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 7, 't': 13, 'action': 'right', 'reward': -0.2991722490663563, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent drove right instead of forward. (rewarded -0.30)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.1321481628042795, None: -4.447300703561894}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: -5.4010844182
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 6, 't': 14, 'action': None, 'reward': -5.401084418197554, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.40)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -30.08356539191574, 'left': -19.6137721724875, 'right': -10.314457460120506, None: 1.5769507236047704}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: -40.6724438459
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -40.67244384594568, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.67)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.524079453353394, 'left': -9.641834157999858, 'right': 0.40530655689034456, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: left, reward: -10.3350885345
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 16, 'action': 'left', 'reward': -10.33508853447439, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.34)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.8278005233682115, 'left': 1.8225711927077868, 'right': 1.2564525824057489, None: -5.045680164355763}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: -5.79186644238
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': None, 'reward': -5.791866442379202, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.79)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.4946062192501341, 'left': 0.49128499748010235, 'right': 0.5646305477340334, None: 0.8822386255297519}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: left, reward: 1.57516977302
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 1.575169773021577, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent followed the waypoint left. (rewarded 1.58)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -7.77810412778626, 'left': -8.412327627634511, 'right': 0.8206224210389526, None: 1.628960376051345}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: None, reward: 1.18765573642
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 1, 't': 19, 'action': None, 'reward': 1.1876557364186826, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.19)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 92
\-------------------------

Environment.reset(): Trial set up with start = (1, 7), destination = (5, 7), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.0598342177161721}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: right, reward: 1.87672354265
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', 'left'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.8767235426460915, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'left', 'left')
Agent drove right instead of left. (rewarded 1.88)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.8654039999731085, 'left': 0.24270504422498995, 'right': 0.26940721727094513, None: -4.963173369259968}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: forward, reward: 2.1113266132
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 2.1113266131969817, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent followed the waypoint forward. (rewarded 2.11)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -6.991676951845406, 'left': -4.950000943770685, 'right': 0.46038862309963297, None: 0.6912272734238653}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: right, reward: 1.93319002805
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 1.9331900280475698, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent drove right instead of forward. (rewarded 1.93)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.501557680781751, 'left': -9.893771583603401, 'right': 0.1849119612250598, None: 1.4633979135530935}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: left, reward: -10.2286359437
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -10.228635943745783, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -10.23)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.6196463321593687, 'left': -10.457848570063845, 'right': 0.0, None: -2.7860201112177005}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (-1, 0), action: right, reward: 1.39220467084
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', None), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.392204670838698, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', None)
Agent drove right instead of left. (rewarded 1.39)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.33837327902890924, 'left': 0.4246014456426003, 'right': 0.4134609761062362, None: -2.9332972625096314}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: right, reward: 2.23664559123
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 2.236645591234117, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent followed the waypoint right. (rewarded 2.24)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.8655193642437065, 'left': 1.1211709333813886, 'right': 0.4132768788097685, None: -0.06799419960188136}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: None, reward: 1.35577235431
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 14, 't': 6, 'action': None, 'reward': 1.3557723543112865, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent idled at a green light with oncoming traffic. (rewarded 1.36)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.11249816202689822, 'left': 0.9689580603068322, 'right': 1.1207916355646377, None: 0.4364052249845423}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 6), heading: (0, -1), action: forward, reward: 0.991606260694
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 0.9916062606938711, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent drove forward instead of right. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.8108755205127882, 'left': 0.9394362930708091, 'right': 1.1727471922073036, None: -4.602771365858199}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: 0.121251862229
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 0.12125186222908402, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove forward instead of right. (rewarded 0.12)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.31801371719264415, 'left': 0.6829247217924677, 'right': 1.340152452884321, None: -2.3610166201851386}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: None, reward: -5.36850987146
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 11, 't': 9, 'action': None, 'reward': -5.368509871461565, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.37)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.31801371719264415, 'left': 0.6829247217924677, 'right': 1.340152452884321, None: -3.864763245823352}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (1, 0), action: right, reward: 2.49906885951
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 2.4990688595089696, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 2.50)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -10.089291169197857, 'left': -10.090455801709716, 'right': 0.7970929770038774, None: 1.7535091992432945}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 6), heading: (0, 1), action: right, reward: -0.274743490505
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': -0.2747434905046604, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded -0.27)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.48279763900676065, 'left': 1.0553712889259184, 'right': 0.6857679880449004, None: -2.577849171873072}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: forward, reward: 1.30520481822
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': 1.3052048182159648, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent drove forward instead of left. (rewarded 1.31)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.1321481628042795, None: -4.924192560879725}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: None, reward: -4.47885636722
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 7, 't': 13, 'action': None, 'reward': -4.478856367223828, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.48)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.2830064613684048, 'left': -10.245900083020738, 'right': 0.15722670441053832, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: None, reward: -4.87629500499
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', 'left'), 'deadline': 6, 't': 14, 'action': None, 'reward': -4.876295004988203, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', 'forward', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.5674714937044425, 'left': 1.9383741304261004, 'right': 1.1321481628042795, None: -4.701524464051776}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: forward, reward: 0.356977519228
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'left'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': 0.3569775192281779, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'left')
Agent drove forward instead of left. (rewarded 0.36)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.5101954761558323, 'left': 1.570424567283783, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: left, reward: 1.83409686403
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 4, 't': 16, 'action': 'left', 'reward': 1.8340968640274489, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.83)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -20.192301083783548, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: left, reward: -39.6975406793
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'forward', 'left'), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -39.69754067926616, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.70)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: left, reward: -9.56819630521
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', 'left'), 'deadline': 2, 't': 18, 'action': 'left', 'reward': -9.568196305206094, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', 'left')
Agent attempted driving left through a red light. (rewarded -9.57)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 2), heading: (1, 0), action: None, reward: -5.63533623484
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', 'left', 'left'), 'deadline': 1, 't': 19, 'action': None, 'reward': -5.635336234839879, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', 'forward', 'left', 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -5.64)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 93
\-------------------------

Environment.reset(): Trial set up with start = (5, 7), destination = (2, 5), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.824277666063679, 'left': -15.277379805247598, 'right': 0.2778330016659695, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: forward, reward: 0.180556260134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': 0.18055626013385528, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded 0.18)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.7788552607979782, 'left': 1.2864089174329307, 'right': 0.3451567725566732, None: -3.944331076198024}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: None, reward: -4.28146958547
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 24, 't': 1, 'action': None, 'reward': -4.2814695854737606, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent idled at a green light with no oncoming traffic. (rewarded -4.28)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 1.0373702368052968, 'left': 0.7176682277388182, 'right': 0.4254638151155375, None: -4.199422231553411}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: forward, reward: 1.93122639527
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 1.9312263952672857, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent followed the waypoint forward. (rewarded 1.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -30.297880834760285, 'left': -30.280947970215564, 'right': -15.377864351262089, None: 1.8213279540068852}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: right, reward: -20.1393861564
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 22, 't': 3, 'action': 'right', 'reward': -20.139386156370577, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.14)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -6.924242793058324, 'left': -19.929222317188433, 'right': 0.42390243535648303, None: 0.5820131931604572}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: left, reward: -39.6746835897
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -39.674683589732425, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.6533130671364495, 'left': 0.13887945432151194, 'right': 0.8116950798228979, None: -4.736969320705786}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: right, reward: 1.32607966542
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 1.3260796654182494, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.33)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': -0.2430472045848613, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -10.0224603212
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, 'left'), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': -10.02246032118554, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.02)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -19.844789135561125, 'left': -19.552627706033697, 'right': -17.69416046237821, None: 1.8938245651212773}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -39.1765772569
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -39.17657725694978, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.18)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -30.058309918928828, 'left': -20.407072234619378, 'right': 0.10768731177632035, None: 0.8714150576196961}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -40.3874211975
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -40.387421197468754, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.39)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 0.3787996367460704, None: -2.9311443648864515}
learning, learn new
zeroAction: ['forward', 'left']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: forward, reward: -0.0295399202251
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'left'), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -0.029539920225143845, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'left')
Agent drove forward instead of left. (rewarded -0.03)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -10.11169112662258, 'left': -9.7277904772404, 'right': 0.26927372339470435, None: 2.01383016317055}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: None, reward: 0.919430740409
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 15, 't': 10, 'action': None, 'reward': 0.9194307404090343, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 0.92)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.524079453353394, 'left': -9.988461346237123, 'right': 0.40530655689034456, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: left, reward: -10.6375677817
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -10.637567781736385, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.64)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.524079453353394, 'left': -10.313014563986755, 'right': 0.40530655689034456, None: 1.1962683965002248}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: None, reward: 2.13472479345
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': None, 'reward': 2.1347247934499007, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.0, 'left': 1.1793536892205825, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: right, reward: 1.54334331242
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', 'left'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 1.543343312418583, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'right', 'left')
Agent drove right instead of left. (rewarded 1.54)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.05894904310659793, 'left': 1.7695400650952917, 'right': -0.03778302349483492, None: 0.8153019713135179}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: None, reward: 0.988870411754
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 0.9888704117541024, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 0.99)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -7.428255477928121, 'left': -4.792915590347111, 'right': 0.7074505672721305, None: 1.058868631800408}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: None, reward: 1.56340348462
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.5634034846151148, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'right')
Agent properly idled at a red light. (rewarded 1.56)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -9.524079453353394, 'left': -10.313014563986755, 'right': 0.40530655689034456, None: 1.6654965949750626}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: right, reward: 1.16795696229
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 1.1679569622888266, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.17)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.717802908212047, 'left': 0.4201186811851193, 'right': 0.5029408890303542, None: -3.3631655834043013}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: left, reward: 1.19885092365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': 1.198850923654671, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove left instead of right. (rewarded 1.20)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 1.6533130671364495, 'left': 0.13887945432151194, 'right': 1.0688873726205737, None: -4.736969320705786}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: left, reward: 0.25809217107
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': 0.2580921710704367, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.26)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -20.202140757956546, 'left': -19.833166387044734, 'right': -9.578054242763955, None: 1.3078885640891256}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: forward, reward: -40.8233388244
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -40.82333882438114, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.82)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -10.094530292640922, 'left': -10.086016616220904, 'right': 1.2767878458856967, None: 2.4694261712483194}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: None, reward: 2.39703817508
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': None, 'reward': 2.3970381750758296, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.40)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 1.116999539250746, 'left': 0.636568114417346, 'right': 1.7138782790864349, None: 0.6991751284308917}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: left, reward: 0.816028231204
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 4, 't': 21, 'action': 'left', 'reward': 0.8160282312037894, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.82)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 1.7788552607979782, 'left': 1.2864089174329307, 'right': 0.3451567725566732, None: -4.112900330835892}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: left, reward: -0.717276559661
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -0.7172765596605855, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove left instead of forward. (rewarded -0.72)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.0931357422166865, 'left': 0.4851066192690431, 'right': 2.3669413218880906, None: -5.126640027574754}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: None, reward: -5.23330736414
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 2, 't': 23, 'action': None, 'reward': -5.233307364142975, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.23)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.0931357422166865, 'left': 0.4851066192690431, 'right': 2.3669413218880906, None: -5.179973695858864}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: right, reward: 0.24884243017
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.24884243016985796, 'waypoint': 'right'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 0.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 94
\-------------------------

Environment.reset(): Trial set up with start = (8, 5), destination = (4, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.8117664406292029, 'left': 0.4225708722263659, 'right': 1.0327289680605438, None: 0.5495312135454218}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: 0.151113180471
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 25, 't': 0, 'action': 'left', 'reward': 0.15111318047129307, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove left instead of right. (rewarded 0.15)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 1.7788552607979782, 'left': 0.28456617888617264, 'right': 0.3451567725566732, None: -4.112900330835892}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: right, reward: 0.225215297969
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 0.22521529796882023, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.23)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.5101954761558323, 'left': 1.702260715655616, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: forward, reward: 0.194129170542
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 0.19412917054221035, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 0.19)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -35.52245149873117, 'left': -29.505935516657814, 'right': -10.20376779627894, None: 1.7623181620498753}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: left, reward: -39.7561553978
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 22, 't': 3, 'action': 'left', 'reward': -39.75615539780717, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.76)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.542371075646654, 'left': -10.119581039079401, 'right': 0.403964634777692, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: left, reward: -9.41415753129
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 21, 't': 4, 'action': 'left', 'reward': -9.41415753128949, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.05894904310659793, 'left': 1.7695400650952917, 'right': -0.03778302349483492, None: 0.9020861915338101}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: None, reward: 1.90437787751
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 20, 't': 5, 'action': None, 'reward': 1.9043778775100968, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent idled at a green light with oncoming traffic. (rewarded 1.90)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.5024169630987672, 'left': -15.277379805247598, 'right': 0.2778330016659695, None: -2.0568724677151353}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: None, reward: -4.51078642926
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 19, 't': 6, 'action': None, 'reward': -4.510786429263598, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.51)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.5024169630987672, 'left': -15.277379805247598, 'right': 0.2778330016659695, None: -3.283829448489367}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: right, reward: 1.6476514884
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.6476514883986446, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove right instead of left. (rewarded 1.65)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.3675569979993027, 'left': 0.2510051396862819, 'right': 0.872604976074755, None: 0.7615445445137894}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: 1.26540812888
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 1.265408128880816, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent drove left instead of forward. (rewarded 1.27)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -7.823219223716316, 'left': -5.387928055003375, 'right': 0.7970204702538358, None: 1.0159805132425914}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 1.85466555946
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 16, 't': 9, 'action': None, 'reward': 1.8546655594563806, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.85)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -9.420263126334003, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 2.217436731043758}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: forward, reward: -9.28408120676
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': -9.284081206756728, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.28)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -7.823219223716316, 'left': -5.387928055003375, 'right': 0.7970204702538358, None: 1.435323036349486}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 1.32587170603
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 14, 't': 11, 'action': None, 'reward': 1.3258717060255774, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent properly idled at a red light. (rewarded 1.33)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.443409612759366, 'left': -10.257782208431163, 'right': 1.397274482189868, None: 1.2356468011765784}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: right, reward: 1.21405276814
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 1.214052768136543, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.21)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -10.089291169197857, 'left': -10.090455801709716, 'right': 0.2611747432496085, None: 1.7535091992432945}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: forward, reward: -10.5118491821
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': -10.511849182117729, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.51)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.300570175657793, 'left': -10.090455801709716, 'right': 0.2611747432496085, None: 1.7535091992432945}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: None, reward: 2.22723328116
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 2.2272332811599744, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.23)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.300570175657793, 'left': -10.090455801709716, 'right': 0.2611747432496085, None: 1.9903712402016345}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: None, reward: 1.84771624966
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.8477162496560369, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.6533130671364495, 'left': 0.19848581269597432, 'right': 1.0688873726205737, None: -4.736969320705786}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: None, reward: -5.76700950166
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -5.767009501664291, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.77)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.7788552607979782, 'left': 0.28456617888617264, 'right': 0.2851860352627467, None: -4.112900330835892}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: forward, reward: 0.767647907335
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': 0.7676479073347968, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 0.77)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.21779820909016448, None: 2.0346670163948115}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: right, reward: -0.139404395623
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': -0.13940439562303175, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded -0.14)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.542371075646654, 'left': -9.766869285184445, 'right': 0.403964634777692, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -9.06670257819
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -9.06670257819432, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.07)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -9.304536826920486, 'left': -9.766869285184445, 'right': 0.403964634777692, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: -10.4984140593
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -10.49841405930109, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.50)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': 0.05894904310659793, 'left': 1.7695400650952917, 'right': -0.03778302349483492, None: 1.4032320345219533}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 1.11124552992
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 4, 't': 21, 'action': 'right', 'reward': 1.1112455299160153, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove right instead of left. (rewarded 1.11)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -9.726249389465096, 'left': -9.3031822039328, 'right': 1.2361340504276157, None: 1.1017956033563556}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 1.5827023051
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 3, 't': 22, 'action': 'right', 'reward': 1.5827023050983182, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent followed the waypoint right. (rewarded 1.58)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -0.2367538979223509, 'left': 0.19306097650467458, 'right': 1.19883369295815, None: -0.11886393746254198}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: right, reward: 1.79382945818
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 1.793829458176258, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent followed the waypoint right. (rewarded 1.79)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 1.4149737942265133, 'left': -0.03953712785440244, 'right': 1.2476697526164655, None: -2.758206280159728}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: None, reward: -5.4661539147
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 1, 't': 24, 'action': None, 'reward': -5.466153914701711, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 95
\-------------------------

Environment.reset(): Trial set up with start = (1, 3), destination = (3, 6), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: -19.9947324027
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', 'left'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': -19.994732402659814, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'forward', 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.99)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -5.331776840641675, 'left': 0.0, 'right': 0.0, None: 1.3330048740770972}
learning, learn new
zeroAction: ['left', 'right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 2.46396087963
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', 'left'), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 2.4639608796279258, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', 'right', 'left')
Agent followed the waypoint right. (rewarded 2.46)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.5263281651809676, 'left': 0.9008143324932225, 'right': 1.7718865120574352, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (1, 0), action: right, reward: 1.83962384324
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 1.8396238432366607, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.2677589901531214, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: right, reward: 0.88553488569
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'forward'), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 0.885534885689591, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, 'forward')
Agent drove right instead of forward. (rewarded 0.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.501557680781751, 'left': -10.061203763674591, 'right': 0.1849119612250598, None: 1.4633979135530935}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 0.815798378196
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 0.8157983781958366, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.726249389465096, 'left': -9.3031822039328, 'right': 1.409418177762967, None: 1.1017956033563556}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -10.4015188713
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': -10.401518871284043, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -10.40)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.0, 'left': 0.0, 'right': 0.48560890699616654, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: -5.64576951271
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', None), 'deadline': 19, 't': 6, 'action': None, 'reward': -5.645769512712366, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'forward', 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.65)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: left, reward: 0.348594520417
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'left'), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 0.34859452041716255, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'left', 'left')
Agent drove left instead of right. (rewarded 0.35)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -10.404313092443504, 'left': -20.420999649401967, 'right': 0.396662864847129, None: 0.8746433973547447}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: left, reward: -39.6740917889
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 17, 't': 8, 'action': 'left', 'reward': -39.674091788873874, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.7256171267558971, 'left': 1.1468224946594585, 'right': -0.016050334634434504, None: -2.5977189400569203}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: None, reward: -5.74665138771
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 16, 't': 9, 'action': None, 'reward': -5.7466513877140395, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.75)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.3521623233490213, 'left': 1.702260715655616, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: 1.69135280697
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 1.6913528069731054, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 1.69)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.468274239306827, 'left': -20.36370753535016, 'right': 0.6790982035521745, None: -4.769845209412209}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: left, reward: -19.5623589592
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -19.562358959173537, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.56)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.8278005233682115, 'left': 1.8225711927077868, 'right': 1.2564525824057489, None: -5.418773303367482}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: None, reward: -4.92851613731
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 13, 't': 12, 'action': None, 'reward': -4.9285161373118385, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.8278005233682115, 'left': 1.8225711927077868, 'right': 1.2564525824057489, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: 1.60175161466
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 1.601751614655494, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.60)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -8.889655278817777, 'left': -4.78455040624589, 'right': 0.5764593467306396, None: 0.912944214911987}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: forward, reward: -10.2221557138
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -10.222155713809416, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.22)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.300570175657793, 'left': -10.090455801709716, 'right': 0.2611747432496085, None: 1.9190437449288358}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: right, reward: 1.42931039339
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': 'right', 'reward': 1.429310393391238, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 1.43)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -10.11169112662258, 'left': -9.7277904772404, 'right': 0.26927372339470435, None: 1.4666304517897921}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 1.8826862586
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 9, 't': 16, 'action': None, 'reward': 1.8826862585966002, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.88)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -37.43651127410148, 'left': -37.15804930756421, 'right': 0.7401934070205161, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: right, reward: 0.0159119246187
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 8, 't': 17, 'action': 'right', 'reward': 0.015911924618736695, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 0.02)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 1.1774270282011263, 'left': 1.0717049495847353, 'right': 0.7599968239915724, None: -4.939066832519487}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: None, reward: -4.23358396008
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 7, 't': 18, 'action': None, 'reward': -4.2335839600847915, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.23)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -9.352172166545365, 'left': -9.631183023046244, 'right': 0.6289009916621053, None: 2.217436731043758}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: left, reward: -9.09363716014
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -9.093637160138199, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.09)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': -10.094530292640922, 'left': -10.086016616220904, 'right': 1.2767878458856967, None: 2.4332321731620743}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: forward, reward: -10.1920564283
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -10.192056428316986, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.19)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -30.51273979116884, 'left': -19.833166387044734, 'right': -9.578054242763955, None: 1.3078885640891256}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: forward, reward: -39.4837307388
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -39.48373073884341, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.48)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': 0.5520522113603847, 'left': 0.9689580603068322, 'right': 1.1207916355646377, None: 0.4364052249845423}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: 1.11399144492
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 1.1139914449183146, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent drove forward instead of right. (rewarded 1.11)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': 1.116999539250746, 'left': 0.7262981728105677, 'right': 1.7138782790864349, None: 0.6991751284308917}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: left, reward: 1.01658669014
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 2, 't': 23, 'action': 'left', 'reward': 1.0165866901381115, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.02)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.8278005233682115, 'left': 1.7121614036816404, 'right': 1.2564525824057489, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: -0.449936426861
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 24, 'action': 'right', 'reward': -0.44993642686076063, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded -0.45)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 96
\-------------------------

Environment.reset(): Trial set up with start = (8, 4), destination = (5, 3), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.7589861682590369, 'left': 0.605134434256815, 'right': 1.5796761582831478, None: 0.21291611379519323}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: None, reward: 1.59932917519
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 20, 't': 0, 'action': None, 'reward': 1.5993291751933958, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent idled at a green light with oncoming traffic. (rewarded 1.60)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.39985983141780135, 'left': 0.37616113942937657, 'right': 0.8925079238852455, None: -2.9548276019880584}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: forward, reward: 0.133975317958
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'right'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 0.1339753179583122, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'forward', 'right')
Agent drove forward instead of right. (rewarded 0.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.7998693356494042, 'left': -0.0485734347393747, 'right': 0.5622384865000678, None: -4.865834064811505}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: 2.78392152671
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 2.7839215267056474, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent followed the waypoint right. (rewarded 2.78)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': 1.6533130671364495, 'left': 0.19848581269597432, 'right': 1.0688873726205737, None: -5.251989411185038}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: forward, reward: 1.7221237548
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 1.722123754800246, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.72)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: forward, reward: -39.4027303385
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'forward', 'forward'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': -39.402730338521714, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.40)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.039196906733566364, None: 2.0346670163948115}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: right, reward: 0.193137212059
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 0.19313721205867118, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.19)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -37.43651127410148, 'left': -37.15804930756421, 'right': 0.3780526658196264, None: 0.8640281376196574}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: right, reward: 1.43898154989
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.4389815498876701, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.44)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.524079453353394, 'left': -10.313014563986755, 'right': 0.7866317595895855, None: 1.6654965949750626}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: right, reward: 1.70569729383
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.7056972938317247, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.8517347734094066, 'left': 0.6859138143467407, 'right': 1.1181447849398056, None: -4.57363600787029}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: 1.04898682542
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 12, 't': 8, 'action': 'left', 'reward': 1.048986825421975, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove left instead of right. (rewarded 1.05)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.376993872006702, 'left': 0.0, 'right': -0.40007635440542244, None: 0.3894894287662357}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: -40.4018052865
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'right', 'left', None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -40.40180528652216, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'right', 'left', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.40)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.3015506886615158}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: forward, reward: -9.01910841121
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -9.01910841121031, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -9.02)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -9.524079453353394, 'left': -10.313014563986755, 'right': 1.2461645267106551, None: 1.6654965949750626}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: -10.1829503686
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -10.182950368601324, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.18)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': -9.501557680781751, 'left': -10.061203763674591, 'right': 0.5003551697104481, None: 1.4633979135530935}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: forward, reward: -10.1898530173
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -10.189853017295832, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light. (rewarded -10.19)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.468274239306827, 'left': -19.963033247261848, 'right': 0.6790982035521745, None: -4.769845209412209}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: None, reward: -5.5481945828
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': -5.5481945827983115, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': 0.13580510957883626, 'left': -17.8346656957853, 'right': 0.002219643778226421, None: -2.0691990998302656}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: left, reward: -20.9256304526
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -20.925630452570598, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.93)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.4783397258548069, 'left': 1.7024435629210446, 'right': 0.7545125119719971, None: -4.927049471362601}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: right, reward: -0.409920718204
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 5, 't': 15, 'action': 'right', 'reward': -0.4099207182037934, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove right instead of left. (rewarded -0.41)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': -7.823219223716316, 'left': -5.387928055003375, 'right': 0.7970204702538358, None: 1.3805973711875317}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (-1, 0), action: right, reward: 2.28999828122
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 2.2899982812219077, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, 'right', None)
Agent followed the waypoint right. (rewarded 2.29)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.4726256014202932, 'left': 0.78642489494956, 'right': 0.4589533924540907, None: -5.097396010542358}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: forward, reward: 0.985010412367
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.9850104123666381, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 0.99)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -30.297880834760285, 'left': -30.280947970215564, 'right': -17.758625253816334, None: 1.8213279540068852}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: None, reward: 0.776842994464
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 2, 't': 18, 'action': None, 'reward': 0.7768429944638393, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 0.78)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 1.6877184109683476, 'left': 0.19848581269597432, 'right': 1.0688873726205737, None: -5.251989411185038}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: forward, reward: 0.234464859391
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': 0.23446485939068906, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 0.23)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 97
\-------------------------

Environment.reset(): Trial set up with start = (1, 5), destination = (2, 2), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -19.52252022066547, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['left', 'right', None]
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: left, reward: -40.6899356281
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'right', 'forward'), 'deadline': 20, 't': 0, 'action': 'left', 'reward': -40.689935628128275, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': -19.94315632677607, 'left': -20.26569258375047, 'right': -0.00045646503962193385, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: None, reward: 1.18138847037
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'forward'), 'deadline': 19, 't': 1, 'action': None, 'reward': 1.1813884703714888, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.901475443110789, 'left': -9.766869285184445, 'right': 0.403964634777692, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: -10.9228842624
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.922884262373211, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -10.92)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -10.412179852742, 'left': -9.766869285184445, 'right': 0.403964634777692, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: -9.64548080173
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -9.645480801725565, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving forward through a red light. (rewarded -9.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -10.028830327233782, 'left': -9.766869285184445, 'right': 0.403964634777692, None: 2.0021290983337834}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (8, 5), heading: (-1, 0), action: right, reward: 1.83760033312
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.837600333121876, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, None)
Agent drove right instead of left. (rewarded 1.84)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.05894904310659793, 'left': 1.7695400650952917, 'right': 0.5367312532105902, None: 1.4032320345219533}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: forward, reward: 1.46777959157
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': 1.4677795915742016, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 1.47)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.7633643173403998, 'left': 1.7695400650952917, 'right': 0.5367312532105902, None: 1.4032320345219533}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (0, 1), action: left, reward: 1.17415373481
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 1.1741537348146958, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, None)
Agent followed the waypoint left. (rewarded 1.17)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.4946062192501341, 'left': 1.0332273852508398, 'right': 0.5646305477340334, None: 0.8822386255297519}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: forward, reward: 0.410786406004
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 0.4107864060036588, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent drove forward instead of left. (rewarded 0.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.80494796985419, 'left': -9.478538813366555, 'right': 0.14458206381830013, None: 1.0206087490538431}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: left, reward: -9.32715366138
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -9.327153661377022, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.33)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.5024169630987672, 'left': -15.277379805247598, 'right': 0.9627422450323071, None: -3.283829448489367}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: left, reward: -20.8735075633
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': -20.873507563270294, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'right', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.87)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.8426208992618491, 'left': 2.2901405895030593, 'right': 0.22406570957439165, None: -4.4835836874225485}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: right, reward: 1.33891471564
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 1.3389147156387713, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove right instead of left. (rewarded 1.34)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.8278005233682115, 'left': 1.7121614036816404, 'right': 0.4032580777724941, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 0.020482411011
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 0.020482411011025103, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.02)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 0.8517347734094066, 'left': 0.8674503198843578, 'right': 1.1181447849398056, None: -4.57363600787029}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: right, reward: 2.56850526924
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 2.5685052692395667, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, 'left')
Agent followed the waypoint right. (rewarded 2.57)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -7.77810412778626, 'left': -8.412327627634511, 'right': 0.8206224210389526, None: 1.4083080562350139}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: -9.81105106919
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': -9.81105106919182, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.81)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.300570175657793, 'left': -10.090455801709716, 'right': 0.8452425683204233, None: 1.9190437449288358}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: -9.54178115649
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -9.541781156493867, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.54)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': -10.300570175657793, 'left': -9.816118479101792, 'right': 0.8452425683204233, None: 1.9190437449288358}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: left, reward: -9.16685677529
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -9.16685677529062, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.17)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 0.9610916351795183, 'left': 0.19848581269597432, 'right': 1.0688873726205737, None: -5.251989411185038}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: None, reward: -5.17004605981
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': -5.170046059805342, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.17)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 1.5851840707162448, 'left': 0.28848309421210044, 'right': 0.2831083366005055, None: -4.664947438122572}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: 0.862131166547
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.8621311665468689, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent followed the waypoint forward. (rewarded 0.86)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.9610916351795183, 'left': 0.19848581269597432, 'right': 1.0688873726205737, None: -5.21101773549519}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: None, reward: -4.66234919704
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': -4.662349197042785, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.66)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': -6.991676951845406, 'left': -4.950000943770685, 'right': 1.1967893255736013, None: 0.6912272734238653}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: -9.43802091473
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': -9.438020914726911, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.44)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 98
\-------------------------

Environment.reset(): Trial set up with start = (2, 6), destination = (6, 6), deadline = 20
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': -0.0005197836940220579, 'left': -10.297785093259217, 'right': 0.198810636507754, None: -2.387809036530959}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: 1.54396090134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', None), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 1.5439609013381044, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', None)
Agent drove forward instead of left. (rewarded 1.54)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.8278005233682115, 'left': 1.7121614036816404, 'right': 0.2118702443917596, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: 1.04592047704
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 1.04592047703549, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': 0.8940012286113628, 'left': 1.0553712889259184, 'right': 0.6857679880449004, None: -2.577849171873072}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: forward, reward: 0.361054887015
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': 0.3610548870153699, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent drove forward instead of left. (rewarded 0.36)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -4.837167337146998, 'left': -9.00175603894574, 'right': 0.6975694546056146, None: 1.262866166156671}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: forward, reward: -10.0242890575
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'right', None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -10.024289057547058, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -7.7258138668295135, 'left': -4.740624278078626, 'right': -0.0321859633613375, None: 1.3087762246935766}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: right, reward: 1.04324325389
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, 'right'), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.0432432538926784, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'left', None, 'right')
Agent drove right instead of left. (rewarded 1.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 0.0, 'left': -4.684346438304335, 'right': 0.07208153369590409, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: None, reward: 2.55859086451
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'right', None), 'deadline': 15, 't': 5, 'action': None, 'reward': 2.5585908645065576, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'right', None)
Agent properly idled at a red light. (rewarded 2.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': -7.913183956902648, 'left': -5.316620902961126, 'right': 0.4939406876573365, None: 1.7569354503537684}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: right, reward: 0.809906506039
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 0.8099065060393815, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent drove right instead of forward. (rewarded 0.81)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': -9.524079453353394, 'left': -10.24798246629404, 'right': 1.2461645267106551, None: 1.6654965949750626}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: None, reward: 2.66858156114
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': 2.6685815611409125, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': -9.524079453353394, 'left': -10.24798246629404, 'right': 1.2461645267106551, None: 2.1670390780579876}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: left, reward: -9.59147725451
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'left', 'reward': -9.591477254514423, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.59)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.9368605002018507, 'left': 1.7121614036816404, 'right': 0.2118702443917596, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: left, reward: 1.33306405594
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': 1.3330640559424194, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.33)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': -10.300570175657793, 'left': -9.491487627196207, 'right': 0.8452425683204233, None: 1.9190437449288358}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: right, reward: -0.118713603758
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': -0.1187136037579789, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded -0.12)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': -0.26461059568637424, 'left': 0.5483859080394431, 'right': 0.0, None: 0.6954709340560028}
learning, learn new
zeroAction: ['right']
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: right, reward: 1.06423678765
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 1.0642367876541676, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'forward', None)
Agent drove right instead of left. (rewarded 1.06)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.0217575651610633, 'left': 1.702260715655616, 'right': 0.8238707581499782, None: -4.705979010242035}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: forward, reward: 1.51046337151
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': 1.5104633715110265, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 1.51)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.11616705939611877, None: 2.0346670163948115}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: None, reward: 2.36727628898
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 7, 't': 13, 'action': None, 'reward': 2.367276288975246, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 2.37)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -9.803032063203608, 'left': -8.316042606030829, 'right': 0.11616705939611877, None: 2.2009716526850287}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: left, reward: -9.57897201322
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': -9.578972013215111, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving left through a red light. (rewarded -9.58)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 1.115822261393763}
learning, learn new
zeroAction: ['forward', 'left', 'right']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: left, reward: -10.9886346927
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', 'right'), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -10.988634692672337, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'forward', 'right', 'right')
Agent attempted driving left through a red light. (rewarded -10.99)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.0618337728920777, 'left': -19.211698031919653, 'right': 0.658314416258026, None: -4.066550417004045}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: None, reward: -4.49692931868
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': -4.4969293186803325, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': 0.9610916351795183, 'left': 0.19848581269597432, 'right': 1.0688873726205737, None: -4.936683466268987}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: right, reward: 0.851297094934
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.8512970949341448, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 0.85)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: right, reward: -0.349032015214
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'left', 'left'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': -0.3490320152140698, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'left', 'left', 'left')
Agent drove right instead of left. (rewarded -0.35)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.0, 'left': -20.37826037399169, 'right': -9.572338352020761, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: None, reward: 1.49722241969
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', 'forward', 'forward'), 'deadline': 1, 't': 19, 'action': None, 'reward': 1.4972224196920165, 'waypoint': 'forward'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'right', 'forward', 'forward')
Agent properly idled at a red light. (rewarded 1.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 99
\-------------------------

Environment.reset(): Trial set up with start = (2, 7), destination = (6, 2), deadline = 25
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 1.488365306585045, 'left': 0.24270504422498995, 'right': 0.26940721727094513, None: -4.963173369259968}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 6), heading: (0, -1), action: right, reward: 1.9410521826
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 1.9410521826005036, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 1.94)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': -9.592401530551928, 'right': 0.11957034879550343, None: -2.053706348654183}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: 1.73654576735
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', 'left'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 1.7365457673526918, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'forward', 'left')
Agent drove forward instead of left. (rewarded 1.74)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -9.524079453353394, 'left': -9.919729860404232, 'right': 1.2461645267106551, None: 2.1670390780579876}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -10.0325728305
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': -10.032572830523561, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.03)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.524079453353394, 'left': -9.976151345463897, 'right': 1.2461645267106551, None: 2.1670390780579876}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: None, reward: 2.07959643833
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.079596438328897, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.524079453353394, 'left': -9.976151345463897, 'right': 1.2461645267106551, None: 2.1233177581934424}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: -9.91836213025
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': -9.918362130252063, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.92)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': -9.721220791802729, 'left': -9.976151345463897, 'right': 1.2461645267106551, None: 2.1233177581934424}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: -9.48525330483
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -9.485253304829223, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.49)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 0.9368605002018507, 'left': 1.52261272981203, 'right': 0.2118702443917596, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: 1.40490613462
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 1.4049061346221619, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.40)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 1.1708833174120064, 'left': 1.52261272981203, 'right': 0.2118702443917596, None: -5.17364472033966}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: -5.94097313832
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': -5.9409731383189985, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 0.6196463321593687, 'left': -10.457848570063845, 'right': 0.696102335419349, None: -2.7860201112177005}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: -5.3117087845
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'forward', 'right', None), 'deadline': 17, 't': 8, 'action': None, 'reward': -5.31170878450331, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.31)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': -5.437513937771278, 'left': -8.792304191906092, 'right': 1.2957178957186573, None: 1.0678978904840006}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: -9.37896891632
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 16, 't': 9, 'action': 'forward', 'reward': -9.378968916320952, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.38)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: -39.3252815622
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', 'forward'), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': -39.32528156215312, 'waypoint': 'left'}
Agent previous state: ('left', 'red', 'forward', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.33)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 0.468274239306827, 'left': -19.963033247261848, 'right': 0.6790982035521745, None: -5.15901989610526}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: left, reward: -19.0958277431
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -19.09582774313341, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.10)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
state
{'forward': 1.1708833174120064, 'left': 1.52261272981203, 'right': 0.2118702443917596, None: -5.557308929329329}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: right, reward: 1.72337688136
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 1.7233768813568293, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 1.72)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
state
{'forward': 0.9610916351795183, 'left': 0.19848581269597432, 'right': 0.9600922337773592, None: -4.936683466268987}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: 2.56343912561
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 2.563439125610194, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 2.56)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
state
{'forward': -10.300570175657793, 'left': -9.491487627196207, 'right': 0.3632644822812222, None: 1.9190437449288358}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: None, reward: 1.10811985015
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 1.1081198501466138, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.11)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
state
{'forward': 0.0, 'left': -4.797956217579087, 'right': 0.260295293935688, None: 1.1933779785915037}
learning, learn new
zeroAction: ['forward']
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: -10.8840230698
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'right'), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -10.88402306976388, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -10.88)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
state
{'forward': 1.2236576186315569, 'left': 0.28848309421210044, 'right': 0.2831083366005055, None: -4.664947438122572}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: left, reward: 1.13673661132
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': 1.1367366113181414, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.14)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
state
{'forward': -9.443409612759366, 'left': -10.257782208431163, 'right': 1.3056636251632054, None: 1.2356468011765784}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: forward, reward: -9.28032403828
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': -9.280324038279836, 'waypoint': 'right'}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.28)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
state
{'forward': -5.169493008610074, 'left': -20.30139091121564, 'right': 1.264822589743049, None: 0.0}
learning, learn new
zeroAction: [None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: None, reward: 1.81875450036
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 7, 't': 18, 'action': None, 'reward': 1.8187545003586947, 'waypoint': 'right'}
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent properly idled at a red light. (rewarded 1.82)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
state
{'forward': 0.717802908212047, 'left': 0.8094848024198951, 'right': 0.5029408890303542, None: -3.3631655834043013}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (3, 3), heading: (-1, 0), action: left, reward: 0.925540963357
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 0.9255409633569375, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove left instead of right. (rewarded 0.93)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
state
{'forward': 0.5263281651809676, 'left': 0.9008143324932225, 'right': 1.805755177647048, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: forward, reward: -0.305451645674
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -0.30545164567392447, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded -0.31)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
state
{'forward': -10.300570175657793, 'left': -9.491487627196207, 'right': 0.3632644822812222, None: 1.5135817975377248}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: right, reward: 0.667761241667
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': 'right', 'reward': 0.6677612416673508, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.67)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
state
{'forward': -9.80494796985419, 'left': -9.402846237371788, 'right': 0.14458206381830013, None: 1.0206087490538431}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: left, reward: -9.83579622488
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -9.83579622488089, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving left through a red light. (rewarded -9.84)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
state
{'forward': -0.014769960112571923, 'left': 0.0, 'right': 0.3787996367460704, None: -2.9311443648864515}
learning, learn new
zeroAction: ['left']
in zeroAction choose: left
At last we choose: left
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: left, reward: -20.4284684628
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'left'), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -20.428468462801, 'waypoint': 'left'}
Agent previous state: ('left', 'green', 'forward', None, 'left')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.43)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
state
{'forward': 0.13580510957883626, 'left': -19.38014807417795, 'right': 0.002219643778226421, None: -2.0691990998302656}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: right, reward: -0.503608574939
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': -0.5036085749387722, 'waypoint': 'left'}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent drove right instead of left. (rewarded -0.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 100
\-------------------------

Environment.reset(): Trial set up with start = (7, 2), destination = (3, 6), deadline = 30
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000
Simulating trial. . . 
epsilon = 1.0000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
state
{'forward': 0.0, 'left': 0.0, 'right': 0.0, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', 'right', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: forward, reward: -40.016733251
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'forward'), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -40.01673325104979, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.02)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
state
{'forward': 0.0, 'left': -4.889391552715835, 'right': 0.5501485411908174, None: 0.0}
learning, learn new
zeroAction: ['forward', None]
in zeroAction choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: None, reward: 2.30928163556
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'right', 'left'), 'deadline': 29, 't': 1, 'action': None, 'reward': 2.309281635555672, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', 'right', 'left')
Agent properly idled at a red light. (rewarded 2.31)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
state
{'forward': -7.913183956902648, 'left': -5.316620902961126, 'right': 0.651923596848359, None: 1.7569354503537684}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: right, reward: 0.644517938328
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 0.6445179383281227, 'waypoint': 'forward'}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent drove right instead of forward. (rewarded 0.64)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
state
{'forward': -9.603237048315975, 'left': -9.976151345463897, 'right': 1.2461645267106551, None: 2.1233177581934424}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: None, reward: 2.01108084886
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 27, 't': 3, 'action': None, 'reward': 2.0110808488615386, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.01)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
state
{'forward': -9.603237048315975, 'left': -9.976151345463897, 'right': 1.2461645267106551, None: 2.0671993035274907}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: left, reward: -10.3780289098
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 26, 't': 4, 'action': 'left', 'reward': -10.37802890975344, 'waypoint': 'left'}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.38)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
state
{'forward': 1.1708833174120064, 'left': 1.52261272981203, 'right': 0.9676235628742944, None: -5.557308929329329}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: None, reward: -5.5200435442
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 25, 't': 5, 'action': None, 'reward': -5.520043544197234, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
state
{'forward': 1.1708833174120064, 'left': 1.52261272981203, 'right': 0.9676235628742944, None: -5.538676236763282}
learning, learn new
zeroAction: []
in random choose: None
At last we choose: None
Environment.act() [POST]: location: (7, 3), heading: (0, 1), action: None, reward: -4.23114991766
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 24, 't': 6, 'action': None, 'reward': -4.231149917655063, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.23)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
state
{'forward': 0.8426208992618491, 'left': 2.2901405895030593, 'right': 0.7814902126065815, None: -4.4835836874225485}
learning, learn new
zeroAction: []
in random choose: right
At last we choose: right
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: right, reward: 0.983241978776
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 0.9832419787756458, 'waypoint': 'left'}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove right instead of left. (rewarded 0.98)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
state
{'forward': 1.7622653803948563, 'left': 0.19848581269597432, 'right': 0.9600922337773592, None: -4.936683466268987}
learning, learn new
zeroAction: []
in random choose: left
At last we choose: left
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: left, reward: 1.42408566903
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 22, 't': 8, 'action': 'left', 'reward': 1.4240856690298278, 'waypoint': 'forward'}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.42)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
state
{'forward': 0.0, 'left': 0.0, 'right': 1.2524342727439588, None: 0.0}
learning, learn new
zeroAction: ['forward', 'left', None]
in zeroAction choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 5), heading: (0, 1), action: forward, reward: 1.56374202182
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', None), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': 1.5637420218167464, 'waypoint': 'right'}
Agent previous state: ('right', 'green', 'right', 'forward', None)
Agent drove forward instead of right. (rewarded 1.56)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
state
{'forward': 0.11043825975352156, 'left': 0.9008143324932225, 'right': 1.805755177647048, None: -4.644722577154872}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: 0.857440778928
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': 0.8574407789277458, 'waypoint': 'right'}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.86)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
state
{'forward': 1.116999539250746, 'left': 0.8714424314743396, 'right': 1.7138782790864349, None: 0.6991751284308917}
learning, learn new
zeroAction: []
in random choose: forward
At last we choose: forward
Environment.act() [POST]: location: (6, 7), heading: (0, 1), action: forward, reward: 1.83239249001
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 19, 't': 11, 'action': 'forward', 'reward': 1.8323924900145134, 'waypoint': 'right'}

Simulation ended. . . 
